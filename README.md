# HOMA-Higher-Order-MultiHead-Attention
A novel parameter efficient fine-tuning method to go beyond dot-product attention
