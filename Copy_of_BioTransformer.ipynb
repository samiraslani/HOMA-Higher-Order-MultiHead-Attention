{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "kjRwy6xQYHtD",
      "metadata": {
        "id": "kjRwy6xQYHtD"
      },
      "source": [
        "- Linformer cannot be implemented on Decoder models.\n",
        "- Multi-ped can be run for longer sequences if we increase the number of peds but performance is not as good as the linformer.\n",
        "- Attention3D and Attention combined cannot be run for longer sequences.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f66d7277-762a-4f17-a292-4e284d32c92a",
      "metadata": {
        "id": "f66d7277-762a-4f17-a292-4e284d32c92a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import contextlib\n",
        "import io\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fGgylUnz4iD2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGgylUnz4iD2",
        "outputId": "27b734b4-eabe-4622-ebed-3d40540efe37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "os.chdir('/content/drive/My Drive/BioTransformer/TAPE ')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eiD5K9aD546K",
      "metadata": {
        "id": "eiD5K9aD546K"
      },
      "source": [
        "## Defining the Models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xrcu2k3nrTrd",
      "metadata": {
        "id": "xrcu2k3nrTrd"
      },
      "source": [
        "- The goal is to first make sure that the models output all sequences instead of only one output so no collapsing no pooling.\n",
        "- Then we will need to check the padding and sequence length is it the same for all of the samples?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "M15RviEFNmkE",
      "metadata": {
        "id": "M15RviEFNmkE"
      },
      "outputs": [],
      "source": [
        "### Modify the code for the linear combination of the encoder output\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, p):\n",
        "        super().__init__()\n",
        "        self.layer1 = nn.Linear(d_model, d_ff)\n",
        "        self.layer2 = nn.Linear(d_ff, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layer2(self.dropout(self.relu(self.layer1(x))))\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, p, dim_feedforward, attn_mechanism, **kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "        # Extract required arguments from kwargs\n",
        "        d_model = kwargs[\"d_model\"]\n",
        "        num_heads = kwargs[\"num_heads\"]\n",
        "        len_seq = kwargs[\"len_seq\"]\n",
        "        self.block_size = kwargs.get('block_size', None)\n",
        "        self.stride = kwargs.get('stride',None)\n",
        "\n",
        "        # Modify len-seq if padding is required\n",
        "        if attn_mechanism in {\"Attn2DMultiPed3Dselected\", \"Multiped2D\", \"MultiLin2D\", \"Attn3DLinformerMultiPed\", \"Multiped3D_Dual\", \"MultiLin3D_Dual\", \"Attn2DMultiPed3Dselected_Dual\"}:\n",
        "              remainder = (len_seq - self.block_size) % self.stride\n",
        "              pad_len = (self.stride - remainder) % self.stride\n",
        "              if pad_len > 0:\n",
        "                len_seq = len_seq + pad_len\n",
        "                #print(f'len of seq in enc is changed to {len_seq}')\n",
        "                kwargs[\"len_seq\"] = len_seq  # Update the value in kwargs  # Update the value in kwargs\n",
        "\n",
        "        # Initialize attention mechanism\n",
        "        self.mha = get_attention(attn_mechanism, **kwargs)\n",
        "\n",
        "        # Feedforward and normalization layers\n",
        "        self.ffn = FeedForward(d_model, dim_feedforward, p)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(p)\n",
        "\n",
        "    def forward(self, x,mask = None):\n",
        "        attn_output = self.dropout(self.mha(x,mask))\n",
        "        x = self.norm1(x + attn_output)\n",
        "        ffn_output = self.dropout(self.ffn(x))\n",
        "        x = self.norm2(x + ffn_output)\n",
        "        return x\n",
        "\n",
        "class ProteinTransformerSS3(nn.Module):\n",
        "    def __init__(self, num_classes, vocab_size, num_layers=2, p=0.2, dim_feedforward=32, attn_mechanism=\"Plain3D\", **kwargs):\n",
        "        super().__init__()\n",
        "        self.kwargs = kwargs\n",
        "        d_model = kwargs[\"d_model\"]\n",
        "        self.attn_mechanism = attn_mechanism\n",
        "        self.num_classes = num_classes\n",
        "        len_seq = kwargs[\"len_seq\"]\n",
        "        self.block_size = kwargs.get('block_size', None)\n",
        "        self.stride = kwargs.get('stride',None)\n",
        "\n",
        "        # Encoder and Attention\n",
        "        self.encoder_layers = nn.ModuleList([\n",
        "            Encoder(p=p, dim_feedforward=dim_feedforward, attn_mechanism=attn_mechanism, **kwargs)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Modify the len-seq based on padding required\n",
        "        if attn_mechanism in {\"Attn2DMultiPed3Dselected\", \"Multiped2D\", \"MultiLin2D\", \"Multiped3D\", \"Attn3DLinformerMultiPed\", \"Multiped3D_Dual\", \"MultiLin3D_Dual\", \"Attn2DMultiPed3Dselected_Dual\"}:\n",
        "          # how much we need to pad\n",
        "          remainder = (len_seq - self.block_size) % self.stride\n",
        "          pad_len = (self.stride - remainder) % self.stride\n",
        "          if pad_len > 0:\n",
        "            len_seq = len_seq + pad_len\n",
        "            #print(f'len of seq var in transformer is changed to {len_seq}')\n",
        "            kwargs[\"len_seq\"] = len_seq  # Update the value in kwargs\n",
        "\n",
        "        # AA embedding layer\n",
        "        self.token_embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
        "\n",
        "        # Positional Embeddings\n",
        "        self.position_embedding = nn.Embedding(len_seq, d_model)\n",
        "\n",
        "        # Feed Forward Layer\n",
        "        self.classifier = nn.Linear(d_model, num_classes)\n",
        "\n",
        "    def sliding_blocks(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x (Tensor): Input tensor of shape (batch, seq_len, d_model).\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Reshaped tensor of shape (batch, num_blocks, l, d_model).\n",
        "        \"\"\"\n",
        "        l = self.block_size\n",
        "        d = self.stride\n",
        "        batch, seq_len = x.shape\n",
        "\n",
        "        # unfold along the sequence dimension\n",
        "        blocks = x.unfold(dimension=1, size=l, step=d)   # (batch, num_blocks, l, d_model)\n",
        "\n",
        "        return blocks\n",
        "\n",
        "    def generate_padding_mask(self, input_seq):\n",
        "      seq_len = input_seq.shape[1]\n",
        "      batch_size = input_seq.shape[0]\n",
        "      mask = input_seq != 0 # shape = [bs, len_seq]\n",
        "      # if self.num_peds:\n",
        "      #   len_ped = seq_len // self.num_peds\n",
        "      if self.attn_mechanism in {\"Multiped3D\",\"Multiped3D_Dual\"}:\n",
        "        return self.sliding_blocks(input_seq !=0).unsqueeze(2).unsqueeze(2).unsqueeze(2)\n",
        "      elif self.attn_mechanism == \"Multiped2D\":\n",
        "        return self.sliding_blocks(input_seq !=0).unsqueeze(2).unsqueeze(2)\n",
        "      elif self.attn_mechanism == 'Linformer2D':\n",
        "        return mask.unsqueeze(1).unsqueeze(3)\n",
        "      elif self.attn_mechanism == 'Plain2D':\n",
        "        return mask.unsqueeze(1).unsqueeze(1)\n",
        "      elif self.attn_mechanism == 'Plain3D':\n",
        "        return mask.unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
        "      elif self.attn_mechanism in {'Linformer3D', 'Linformer3D_Dual'}:\n",
        "        return mask.unsqueeze(1).unsqueeze(3).unsqueeze(3)\n",
        "      elif self.attn_mechanism == 'MultiLin2D':\n",
        "        return self.sliding_blocks(input_seq !=0).unsqueeze(2).unsqueeze(4)\n",
        "      elif self.attn_mechanism in {\"Attn3DLinformerMultiPed\", \"MultiLin3D_Dual\"}:\n",
        "        return self.sliding_blocks(input_seq !=0).unsqueeze(2).unsqueeze(4).unsqueeze(4)\n",
        "      elif self.attn_mechanism == \"Combined2D3D\":\n",
        "        mask2d = mask.unsqueeze(1).unsqueeze(1)\n",
        "        mask3d = mask.unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
        "        return tuple([mask2d,mask3d])\n",
        "      elif self.attn_mechanism == \"Attn2D3DLin\":\n",
        "        mask2d = mask.unsqueeze(1).unsqueeze(1)\n",
        "        mask3d =  mask.unsqueeze(1).unsqueeze(3).unsqueeze(3)\n",
        "        return tuple([mask2d,mask3d])\n",
        "      elif self.attn_mechanism == \"Attn2D3D_MultiPed\":\n",
        "        mask2d = mask.unsqueeze(1).unsqueeze(1)\n",
        "        mask3d = mask.reshape(batch_size, self.num_peds, len_ped).unsqueeze(2).unsqueeze(2).unsqueeze(2)\n",
        "        return tuple([mask2d,mask3d])\n",
        "      elif self.attn_mechanism == \"Attn2D3D_MultiPedLin\":\n",
        "        mask2d = mask.unsqueeze(1).unsqueeze(1)\n",
        "        mask3d = mask.reshape(batch_size, self.num_peds, len_ped).unsqueeze(2).unsqueeze(4).unsqueeze(4)\n",
        "        return tuple([mask2d,mask3d])\n",
        "      elif self.attn_mechanism == \"Attn2D3DLinAlter\":\n",
        "        mask2d = mask.unsqueeze(1).unsqueeze(1)\n",
        "        mask3d = mask.unsqueeze(1).unsqueeze(3).unsqueeze(3)\n",
        "        return tuple([mask2d,mask3d])\n",
        "      elif self.attn_mechanism == \"Attn2D3D_MultiPedLinAlter\":\n",
        "        mask2d = mask.unsqueeze(1).unsqueeze(1)\n",
        "        mask3d = mask.reshape(batch_size, self.num_peds, len_ped).unsqueeze(2).unsqueeze(4).unsqueeze(4)\n",
        "        return tuple([mask2d,mask3d])\n",
        "      elif self.attn_mechanism == \"Attn2D3D_MultiPedAlter\":\n",
        "        mask2d = mask.unsqueeze(1).unsqueeze(1)\n",
        "        mask3d = mask.reshape(batch_size, self.num_peds, len_ped).unsqueeze(2).unsqueeze(2).unsqueeze(2)\n",
        "        return tuple([mask2d,mask3d])\n",
        "      elif self.attn_mechanism in {\"Attn2DMultiPed3Dselected\", \"Attn2DMultiPed3Dselected_Dual\"}:\n",
        "        return self.sliding_blocks(input_seq !=0)\n",
        "\n",
        "    def pad_tensor(self, labels, pad_value, len_seq):\n",
        "      return torch.nn.functional.pad(labels, (0, len_seq-labels.size(1)), value=pad_value)\n",
        "\n",
        "    def pad_to_sliding_blocks(self, x, pad_value=0):\n",
        "        \"\"\"\n",
        "        Pad sequence length so it divides evenly into sliding-window blocks.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): (B, L, D) input sequence\n",
        "            block_size (int): size of each block (l)\n",
        "            stride (int): sliding window step (d)\n",
        "            pad_value (float, optional): value to use for padding. Defaults to 0.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: padded sequence of shape (B, L', D), where\n",
        "                    (L' - block_size) % stride == 0\n",
        "            int: number of padding tokens added\n",
        "        \"\"\"\n",
        "        B, L = x.shape\n",
        "\n",
        "        # how much we need to pad\n",
        "        remainder = (L - self.block_size) % self.stride\n",
        "        pad_len = (self.stride - remainder) % self.stride\n",
        "        #print(f'pad_len : {pad_len}')\n",
        "        if pad_len > 0:\n",
        "            pad_tensor = x.new_full((B, pad_len), pad_value)\n",
        "            x = torch.cat([x, pad_tensor], dim=1)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, input_ids, labels):\n",
        "      B, L = input_ids.shape\n",
        "\n",
        "      is_multiped = self.attn_mechanism in {\"Attn3DLinformerMultiPed\", \"MultiLin2D\", \"Attn2D3D_MultiPed\", \"Attn2D3D_MultiPedLin\", \"Attn2DMultiPed3Dselected\", \"Multiped2D\", \"Multiped3D\", \"Multiped3D_Dual\", \"MultiLin3D_Dual\", \"Attn2DMultiPed3Dselected_Dual\"}\n",
        "      #print(f'is_multiped: {is_multiped}')\n",
        "      # Add Padding if multiped\n",
        "      if is_multiped:\n",
        "        if self.attn_mechanism in {\"Attn2DMultiPed3Dselected\", \"Multiped2D\", \"MultiLin2D\", \"Multiped3D\", \"Attn3DLinformerMultiPed\", \"Multiped3D_Dual\", \"MultiLin3D_Dual\", \"Attn2DMultiPed3Dselected_Dual\"}:\n",
        "          # how much we need to pad\n",
        "          remainder = (L - self.block_size) % self.stride\n",
        "          pad_len = (self.stride - remainder) % self.stride\n",
        "          #print(f'number of pads needed: {pad_len}')\n",
        "          if pad_len > 0:\n",
        "            input_ids = F.pad(input_ids, (0, pad_len), value=0)\n",
        "            # Modify the labels\n",
        "            L = input_ids.shape[1]\n",
        "            labels = self.pad_tensor(labels, -100, L)\n",
        "            #print(f'padding is applied to the input shape: {input_ids.shape}')\n",
        "            #print(f'padding is applied to the labels shape: {labels.shape}')\n",
        "\n",
        "        # Create the mask internally\n",
        "      masks = self.generate_padding_mask(input_ids)\n",
        "      # Create placeholder for positions in the sequence\n",
        "      pos_ids = torch.arange(L, device=input_ids.device).unsqueeze(0).expand(B, L)\n",
        "      # Embedding of AAs + Positional Encoding\n",
        "      x = self.token_embedding(input_ids) + self.position_embedding(pos_ids)\n",
        "      for layer in self.encoder_layers:\n",
        "        x = layer(x, mask=masks)\n",
        "\n",
        "      return self.classifier(x), labels"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "t5AiSMrK1ak1",
      "metadata": {
        "id": "t5AiSMrK1ak1"
      },
      "source": [
        "### 2D Attention Mechanisms"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8N0UBn7A1WAH",
      "metadata": {
        "id": "8N0UBn7A1WAH"
      },
      "source": [
        "#### Attention 2D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JtWDcr0ks5lc",
      "metadata": {
        "id": "JtWDcr0ks5lc"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttn2D(nn.Module):\n",
        "\n",
        "    def __init__(self,num_heads, d_model):\n",
        "        super(MultiHeadAttn2D,self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_params = d_model//num_heads\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, d_model) # mapping to the same number of features\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # Check if the number of len_emb (d_model) is divisable by num_heads\n",
        "        assert d_model % num_heads == 0, \"d_model is not divisable by the number of heads\"\n",
        "\n",
        "\n",
        "    def self_attention(self, q, k, v, mask):\n",
        "\n",
        "        # Calculate the dot produce of Q and K\n",
        "        dotqk = torch.matmul(q, k.transpose(-2,-1))/(self.head_params**0.5) # shape = (#samples, num_heads, len_seq, len_seq)\n",
        "        # Apply the mask if givne\n",
        "        if mask is not None:\n",
        "            dotqk = dotqk.masked_fill(mask == 0, -1e9)\n",
        "        # Apply the Softmax to the attention weights\n",
        "        attention_scores = torch.softmax(dotqk, dim = -1)\n",
        "        # Multiply to the value matrix\n",
        "        result = torch.matmul(attention_scores, v)\n",
        "        return(attention_scores,result)\n",
        "\n",
        "    def split_heads(self, q):\n",
        "\n",
        "        \"\"\"\n",
        "        Reshaping\n",
        "        Input of shape : (#samples, seq_len, d_model)\n",
        "\n",
        "        to\n",
        "        output of shape: (#samples, num_heads, seq_len, head_parameters)\n",
        "        \"\"\"\n",
        "        samples, seq_len, _ = q.shape\n",
        "        return q.view(samples, seq_len, self.num_heads, self.head_params).transpose(1,2)\n",
        "\n",
        "    def forward(self, q, mask=None):\n",
        "        k = q.clone()\n",
        "        v = q.clone()\n",
        "\n",
        "        # Define Query, Key, Value\n",
        "        Query = self.split_heads(self.W_q(q))\n",
        "        Key  = self.split_heads(self.W_k(k))\n",
        "        Value = self.split_heads(self.W_v(v))\n",
        "\n",
        "        # Run the self-attention\n",
        "        attention_scores, attn_output = self.self_attention(Query, Key,Value, mask)\n",
        "\n",
        "        # Concatenate the heads\n",
        "        batch_size, _,seq_len, _= attn_output.shape\n",
        "        attn_output = attn_output.transpose(1,2).contiguous().view(batch_size, seq_len, self.d_model) #shape = (#samples, len_seq, d_model)\n",
        "\n",
        "        # Apply through the last linear layer\n",
        "        result_final = self.W_o(attn_output) #shape = (#samples, len_seq, d_model)\n",
        "\n",
        "        return result_final\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_igrvH8q0eV0",
      "metadata": {
        "id": "_igrvH8q0eV0"
      },
      "source": [
        "#### Attention 2D Multi-Ped"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8X1EPrrYySne",
      "metadata": {
        "id": "8X1EPrrYySne"
      },
      "outputs": [],
      "source": [
        "class Attn2D_MultiPed(nn.Module):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,num_heads, d_model, stride = 10, block_size = 40):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_params = d_model//num_heads\n",
        "        self.stride = stride\n",
        "        self.block_size = block_size\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, d_model) # mapping to the same number of features\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def self_attention(self, q, k, v, mask):\n",
        "        # Calculate the dot produce of Q and K\n",
        "        dotqk = torch.matmul(q, k.transpose(-2,-1))/(self.head_params**0.5) # shape = (#samples, num_heads, len_seq, len_seq)\n",
        "        #print(f'shape of dot-product: {dotqk.shape}')\n",
        "        # Apply the mask if givne\n",
        "        if mask is not None:\n",
        "            dotqk = dotqk.masked_fill(mask == 0, -1e9)\n",
        "            #print(f'mask applied in the new sliding block multiped2d')\n",
        "        # Apply the Softmax to the attention weights\n",
        "        attention_scores = torch.softmax(dotqk, dim = -1)\n",
        "        # Multiply to the value matrix\n",
        "        result = torch.matmul(attention_scores, v)\n",
        "        return(result)\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        # Shape: (batch, seq_len, d_model) --> (batch, num_heads, seq_len, head_dim)\n",
        "        bsz, num_peds, len_peds, d_model = x.size()\n",
        "        return x.reshape(bsz, num_peds, len_peds, self.num_heads, self.head_params).transpose(2,3)\n",
        "\n",
        "    def _sliding_blocks(self, x):\n",
        "        \"\"\"\n",
        "        Create sliding-window blocks from the sequence.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): (B, L, D).\n",
        "\n",
        "        Returns:\n",
        "            Tensor: (B, num_blocks, block_size, D).\n",
        "        \"\"\"\n",
        "        l = self.block_size\n",
        "        d = self.stride\n",
        "        B, L, D = x.shape\n",
        "\n",
        "        num_blocks = (L - l) // d + 1\n",
        "\n",
        "        # Compute new shape and strides\n",
        "        shape = (B, num_blocks, l, D)\n",
        "        strides = (\n",
        "            x.stride(0),        # batch stride\n",
        "            d * x.stride(1),    # jump d steps for each block\n",
        "            x.stride(1),        # step 1 inside a block\n",
        "            x.stride(2),        # feature stride\n",
        "        )\n",
        "\n",
        "        blocks = x.as_strided(shape, strides)\n",
        "        return blocks.contiguous()\n",
        "\n",
        "    def _reconstruct_from_blocks(self, blocks, seq_len):\n",
        "        \"\"\"\n",
        "        Reconstruct sequence from overlapping sliding blocks.\n",
        "\n",
        "        Args:\n",
        "            blocks (Tensor): (B, num_blocks, block_size, D).\n",
        "            seq_len (int): Original sequence length.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: (B, seq_len, D).\n",
        "        \"\"\"\n",
        "        l = self.block_size\n",
        "        d = self.stride\n",
        "        B, num_blocks, _, D = blocks.shape\n",
        "\n",
        "        device = blocks.device\n",
        "\n",
        "        # Build index map: (num_blocks, block_size) -> positions in seq_len\n",
        "        start_idx = torch.arange(num_blocks, device=device) * d\n",
        "        block_offsets = torch.arange(l, device=device)\n",
        "        positions = start_idx[:, None] + block_offsets[None, :]  # (num_blocks, block_size)\n",
        "\n",
        "        # Expand to batch and feature dims\n",
        "        positions = positions.unsqueeze(0).expand(B, -1, -1)  # (B, num_blocks, block_size)\n",
        "\n",
        "        # Flatten everything for scatter\n",
        "        flat_pos = positions.reshape(B, -1)  # (B, num_blocks * block_size)\n",
        "        flat_blocks = blocks.reshape(B, -1, D)  # (B, num_blocks * block_size, D)\n",
        "\n",
        "        # Allocate output\n",
        "        out = torch.zeros(B, seq_len, D, device=device)\n",
        "        counts = torch.zeros(B, seq_len, 1, device=device)\n",
        "\n",
        "        # Scatter add the block values into the right positions\n",
        "        out.scatter_add_(1, flat_pos.unsqueeze(-1).expand(-1, -1, D), flat_blocks)\n",
        "        counts.scatter_add_(1, flat_pos.unsqueeze(-1), torch.ones_like(flat_pos, device=device).unsqueeze(-1).float())\n",
        "\n",
        "        return out / counts\n",
        "\n",
        "    def forward(self, q,mask = None):\n",
        "        k = q.clone()\n",
        "        v = q.clone()\n",
        "        # Define Query, Key, Value\n",
        "        Query = self.split_heads(self._sliding_blocks(self.W_q(q)))\n",
        "        Key  = self.split_heads(self._sliding_blocks(self.W_k(k)))\n",
        "        Value = self.split_heads(self._sliding_blocks(self.W_v(v)))\n",
        "\n",
        "        # Run the self-attention\n",
        "        attn_output = self.self_attention(Query, Key, Value, mask = mask)\n",
        "        #print(f'shape of the attn outputs:{attn_output.shape}')\n",
        "\n",
        "        batch_size, num_peds, num_heads, len_ped ,len_head = attn_output.shape\n",
        "        attn_output = attn_output.contiguous().view(batch_size, num_peds, len_ped, self.d_model)\n",
        "        #print(f'shape of attention output after combining heads {attn_output.shape}')\n",
        "\n",
        "        # Combine peds -> (batch_size, len_peds * num_peds, d_model)\n",
        "        attn_output = self._reconstruct_from_blocks(attn_output, q.shape[1])\n",
        "\n",
        "        # Apply through the last linear layer\n",
        "        result_final = self.W_o(attn_output) #shape = (#samples, len_seq, d_model)\n",
        "\n",
        "        return result_final"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EHOV5T0jxmUo",
      "metadata": {
        "id": "EHOV5T0jxmUo"
      },
      "source": [
        "#### 2D Attention Linformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SOLq8WYtnvVk",
      "metadata": {
        "id": "SOLq8WYtnvVk"
      },
      "outputs": [],
      "source": [
        "class Attn2DLinformer(nn.Module):\n",
        "    \"\"\"\n",
        "    Modified Multi-Head Attention with 3D product using E, F, G projections.\n",
        "\n",
        "    Args:\n",
        "        num_heads (int): Number of attention heads\n",
        "        d_model (int): Input dimension\n",
        "        k (int): Latent projection size for E, F, G\n",
        "        len_seq (int): Sequence length\n",
        "\n",
        "        Note that the length of the sequence must be inputted correctly here.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_heads, d_model, k, len_seq):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        self.head_dim = d_model // num_heads\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.E = nn.Linear(len_seq, k)\n",
        "        self.F = nn.Linear(len_seq, k)\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        B, L, D = x.size()\n",
        "        x = x.view(B, L, self.num_heads, self.head_dim)\n",
        "        return x.transpose(1, 2)  # [B, H, L, D]\n",
        "\n",
        "    def self_attention(self, q, k, v, mask):\n",
        "\n",
        "        # Calculate the dot produce of Q and K\n",
        "        dotqk = torch.matmul(q, k.transpose(-2,-1))/(self.head_dim**0.5) # shape = (#samples, num_heads, len_seq, len_seq)\n",
        "        # Apply the mask if givne\n",
        "        if mask is not None:\n",
        "            dotqk = dotqk.masked_fill(mask == 0, -1e9)\n",
        "            #print(f'mask applied in the linformer')\n",
        "        # Apply the Softmax to the attention weights\n",
        "        attention_scores = torch.softmax(dotqk, dim = -1)\n",
        "        # Multiply to the value matrix\n",
        "        result = torch.matmul(attention_scores, v)\n",
        "        return(result)\n",
        "\n",
        "\n",
        "    def forward(self, q,mask = None):\n",
        "        k = q.clone()\n",
        "        v = q.clone()\n",
        "        Q = self.split_heads(self.W_q(q))\n",
        "\n",
        "        def project(x, linear_proj, W):\n",
        "            x = W(x).permute(0, 2, 1)\n",
        "            x = linear_proj(x).permute(0, 2, 1)\n",
        "            return self.split_heads(x)\n",
        "\n",
        "        K = project(k, self.E, self.W_k)\n",
        "        V = project(v, self.F, self.W_v)\n",
        "\n",
        "        attn_out = self.self_attention(Q, K, V, mask)\n",
        "\n",
        "        B, H, L, D = attn_out.shape\n",
        "        concat = attn_out.transpose(1, 2).contiguous().view(B, L, self.d_model)\n",
        "        return self.W_o(concat)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OqBtEjdyAQuz",
      "metadata": {
        "id": "OqBtEjdyAQuz"
      },
      "source": [
        "#### 2D Linformer + Multi-Ped"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RGNznmsuAR4c",
      "metadata": {
        "id": "RGNznmsuAR4c"
      },
      "outputs": [],
      "source": [
        "class Attn2DLinformerMultiPed(nn.Module):\n",
        "    \"\"\"\n",
        "    Modified Multi-Head Attention with 3D product using E, F, G projections.\n",
        "\n",
        "    Args:\n",
        "        num_heads (int): Number of attention heads\n",
        "        d_model (int): Input dimension\n",
        "        k (int): Latent projection size for E, F, G\n",
        "        len_seq (int): Sequence length\n",
        "\n",
        "        Note that the length of the sequence must be inputted correctly here.\n",
        "        first multi-ped then linformer attention\n",
        "    \"\"\"\n",
        "    def __init__(self, num_heads, d_model, k, len_seq, num_peds):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        self.head_dim = d_model // num_heads\n",
        "        self.num_peds = num_peds\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.E = nn.Linear(len_seq//num_peds, k)\n",
        "        self.F = nn.Linear(len_seq//num_peds, k)\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        # Shape: (batch, seq_len, d_model) --> (batch, num_heads, seq_len, head_dim)\n",
        "        bsz, num_peds, len_peds, d_model = x.size()\n",
        "        return x.reshape(bsz, num_peds, len_peds, self.num_heads, self.head_dim).transpose(2,3)\n",
        "\n",
        "    def self_attention(self, q, k, v, mask= None):\n",
        "\n",
        "        # Calculate the dot produce of Q and K\n",
        "        dotqk = torch.matmul(q, k.transpose(-2,-1))/(self.head_dim**0.5) # shape = (#samples, num_heads, len_seq, len_seq)\n",
        "        # Apply the mask if givne\n",
        "        if mask is not None:\n",
        "            dotqk = dotqk.masked_fill(mask == 0, -1e9)\n",
        "            print(f'mask in implemented')\n",
        "            print(f'shape of the mask implemented: {mask.shape}')\n",
        "            print(f'shape of the attn: {dotqk.shape}')\n",
        "\n",
        "        # Apply the Softmax to the attention weights\n",
        "        attention_scores = torch.softmax(dotqk, dim = -1)\n",
        "        # Multiply to the value matrix\n",
        "        result = torch.matmul(attention_scores, v)\n",
        "        return(result)\n",
        "\n",
        "    def split_sequence(self, q):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            q (Tensor): Input tensor of shape (samples, num_heads, seq_len, d_model).\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Reshaped tensor of shape (samples, num_heads, num_peds, seq_len // num_peds, d_model).\n",
        "        \"\"\"\n",
        "\n",
        "        samples, seq_len, d_model = q.shape\n",
        "        assert seq_len % self.num_peds == 0, \"Sequence length is not divisible by number of peds\"\n",
        "        return q.reshape(samples, self.num_peds, seq_len // self.num_peds, d_model)\n",
        "\n",
        "\n",
        "    def forward(self, q,mask = None):\n",
        "        k = q.clone()\n",
        "        v = q.clone()\n",
        "        Q = self.split_heads(self.split_sequence(self.W_q(q)))\n",
        "\n",
        "        def project(x, linear_proj, W):\n",
        "            '''\n",
        "            Expected input shape: ([batch_size, len_seq, d_model])\n",
        "\n",
        "            '''\n",
        "            x = self.split_sequence(W(x)) #shape: ([batch_size, num_peds, len_ped, d_model])\n",
        "            x = x.permute(0, 1, 3, 2) #shape: ([batch_size, num_peds, d_model, len_ped])\n",
        "            #print(f'linear proj: {linear_proj}, shape of x: {x.shape}')\n",
        "            x = linear_proj(x) # shape: torch.Size([batch_size, num_peds, d_model, k])\n",
        "            x = x.permute(0, 1, 3, 2) # shape: torch.Size([batch_size, num_peds, k, d_model])\n",
        "            return self.split_heads((x)) # shape: torch.Size([batch_size, num_peds, num_heads, k , head_dim])\n",
        "\n",
        "        K = project(k, self.E, self.W_k)\n",
        "        V = project(v, self.F, self.W_v)\n",
        "\n",
        "        attn_output = self.self_attention(Q, K, V,mask = mask)\n",
        "\n",
        "        batch_size, num_peds, num_heads, len_ped ,len_head = attn_output.shape\n",
        "        attn_output = attn_output.contiguous().view(batch_size, num_peds, len_ped, self.d_model)\n",
        "        #print(f'shape of attention output after combining heads {attn_output.shape}')\n",
        "\n",
        "        # Combine peds -> (batch_size, len_peds * num_peds, d_model)\n",
        "        attn_output = attn_output.contiguous().view(batch_size, len_ped * num_peds, self.d_model)\n",
        "        #print(f'shape of attention output after combining the peds {attn_output.shape}')\n",
        "\n",
        "        # Apply through the last linear layer\n",
        "        result_final = self.W_o(attn_output) #shape = (#samples, len_seq, d_model)\n",
        "        #print(f'shape of final output: {result_final.shape}')\n",
        "\n",
        "        return result_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ArQvH3LTynO_",
      "metadata": {
        "id": "ArQvH3LTynO_"
      },
      "outputs": [],
      "source": [
        "class Attn2DLinformerMultiPed(nn.Module):\n",
        "    \"\"\"\n",
        "    Modified Multi-Head Attention with 3D product using E, F, G projections.\n",
        "\n",
        "    Args:\n",
        "        num_heads (int): Number of attention heads\n",
        "        d_model (int): Input dimension\n",
        "        k (int): Latent projection size for E, F, G\n",
        "        len_seq (int): Sequence length\n",
        "\n",
        "        Note that the length of the sequence must be inputted correctly here.\n",
        "        first multi-ped then linformer attention\n",
        "    \"\"\"\n",
        "    def __init__(self, num_heads, d_model, k, len_seq, stride, block_size):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        self.head_dim = d_model // num_heads\n",
        "        self.stride = stride\n",
        "        self.block_size = block_size\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.E = nn.Linear(block_size, k)\n",
        "        self.F = nn.Linear(block_size, k)\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        # Shape: (batch, seq_len, d_model) --> (batch, num_heads, seq_len, head_dim)\n",
        "        bsz, num_peds, len_peds, d_model = x.size()\n",
        "        return x.reshape(bsz, num_peds, len_peds, self.num_heads, self.head_dim).transpose(2,3)\n",
        "\n",
        "    def self_attention(self, q, k, v, mask):\n",
        "\n",
        "        # Calculate the dot produce of Q and K\n",
        "        dotqk = torch.matmul(q, k.transpose(-2,-1))/(self.head_dim**0.5) # shape = (#samples, num_heads, len_seq, len_seq)\n",
        "        #print(f'shape of dotqk: {dotqk.shape}')\n",
        "        # Apply the mask if givne\n",
        "        if mask is not None:\n",
        "            dotqk = dotqk.masked_fill(mask == 0, -1e9)\n",
        "            #print(f'shape of the mask: {mask.shape}')\n",
        "        # Apply the Softmax to the attention weights\n",
        "        attention_scores = torch.softmax(dotqk, dim = -1)\n",
        "        # Multiply to the value matrix\n",
        "        result = torch.matmul(attention_scores, v)\n",
        "        return(result)\n",
        "\n",
        "    def _sliding_blocks(self, x):\n",
        "        \"\"\"\n",
        "        Create sliding-window blocks from the sequence.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): (B, L, D).\n",
        "\n",
        "        Returns:\n",
        "            Tensor: (B, num_blocks, block_size, D).\n",
        "        \"\"\"\n",
        "        l = self.block_size\n",
        "        d = self.stride\n",
        "        B, L, D = x.shape\n",
        "\n",
        "        num_blocks = (L - l) // d + 1\n",
        "\n",
        "        # Compute new shape and strides\n",
        "        shape = (B, num_blocks, l, D)\n",
        "        strides = (\n",
        "            x.stride(0),        # batch stride\n",
        "            d * x.stride(1),    # jump d steps for each block\n",
        "            x.stride(1),        # step 1 inside a block\n",
        "            x.stride(2),        # feature stride\n",
        "        )\n",
        "\n",
        "        blocks = x.as_strided(shape, strides)\n",
        "        return blocks.contiguous()\n",
        "\n",
        "    def _reconstruct_from_blocks(self, blocks, seq_len):\n",
        "        \"\"\"\n",
        "        Reconstruct sequence from overlapping sliding blocks.\n",
        "\n",
        "        Args:\n",
        "            blocks (Tensor): (B, num_blocks, block_size, D).\n",
        "            seq_len (int): Original sequence length.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: (B, seq_len, D).\n",
        "        \"\"\"\n",
        "        l = self.block_size\n",
        "        d = self.stride\n",
        "        B, num_blocks, _, D = blocks.shape\n",
        "\n",
        "        device = blocks.device\n",
        "\n",
        "        # Build index map: (num_blocks, block_size) -> positions in seq_len\n",
        "        start_idx = torch.arange(num_blocks, device=device) * d\n",
        "        block_offsets = torch.arange(l, device=device)\n",
        "        positions = start_idx[:, None] + block_offsets[None, :]  # (num_blocks, block_size)\n",
        "\n",
        "        # Expand to batch and feature dims\n",
        "        positions = positions.unsqueeze(0).expand(B, -1, -1)  # (B, num_blocks, block_size)\n",
        "\n",
        "        # Flatten everything for scatter\n",
        "        flat_pos = positions.reshape(B, -1)  # (B, num_blocks * block_size)\n",
        "        flat_blocks = blocks.reshape(B, -1, D)  # (B, num_blocks * block_size, D)\n",
        "\n",
        "        # Allocate output\n",
        "        out = torch.zeros(B, seq_len, D, device=device)\n",
        "        counts = torch.zeros(B, seq_len, 1, device=device)\n",
        "\n",
        "        # Scatter add the block values into the right positions\n",
        "        out.scatter_add_(1, flat_pos.unsqueeze(-1).expand(-1, -1, D), flat_blocks)\n",
        "        counts.scatter_add_(1, flat_pos.unsqueeze(-1), torch.ones_like(flat_pos, device=device).unsqueeze(-1).float())\n",
        "\n",
        "        return out / counts\n",
        "\n",
        "    def forward(self, q,mask = None):\n",
        "        k = q.clone()\n",
        "        v = q.clone()\n",
        "        Q = self.split_heads(self._sliding_blocks(self.W_q(q)))\n",
        "        #print(f'shape of query: {Q.shape}')\n",
        "\n",
        "        def project(x, linear_proj, W):\n",
        "            '''\n",
        "            Expected input shape: ([batch_size, len_seq, d_model])\n",
        "\n",
        "            '''\n",
        "            x = self._sliding_blocks(W(x)) #shape: ([batch_size, num_peds, len_ped, d_model])\n",
        "            x = x.permute(0, 1, 3, 2) #shape: ([batch_size, num_peds, d_model, len_ped])\n",
        "            #print(f'linear proj: {linear_proj}, shape of x permuted: {x.shape}')\n",
        "            x = linear_proj(x) # shape: torch.Size([batch_size, num_peds, d_model, k])\n",
        "            x = x.permute(0, 1, 3, 2) # shape: torch.Size([batch_size, num_peds, k, d_model])\n",
        "            return self.split_heads((x)) # shape: torch.Size([batch_size, num_peds, num_heads, k , head_dim])\n",
        "\n",
        "        K = project(k, self.E, self.W_k)\n",
        "        #print(f'shape of key lin: {K.shape}')\n",
        "        V = project(v, self.F, self.W_v)\n",
        "\n",
        "        attn_output = self.self_attention(Q, K, V,mask)\n",
        "\n",
        "        batch_size, num_peds, num_heads, len_ped ,len_head = attn_output.shape\n",
        "        attn_output = attn_output.contiguous().view(batch_size, num_peds, len_ped, self.d_model)\n",
        "        #print(f'shape of attention output after combining heads {attn_output.shape}')\n",
        "\n",
        "        # Combine peds -> (batch_size, len_peds * num_peds, d_model)\n",
        "        attn_output = self._reconstruct_from_blocks(attn_output, q.shape[1])\n",
        "        #print(f'shape of attention output after combining the peds {attn_output.shape}')\n",
        "\n",
        "        # Apply through the last linear layer\n",
        "        result_final = self.W_o(attn_output) #shape = (#samples, len_seq, d_model)\n",
        "        #print(f'shape of final output: {result_final.shape}')\n",
        "\n",
        "        return result_final"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SvoLAqKi1l3y",
      "metadata": {
        "id": "SvoLAqKi1l3y"
      },
      "source": [
        "### 3D Attention Mechanisms"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "isKqf_0s67l3",
      "metadata": {
        "id": "isKqf_0s67l3"
      },
      "source": [
        "#### Combined 2D+ 3D Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Pbjv2UAD6xLb",
      "metadata": {
        "id": "Pbjv2UAD6xLb"
      },
      "outputs": [],
      "source": [
        "class Attn_2D3DC(nn.Module):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,num_heads, d_model):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_params = d_model//num_heads\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, d_model) # mapping to the same number of features\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_l = nn.Linear(d_model,d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(self.head_params)\n",
        "        self.norm2 = nn.LayerNorm(self.head_params)\n",
        "\n",
        "    def self_attention1(self, q, k, v, mask= None):\n",
        "        # Calculate the dot produce of Q and K\n",
        "        dotqk = torch.matmul(q, k.transpose(-2,-1))/(self.head_params**0.5) # shape = (#samples, num_heads, len_seq, len_seq)\n",
        "        #print(f'shape of dotqk: {dotqk.shape}')\n",
        "        if mask is not None:\n",
        "            #print(f'shape of the mask applied: {mask.shape}')\n",
        "            dotqk = dotqk.masked_fill(mask == 0, -1e9)\n",
        "            #print(f'mask applied')\n",
        "        # Apply the Softmax to the attention weights\n",
        "        attention_scores = torch.softmax(dotqk, dim = -1)\n",
        "        # Multiply to the value matrix\n",
        "        result = torch.matmul(attention_scores, v)\n",
        "        return(result)\n",
        "\n",
        "    def self_attention2(self, q, k,l, v,mask= None):\n",
        "        # Save the length of the sequence\n",
        "        #seq_len = k.shape[2]\n",
        "        # Calculate the dot produce of Q and K\n",
        "        mul_qkl = torch.einsum('abid,abjd,abkd -> abijk', q, k, l) /(self.head_params**0.5) # Shape: (#samples,num_heads,len_seq,len_seq, len_seq)\n",
        "        #print(f'shape of mul_qkl: {mul_qkl.shape}')\n",
        "        if mask is not None:\n",
        "            #print(f'shape of the mask received: {mask.shape}')\n",
        "            mul_qkl = mul_qkl.masked_fill(mask == 0, -1e9)\n",
        "            #print(f'mask applied')\n",
        "        # Apply the Softmax to the attention weights\n",
        "        attention_scores = softmax_5d(mul_qkl, axis = (-1,-2))\n",
        "        # Calculate the V^2 matrix\n",
        "        vtilde = torch.einsum('abcd,abed->abced', v, v)\n",
        "        # Multiply 3D attention with V^2\n",
        "        result = torch.einsum('abijk,abjkl->abil', attention_scores, vtilde)\n",
        "        return(result)\n",
        "\n",
        "    def combined_attention(self, q, k,l, v, masks):\n",
        "      \"\"\"\n",
        "      This is combining the original attention with the modified attention.\n",
        "      mask1 is the mask with the original dimensions\n",
        "      mask2 is the mask with the modified dimensions\n",
        "      \"\"\"\n",
        "      mask2d, mask3d = masks\n",
        "      result1_attention = self.self_attention1(q, k, v, mask2d)\n",
        "      result2_attention = self.self_attention2(q, k,l, v, mask3d)\n",
        "\n",
        "      return(self.norm1(result1_attention)+self.norm2(result2_attention)) #combined normalized results\n",
        "\n",
        "\n",
        "    def split_heads(self, q):\n",
        "\n",
        "        \"\"\"\n",
        "        Reshaping\n",
        "        Input of shape : (#samples, seq_len, d_model)\n",
        "\n",
        "        to\n",
        "        output of shape: (#samples, num_heads, seq_len, head_parameters)\n",
        "        \"\"\"\n",
        "        samples, seq_len, d_model = q.shape\n",
        "        # Check if dimensions are valid\n",
        "        assert d_model % self.num_heads == 0, \"d_model is not divisable by the number of heads\"\n",
        "        return q.view(samples, seq_len, self.num_heads, self.head_params).transpose(1,2)\n",
        "\n",
        "    def forward(self, q, masks = None):\n",
        "        k = q.clone()\n",
        "        l = q.clone()\n",
        "        v = q.clone()\n",
        "\n",
        "        # Define Query, Key, Value\n",
        "        Query = self.split_heads(self.W_q(q))\n",
        "        Key  = self.split_heads(self.W_k(k))\n",
        "        L_matrix = self.split_heads(self.W_l(l))\n",
        "        Value = self.split_heads(self.W_v(v))\n",
        "\n",
        "        # Run the self-attention\n",
        "        attn_outputs_combined = self.combined_attention(Query, Key,L_matrix, Value, masks)\n",
        "\n",
        "        # Concatenate the heads\n",
        "        batch_size, _,seq_len, _= attn_outputs_combined.shape\n",
        "        attn_outputs_combined = attn_outputs_combined.transpose(1,2).contiguous().view(batch_size, seq_len, self.d_model) #shape = (#samples, len_seq, d_model)\n",
        "\n",
        "        # Apply through the last linear layer\n",
        "        result_final = self.W_o(attn_outputs_combined) #shape = (#samples, len_seq, d_model)\n",
        "\n",
        "        return result_final"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EqH2H92sF2iq",
      "metadata": {
        "id": "EqH2H92sF2iq"
      },
      "source": [
        "#### Combined 2D + Linformer 3D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yF4JG_NHF7kS",
      "metadata": {
        "id": "yF4JG_NHF7kS"
      },
      "outputs": [],
      "source": [
        "class Attn2D3DLin(nn.Module):\n",
        "\n",
        "    \"\"\"\n",
        "    Combined 2D and 3D Linformer Attention\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,num_heads, d_model, k, len_seq):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_params = d_model//num_heads\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, d_model) # mapping to the same number of features\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_l = nn.Linear(d_model,d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # Linformer Projections\n",
        "        self.E = nn.Linear(len_seq, k)\n",
        "        self.F = nn.Linear(len_seq, k)\n",
        "        self.G = nn.Linear(len_seq, k)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(self.head_params)\n",
        "        self.norm2 = nn.LayerNorm(self.head_params)\n",
        "\n",
        "    def self_attention2D(self, q, k, v, mask= None):\n",
        "        # Calculate the dot produce of Q and K\n",
        "        dotqk = torch.matmul(q, k.transpose(-2,-1))/(self.head_params**0.5) # shape = (#samples, num_heads, len_seq, len_seq)\n",
        "        #print(f'shape of dotqk: {dotqk.shape}')\n",
        "        if mask is not None:\n",
        "            #print(f'shape of the mask applied: {mask.shape}')\n",
        "            dotqk = dotqk.masked_fill(mask == 0, -1e9)\n",
        "            #print(f'mask applied in 2D attn')\n",
        "        # Apply the Softmax to the attention weights\n",
        "        attention_scores = torch.softmax(dotqk, dim = -1)\n",
        "        # Multiply to the value matrix\n",
        "        result = torch.matmul(attention_scores, v)\n",
        "        return(result)\n",
        "\n",
        "    def self_attention3DLin(self, q, k, l, v, mask = None):\n",
        "        # Compute trilinear attention scores\n",
        "        scores = torch.einsum('bhid,bhjd,bhkd->bhijk', q, k, l) / (self.head_params ** 0.5)\n",
        "        #print(f'shape of scores: {scores.shape}')\n",
        "        if mask is not None:\n",
        "            #print(f'shape of the mask applied: {mask.shape}')\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "            #print(f'mask applied in 3D attn')\n",
        "        attn_weights = softmax_5d(scores, axis=(-1, -2))\n",
        "\n",
        "        v_3d = torch.einsum('bhld,bhLd->bhlLd', v, v)  # Could be simplified\n",
        "        output = torch.einsum('bhijk,bhjkl->bhil', attn_weights, v_3d)\n",
        "        return output\n",
        "\n",
        "\n",
        "    def split_heads(self, q):\n",
        "\n",
        "        \"\"\"\n",
        "        Reshaping\n",
        "        Input of shape : (#samples, seq_len, d_model)\n",
        "\n",
        "        output of shape: (#samples, num_heads, seq_len, head_parameters)\n",
        "        \"\"\"\n",
        "        samples, seq_len, d_model = q.shape\n",
        "        # Check if dimensions are valid\n",
        "        assert d_model % self.num_heads == 0, \"d_model is not divisable by the number of heads\"\n",
        "        return q.view(samples, seq_len, self.num_heads, self.head_params).transpose(1,2)\n",
        "\n",
        "    def forward(self, q, mask = None):\n",
        "        # Clone Query to define key, utinity, and value matrices\n",
        "        k = q.clone()\n",
        "        l = q.clone()\n",
        "        v = q.clone()\n",
        "\n",
        "        # Unwarp the masks\n",
        "        mask2D = mask[0]\n",
        "        mask3D = mask[1]\n",
        "\n",
        "        # Query will be the same for both attentions\n",
        "        Q = self.split_heads(self.W_q(q))\n",
        "\n",
        "        # Define Key and Value for 2D Attention\n",
        "        K_2D  = self.split_heads(self.W_k(k))\n",
        "        V_2D = self.split_heads(self.W_v(v))\n",
        "\n",
        "        # Run the 2D self-attention\n",
        "        attn_outputs_2D = self.self_attention2D(Q, K_2D, V_2D, mask2D)\n",
        "\n",
        "        def project(x, linear_proj, W):\n",
        "          x = W(x).permute(0, 2, 1)\n",
        "          x = linear_proj(x).permute(0, 2, 1)\n",
        "          return self.split_heads(x)\n",
        "\n",
        "        # Define the Query, Key, Utinity and Value matrices for Linformer 3D\n",
        "        K_Lin = project(k, self.E, self.W_k)\n",
        "        L_Lin = project(l, self.F, self.W_l)\n",
        "        V_Lin = project(v, self.G, self.W_v)\n",
        "\n",
        "        # Run Linformer 3D attention\n",
        "        attn_outputs_3DLin= self.self_attention3DLin(Q, K_Lin, L_Lin, V_Lin, mask3D)\n",
        "\n",
        "        # Combine the attentions\n",
        "        attn_outputs_combined = self.norm1(attn_outputs_2D) + self.norm2(attn_outputs_3DLin)\n",
        "\n",
        "        # Concatenate the heads\n",
        "        batch_size, _,seq_len, _= attn_outputs_combined.shape\n",
        "        attn_outputs_combined = attn_outputs_combined.transpose(1,2).contiguous().view(batch_size, seq_len, self.d_model) #shape = (#samples, len_seq, d_model)\n",
        "\n",
        "        # Apply through the last linear layer\n",
        "        result_final = self.W_o(attn_outputs_combined) #shape = (#samples, len_seq, d_model)\n",
        "\n",
        "        return result_final"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QJ_WvQiwTB-O",
      "metadata": {
        "id": "QJ_WvQiwTB-O"
      },
      "source": [
        "#### Alternate Combined 2D + 3D Linformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eM0RvXlyTAef",
      "metadata": {
        "id": "eM0RvXlyTAef"
      },
      "outputs": [],
      "source": [
        "class Attn2D3DLinAlter(nn.Module):\n",
        "\n",
        "    \"\"\"\n",
        "    Combined 2D and 3D Linformer Attention\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,num_heads, d_model, k, len_seq):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_params = d_model//num_heads\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, d_model) # mapping to the same number of features\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_l = nn.Linear(d_model,d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # Linformer Projections\n",
        "        self.E = nn.Linear(len_seq, k)\n",
        "        self.F = nn.Linear(len_seq, k)\n",
        "        self.G = nn.Linear(len_seq, k)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(self.head_params)\n",
        "        self.norm2 = nn.LayerNorm(self.head_params)\n",
        "\n",
        "        self.linear = nn.Linear(2*self.head_params,self.head_params)\n",
        "\n",
        "    def self_attention2D(self, q, k, v, mask= None):\n",
        "        # Calculate the dot produce of Q and K\n",
        "        dotqk = torch.matmul(q, k.transpose(-2,-1))/(self.head_params**0.5) # shape = (#samples, num_heads, len_seq, len_seq)\n",
        "        #print(f'shape of dotqk 2D: {dotqk.shape}')\n",
        "        if mask is not None:\n",
        "            #print(f'shape of the mask applied: {mask.shape}')\n",
        "            dotqk = dotqk.masked_fill(mask == 0, -1e9)\n",
        "            #print(f'mask applied in 2D attn')\n",
        "        # Apply the Softmax to the attention weights\n",
        "        attention_scores = torch.softmax(dotqk, dim = -1)\n",
        "        # Multiply to the value matrix\n",
        "        result = torch.matmul(attention_scores, v)\n",
        "        return(result)\n",
        "\n",
        "    def self_attention3DLin(self, q, k, l, v, mask = None):\n",
        "        # Compute trilinear attention scores\n",
        "        scores = torch.einsum('bhid,bhjd,bhkd->bhijk', q, k, l) / (self.head_params ** 0.5)\n",
        "        #print(f'shape of scores Linformer: {scores.shape}')\n",
        "        if mask is not None:\n",
        "            #print(f'shape of the mask applied: {mask.shape}')\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "            #print(f'mask applied in 3D attn')\n",
        "        attn_weights = softmax_5d(scores, axis=(-1, -2))\n",
        "\n",
        "        v_3d = torch.einsum('bhld,bhLd->bhlLd', v, v)  # Could be simplified\n",
        "        output = torch.einsum('bhijk,bhjkl->bhil', attn_weights, v_3d)\n",
        "        return output\n",
        "\n",
        "\n",
        "    def split_heads(self, q):\n",
        "\n",
        "        \"\"\"\n",
        "        Reshaping\n",
        "        Input of shape : (#samples, seq_len, d_model)\n",
        "\n",
        "        output of shape: (#samples, num_heads, seq_len, head_parameters)\n",
        "        \"\"\"\n",
        "        samples, seq_len, d_model = q.shape\n",
        "        # Check if dimensions are valid\n",
        "        assert d_model % self.num_heads == 0, \"d_model is not divisable by the number of heads\"\n",
        "        return q.view(samples, seq_len, self.num_heads, self.head_params).transpose(1,2)\n",
        "\n",
        "    def forward(self, q, mask = None):\n",
        "        # Clone Query to define key, utinity, and value matrices\n",
        "        k = q.clone()\n",
        "        l = q.clone()\n",
        "        v = q.clone()\n",
        "\n",
        "        # Unwarp the masks\n",
        "        mask2D = mask[0]\n",
        "        mask3D = mask[1]\n",
        "\n",
        "        # Query will be the same for both attentions\n",
        "        Q = self.split_heads(self.W_q(q))\n",
        "\n",
        "        # Define Key and Value for 2D Attention\n",
        "        K_2D  = self.split_heads(self.W_k(k))\n",
        "        V_2D = self.split_heads(self.W_v(v))\n",
        "\n",
        "        # Run the 2D self-attention\n",
        "        attn_outputs_2D = self.self_attention2D(Q, K_2D, V_2D, mask2D)\n",
        "\n",
        "        def project(x, linear_proj, W):\n",
        "          x = W(x).permute(0, 2, 1)\n",
        "          x = linear_proj(x).permute(0, 2, 1)\n",
        "          return self.split_heads(x)\n",
        "\n",
        "        # Define the Query, Key, Utinity and Value matrices for Linformer 3D\n",
        "        K_Lin = project(k, self.E, self.W_k)\n",
        "        L_Lin = project(l, self.F, self.W_l)\n",
        "        V_Lin = project(v, self.G, self.W_v)\n",
        "\n",
        "        # Run Linformer 3D attention\n",
        "        attn_outputs_3DLin= self.self_attention3DLin(Q, K_Lin, L_Lin, V_Lin, mask3D)\n",
        "\n",
        "        # Concatenate the two attentions\n",
        "        combined_attention = torch.cat((attn_outputs_2D, attn_outputs_3DLin), dim=-1)\n",
        "        #print(f'shape of combined attention before linear: {combined_attention.shape}')\n",
        "        combined_attention = self.linear(combined_attention)\n",
        "        #print(f'shape of combined attention after linear: {combined_attention.shape}')\n",
        "\n",
        "        # Concatenate the heads\n",
        "        batch_size, _,seq_len, _= combined_attention.shape\n",
        "        combined_attention = combined_attention.transpose(1,2).contiguous().view(batch_size, seq_len, self.d_model) #shape = (#samples, len_seq, d_model)\n",
        "\n",
        "        # Apply through the last linear layer\n",
        "        result_final = self.W_o(combined_attention) #shape = (#samples, len_seq, d_model)\n",
        "\n",
        "        return result_final"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DOcpiCk6dqaV",
      "metadata": {
        "id": "DOcpiCk6dqaV"
      },
      "source": [
        "#### Combined 2D and 3D Multiped Alternative"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WwRFeojZcRFZ",
      "metadata": {
        "id": "WwRFeojZcRFZ"
      },
      "outputs": [],
      "source": [
        "class Attn2D3D_MultiPedAlter(nn.Module):\n",
        "\n",
        "    \"\"\"\n",
        "    Combined 2D and 3D Multi-Ped Attention\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,num_heads, d_model, num_peds):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_params = d_model//num_heads\n",
        "        self.num_peds = num_peds\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, d_model) # mapping to the same number of features\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_l = nn.Linear(d_model,d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(self.head_params)\n",
        "        self.norm2 = nn.LayerNorm(self.head_params)\n",
        "\n",
        "        self.linear = nn.Linear(2*self.head_params,self.head_params)\n",
        "\n",
        "    def self_attention2D(self, q, k, v, mask = None):\n",
        "        # Calculate the dot produce of Q and K\n",
        "        dotqk = torch.matmul(q, k.transpose(-2,-1))/(self.head_params**0.5) # shape = (#samples, num_heads, len_seq, len_seq)\n",
        "        #print(f'shape of dotqk in 2D : {dotqk.shape}')\n",
        "        if mask is not None:\n",
        "            #print(f'shape of the mask applied: {mask.shape}')\n",
        "            dotqk = dotqk.masked_fill(mask == 0, -1e9)\n",
        "            #print(f'mask applied in 2D attn')\n",
        "        # Apply the Softmax to the attention weights\n",
        "        attention_scores = torch.softmax(dotqk, dim = -1)\n",
        "        # Multiply to the value matrix\n",
        "        result = torch.matmul(attention_scores, v)\n",
        "        return(result)\n",
        "\n",
        "    def self_attention3DMultiPed(self, q, k,l, v,mask = None):\n",
        "        # Save the length of the sequence\n",
        "        seq_len = k.shape[2]\n",
        "        # Calculate the dot produce of Q and K\n",
        "        mul_qkl = torch.einsum('abcid,abcjd,abckd -> abcijk', q, k, l) /(self.head_params**0.5)# Shape: (#samples,num_heads,len_seq,len_seq, len_seq)\n",
        "        #print(f'shape of mul_qkl in 3D MultiPed: {mul_qkl.shape}')\n",
        "        #print(f'shape of mul_qkl: {mul_qkl.shape}')\n",
        "        if mask is not None:\n",
        "            #print(f'shape of the mask applied: {mask.shape}')\n",
        "            mul_qkl = mul_qkl.masked_fill(mask == 0, -1e9)\n",
        "            #print(f'mask applied in 3D attn')\n",
        "            #print(f'mask applied to 3D multiped; shape of the mask: {mask.shape}')\n",
        "        # Apply the Softmax to the attention weights\n",
        "        attention_scores = softmax_5d(mul_qkl, axis = (-1,-2))\n",
        "        # Calculate V^2\n",
        "        vtilde = torch.einsum('abcil,abcjl->abcijl', v, v)\n",
        "        # Multiply to the value matrix\n",
        "        result = torch.einsum('abcijk,abcjkl->abcil', attention_scores, vtilde)\n",
        "        #print(f'shape of attention results: {result.shape}')\n",
        "        return(result)\n",
        "\n",
        "    def split_heads2D(self, q):\n",
        "\n",
        "        \"\"\"\n",
        "        Reshaping\n",
        "        Input of shape : (#samples, seq_len, d_model)\n",
        "\n",
        "        output of shape: (#samples, num_heads, seq_len, head_parameters)\n",
        "        \"\"\"\n",
        "        samples, seq_len, d_model = q.shape\n",
        "        # Check if dimensions are valid\n",
        "        assert d_model % self.num_heads == 0, \"d_model is not divisable by the number of heads\"\n",
        "        return q.view(samples, seq_len, self.num_heads, self.head_params).transpose(1,2)\n",
        "\n",
        "\n",
        "    def split_sequence(self, q):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            q (Tensor): Input tensor of shape (samples, num_heads, seq_len, d_model).\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Reshaped tensor of shape (samples, num_heads, num_peds, seq_len // num_peds, d_model).\n",
        "        \"\"\"\n",
        "\n",
        "        samples, seq_len, d_model = q.shape\n",
        "        assert seq_len % self.num_peds == 0, \"Sequence length is not divisible by number of peds\"\n",
        "        #print(f'shape of q: {q.shape}')\n",
        "\n",
        "        return q.reshape(samples, self.num_peds, seq_len // self.num_peds, d_model)\n",
        "\n",
        "    def split_heads3D(self, x):\n",
        "        # Shape: (batch, seq_len, d_model) --> (batch, num_heads, seq_len, head_dim)\n",
        "        bsz, num_peds, len_peds, d_model = x.size()\n",
        "        return x.reshape(bsz, num_peds, len_peds, self.num_heads, self.head_params).transpose(2,3)\n",
        "\n",
        "    def forward(self, q, mask = None):\n",
        "        #print(f'combined 2D and 3D multiped is activated')\n",
        "\n",
        "        # Clone Query to define key, utinity, and value matrices\n",
        "        k = q.clone()\n",
        "        l = q.clone()\n",
        "        v = q.clone()\n",
        "\n",
        "        mask_2D = None\n",
        "        mask_3DMP = None\n",
        "\n",
        "        # Unpack the masks\n",
        "        if mask is not None:\n",
        "          mask_2D = mask[0]\n",
        "          #print(f'mask_2D{mask_2D}, shape = {mask_2D.shape}')\n",
        "          mask_3DMP = mask[1]\n",
        "          #print(f'mask_3DMP{mask_3DMP}, shape = {mask_3DMP.shape}')\n",
        "\n",
        "        Query_2D = self.split_heads2D(self.W_q(q))\n",
        "        Key_2D  = self.split_heads2D(self.W_k(k))\n",
        "        Value_2D = self.split_heads2D(self.W_v(v))\n",
        "\n",
        "        # Run the 2D self-attention\n",
        "        attn_outputs_2D = self.self_attention2D(Query_2D, Key_2D, Value_2D, mask = mask_2D) # (batch_size, num_heads, len_seq, head_params)\n",
        "\n",
        "        # Define the Query, Key, Utinity and Value matrices for Multi-Ped 3D\n",
        "        Query_3D = self.split_heads3D(self.split_sequence(self.W_q(q)))\n",
        "        Key_3D  = self.split_heads3D(self.split_sequence(self.W_k(k)))\n",
        "        Value_3D = self.split_heads3D(self.split_sequence(self.W_v(v)))\n",
        "        L_matrix = self.split_heads3D(self.split_sequence(self.W_l(l)))\n",
        "\n",
        "        # Run MultiPed 3D attention\n",
        "        attn_outputs_3DMultiPed= self.self_attention3DMultiPed(Query_3D, Key_3D, L_matrix, Value_3D, mask = mask_3DMP) # (batch_size, num_peds, num_heads, len_ped, head_params)\n",
        "        #print(f'shape of attention 2D output: {attn_outputs_3DMultiPed.shape}')\n",
        "\n",
        "        # Combine the peds\n",
        "        batch_size, num_peds, num_heads, len_ped ,len_head = attn_outputs_3DMultiPed.shape\n",
        "        attn_outputs_3DMultiPed = attn_outputs_3DMultiPed.contiguous().view(batch_size, num_heads, num_peds*len_ped, len_head)\n",
        "        #print(f'shape of attention 3D multi-ped output after combining heads {attn_outputs_3DMultiPed.shape}')\n",
        "        # Concatenate the output of the attentions\n",
        "        attn_outputs_concat = torch.cat((attn_outputs_2D, attn_outputs_3DMultiPed), dim=-1)\n",
        "        #print(f'shape of attns concatednate : {attn_outputs_concat.shape}')\n",
        "\n",
        "        # Combine the attentions\n",
        "        attn_outputs_combined = self.linear(attn_outputs_concat)\n",
        "        #print(f'shape of attention 3D multi-ped output after combining heads {attn_outputs_combined.shape}')\n",
        "\n",
        "        # Concatenate the heads\n",
        "        batch_size, _,seq_len, _= attn_outputs_combined.shape\n",
        "        attn_outputs_combined = attn_outputs_combined.transpose(1,2).contiguous().view(batch_size, seq_len, self.d_model) #shape = (#samples, len_seq, d_model)\n",
        "\n",
        "        # Apply through the last linear layer\n",
        "        result_final = self.W_o(attn_outputs_combined) #shape = (#samples, len_seq, d_model)\n",
        "\n",
        "        return result_final"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JJusOvNIa8Bh",
      "metadata": {
        "id": "JJusOvNIa8Bh"
      },
      "source": [
        "#### Combined 2D + Multi-Ped 3D (global 2D + Local 3D)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yeFk5f0Aa7Rk",
      "metadata": {
        "id": "yeFk5f0Aa7Rk"
      },
      "outputs": [],
      "source": [
        "class Attn2D3D_MultiPed(nn.Module):\n",
        "\n",
        "    \"\"\"\n",
        "    Combined 2D and 3D Multi-Ped Attention\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,num_heads, d_model, num_peds):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_params = d_model//num_heads\n",
        "        self.num_peds = num_peds\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, d_model) # mapping to the same number of features\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_l = nn.Linear(d_model,d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(self.head_params)\n",
        "        self.norm2 = nn.LayerNorm(self.head_params)\n",
        "\n",
        "    def self_attention2D(self, q, k, v, mask = None):\n",
        "        # Calculate the dot produce of Q and K\n",
        "        dotqk = torch.matmul(q, k.transpose(-2,-1))/(self.head_params**0.5) # shape = (#samples, num_heads, len_seq, len_seq)\n",
        "        #print(f'shape of dotqk in 2D : {dotqk.shape}')\n",
        "        if mask is not None:\n",
        "            #print(f'shape of the mask applied: {mask.shape}')\n",
        "            dotqk = dotqk.masked_fill(mask == 0, -1e9)\n",
        "            #print(f'mask applied in 2D attn')\n",
        "        # Apply the Softmax to the attention weights\n",
        "        attention_scores = torch.softmax(dotqk, dim = -1)\n",
        "        # Multiply to the value matrix\n",
        "        result = torch.matmul(attention_scores, v)\n",
        "        return(result)\n",
        "\n",
        "    def self_attention3DMultiPed(self, q, k,l, v,mask = None):\n",
        "        # Save the length of the sequence\n",
        "        seq_len = k.shape[2]\n",
        "        # Calculate the dot produce of Q and K\n",
        "        mul_qkl = torch.einsum('abcid,abcjd,abckd -> abcijk', q, k, l) /(self.head_params**0.5)# Shape: (#samples,num_heads,len_seq,len_seq, len_seq)\n",
        "        #print(f'shape of mul_qkl in 3D MultiPed: {mul_qkl.shape}')\n",
        "        #print(f'shape of mul_qkl: {mul_qkl.shape}')\n",
        "        if mask is not None:\n",
        "            #print(f'shape of the mask applied: {mask.shape}')\n",
        "            mul_qkl = mul_qkl.masked_fill(mask == 0, -1e9)\n",
        "            #print(f'mask applied in 3D attn')\n",
        "            #print(f'mask applied to 3D multiped; shape of the mask: {mask.shape}')\n",
        "        # Apply the Softmax to the attention weights\n",
        "        attention_scores = softmax_5d(mul_qkl, axis = (-1,-2))\n",
        "        # Calculate V^2\n",
        "        vtilde = torch.einsum('abcil,abcjl->abcijl', v, v)\n",
        "        # Multiply to the value matrix\n",
        "        result = torch.einsum('abcijk,abcjkl->abcil', attention_scores, vtilde)\n",
        "        #print(f'shape of attention results: {result.shape}')\n",
        "        return(result)\n",
        "\n",
        "    def split_heads2D(self, q):\n",
        "\n",
        "        \"\"\"\n",
        "        Reshaping\n",
        "        Input of shape : (#samples, seq_len, d_model)\n",
        "\n",
        "        output of shape: (#samples, num_heads, seq_len, head_parameters)\n",
        "        \"\"\"\n",
        "        samples, seq_len, d_model = q.shape\n",
        "        # Check if dimensions are valid\n",
        "        assert d_model % self.num_heads == 0, \"d_model is not divisable by the number of heads\"\n",
        "        return q.view(samples, seq_len, self.num_heads, self.head_params).transpose(1,2)\n",
        "\n",
        "\n",
        "    def split_sequence(self, q):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            q (Tensor): Input tensor of shape (samples, num_heads, seq_len, d_model).\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Reshaped tensor of shape (samples, num_heads, num_peds, seq_len // num_peds, d_model).\n",
        "        \"\"\"\n",
        "\n",
        "        samples, seq_len, d_model = q.shape\n",
        "        assert seq_len % self.num_peds == 0, \"Sequence length is not divisible by number of peds\"\n",
        "        #print(f'shape of q: {q.shape}')\n",
        "\n",
        "        return q.reshape(samples, self.num_peds, seq_len // self.num_peds, d_model)\n",
        "\n",
        "    def split_heads3D(self, x):\n",
        "        # Shape: (batch, seq_len, d_model) --> (batch, num_heads, seq_len, head_dim)\n",
        "        bsz, num_peds, len_peds, d_model = x.size()\n",
        "        return x.reshape(bsz, num_peds, len_peds, self.num_heads, self.head_params).transpose(2,3)\n",
        "\n",
        "    def forward(self, q, mask = None):\n",
        "        #print(f'combined 2D and 3D multiped is activated')\n",
        "\n",
        "        # Clone Query to define key, utinity, and value matrices\n",
        "        k = q.clone()\n",
        "        l = q.clone()\n",
        "        v = q.clone()\n",
        "\n",
        "        mask_2D = None\n",
        "        mask_3DMP = None\n",
        "\n",
        "        # Unpack the masks\n",
        "        if mask is not None:\n",
        "          mask_2D = mask[0]\n",
        "          #print(f'mask_2D{mask_2D}, shape = {mask_2D.shape}')\n",
        "          mask_3DMP = mask[1]\n",
        "          #print(f'mask_3DMP{mask_3DMP}, shape = {mask_3DMP.shape}')\n",
        "\n",
        "        Query_2D = self.split_heads2D(self.W_q(q))\n",
        "        Key_2D  = self.split_heads2D(self.W_k(k))\n",
        "        Value_2D = self.split_heads2D(self.W_v(v))\n",
        "\n",
        "        # Run the 2D self-attention\n",
        "        attn_outputs_2D = self.self_attention2D(Query_2D, Key_2D, Value_2D, mask = mask_2D) # (batch_size, num_heads, len_seq, head_params)\n",
        "\n",
        "        # Define the Query, Key, Utinity and Value matrices for Multi-Ped 3D\n",
        "        Query_3D = self.split_heads3D(self.split_sequence(self.W_q(q)))\n",
        "        Key_3D  = self.split_heads3D(self.split_sequence(self.W_k(k)))\n",
        "        Value_3D = self.split_heads3D(self.split_sequence(self.W_v(v)))\n",
        "        L_matrix = self.split_heads3D(self.split_sequence(self.W_l(l)))\n",
        "\n",
        "        # Run MultiPed 3D attention\n",
        "        attn_outputs_3DMultiPed= self.self_attention3DMultiPed(Query_3D, Key_3D, L_matrix, Value_3D, mask = mask_3DMP) # (batch_size, num_peds, num_heads, len_ped, head_params)\n",
        "        #print(f'shape of attention 2D output: {attn_outputs_3DMultiPed.shape}')\n",
        "\n",
        "        # Combine the peds\n",
        "        batch_size, num_peds, num_heads, len_ped ,len_head = attn_outputs_3DMultiPed.shape\n",
        "        attn_outputs_3DMultiPed = attn_outputs_3DMultiPed.contiguous().view(batch_size, num_heads, num_peds*len_ped, len_head)\n",
        "        #print(f'shape of attention 3D multi-ped output after combining heads {attn_outputs_3DMultiPed.shape}')\n",
        "\n",
        "        # Combine the attentions\n",
        "        attn_outputs_combined = self.norm1(attn_outputs_2D) + self.norm2(attn_outputs_3DMultiPed)\n",
        "\n",
        "        # Concatenate the heads\n",
        "        batch_size, _,seq_len, _= attn_outputs_combined.shape\n",
        "        attn_outputs_combined = attn_outputs_combined.transpose(1,2).contiguous().view(batch_size, seq_len, self.d_model) #shape = (#samples, len_seq, d_model)\n",
        "\n",
        "        # Apply through the last linear layer\n",
        "        result_final = self.W_o(attn_outputs_combined) #shape = (#samples, len_seq, d_model)\n",
        "\n",
        "        return result_final"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dual 3D Attns"
      ],
      "metadata": {
        "id": "CcF2EtAlN-Yc"
      },
      "id": "CcF2EtAlN-Yc"
    },
    {
      "cell_type": "code",
      "source": [
        "class Attn3D_MultiPed_Dual(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-ped 3D Attention with two parallel attention modules\n",
        "    whose outputs are summed together.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, d_model, block_size, stride):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_params = d_model // num_heads\n",
        "        self.block_size = block_size\n",
        "        self.stride = stride\n",
        "\n",
        "        # First attention set\n",
        "        self.W_q1 = nn.Linear(d_model, d_model)\n",
        "        self.W_k1 = nn.Linear(d_model, d_model)\n",
        "        self.W_l1 = nn.Linear(d_model, d_model)\n",
        "        self.W_v1 = nn.Linear(d_model, d_model)\n",
        "        self.W_o1 = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # Second attention set\n",
        "        self.W_q2 = nn.Linear(d_model, d_model)\n",
        "        self.W_k2 = nn.Linear(d_model, d_model)\n",
        "        self.W_l2 = nn.Linear(d_model, d_model)\n",
        "        self.W_v2 = nn.Linear(d_model, d_model)\n",
        "        self.W_o2 = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def self_attention(self, q, k, l, v, mask=None):\n",
        "        mul_qkl = torch.einsum(\n",
        "            'abcid,abcjd,abckd -> abcijk', q, k, l\n",
        "        ) / (self.head_params ** 0.5)\n",
        "\n",
        "        if mask is not None:\n",
        "            mul_qkl = mul_qkl.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        attention_scores = softmax_5d(mul_qkl, axis=(-1, -2))\n",
        "\n",
        "        vtilde = torch.einsum('abcil,abcjl->abcijl', v, v)\n",
        "        result = torch.einsum('abcijk,abcjkl->abcil', attention_scores, vtilde)\n",
        "        return result\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        bsz, num_peds, len_peds, d_model = x.size()\n",
        "        return x.reshape(bsz, num_peds, len_peds, self.num_heads, self.head_params).transpose(2, 3)\n",
        "\n",
        "    def _sliding_blocks(self, x):\n",
        "        l = self.block_size\n",
        "        d = self.stride\n",
        "        B, L, D = x.shape\n",
        "        num_blocks = (L - l) // d + 1\n",
        "\n",
        "        shape = (B, num_blocks, l, D)\n",
        "        strides = (\n",
        "            x.stride(0),\n",
        "            d * x.stride(1),\n",
        "            x.stride(1),\n",
        "            x.stride(2),\n",
        "        )\n",
        "\n",
        "        blocks = x.as_strided(shape, strides)\n",
        "        return blocks.contiguous()\n",
        "\n",
        "    def _reconstruct_from_blocks(self, blocks, seq_len):\n",
        "        l = self.block_size\n",
        "        d = self.stride\n",
        "        B, num_blocks, _, D = blocks.shape\n",
        "        device = blocks.device\n",
        "\n",
        "        start_idx = torch.arange(num_blocks, device=device) * d\n",
        "        block_offsets = torch.arange(l, device=device)\n",
        "        positions = start_idx[:, None] + block_offsets[None, :]  # (num_blocks, block_size)\n",
        "\n",
        "        positions = positions.unsqueeze(0).expand(B, -1, -1)  # (B, num_blocks, block_size)\n",
        "        flat_pos = positions.reshape(B, -1)\n",
        "        flat_blocks = blocks.reshape(B, -1, D)\n",
        "\n",
        "        out = torch.zeros(B, seq_len, D, device=device)\n",
        "        counts = torch.zeros(B, seq_len, 1, device=device)\n",
        "\n",
        "        out.scatter_add_(1, flat_pos.unsqueeze(-1).expand(-1, -1, D), flat_blocks)\n",
        "        counts.scatter_add_(1, flat_pos.unsqueeze(-1), torch.ones_like(flat_pos, device=device).unsqueeze(-1).float())\n",
        "\n",
        "        return out / counts\n",
        "\n",
        "    def _run_attention_set(self, q, W_q, W_k, W_l, W_v, W_o, mask=None):\n",
        "        k = q.clone()\n",
        "        l = q.clone()\n",
        "        v = q.clone()\n",
        "\n",
        "        Query = self.split_heads(self._sliding_blocks(W_q(q)))\n",
        "        Key = self.split_heads(self._sliding_blocks(W_k(k)))\n",
        "        L_matrix = self.split_heads(self._sliding_blocks(W_l(l)))\n",
        "        Value = self.split_heads(self._sliding_blocks(W_v(v)))\n",
        "\n",
        "        attn_output = self.self_attention(Query, Key, L_matrix, Value, mask=mask)\n",
        "\n",
        "        batch_size, num_peds, num_heads, len_ped, len_head = attn_output.shape\n",
        "        attn_output = attn_output.contiguous().view(batch_size, num_peds, len_ped, self.d_model)\n",
        "\n",
        "        len_seq = q.shape[1]\n",
        "        attn_output = self._reconstruct_from_blocks(attn_output, len_seq)\n",
        "\n",
        "        return W_o(attn_output)\n",
        "\n",
        "    def forward(self, q, mask=None):\n",
        "        # Run first attention set\n",
        "        out1 = self._run_attention_set(q, self.W_q1, self.W_k1, self.W_l1, self.W_v1, self.W_o1, mask)\n",
        "\n",
        "        # Run second attention set\n",
        "        out2 = self._run_attention_set(q, self.W_q2, self.W_k2, self.W_l2, self.W_v2, self.W_o2, mask)\n",
        "\n",
        "        # Add results\n",
        "        return out1 + out2\n"
      ],
      "metadata": {
        "id": "RF6bFHVcN-Ye"
      },
      "execution_count": null,
      "outputs": [],
      "id": "RF6bFHVcN-Ye"
    },
    {
      "cell_type": "code",
      "source": [
        "class Attn3DLinformer_Dual(nn.Module):\n",
        "    \"\"\"\n",
        "    Dual Attention 3D Linformer:\n",
        "    Runs two independent attention mechanisms (with separate weights)\n",
        "    and sums their outputs.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_heads, d_model, k, len_seq):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        self.head_dim = d_model // num_heads\n",
        "\n",
        "        # First attention weights\n",
        "        self.W_q1 = nn.Linear(d_model, d_model)\n",
        "        self.W_k1 = nn.Linear(d_model, d_model)\n",
        "        self.W_l1 = nn.Linear(d_model, d_model)\n",
        "        self.W_v1 = nn.Linear(d_model, d_model)\n",
        "        self.W_o1 = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.E1 = nn.Linear(len_seq, k)\n",
        "        self.F1 = nn.Linear(len_seq, k)\n",
        "        self.G1 = nn.Linear(len_seq, k)\n",
        "\n",
        "        # Second attention weights\n",
        "        self.W_q2 = nn.Linear(d_model, d_model)\n",
        "        self.W_k2 = nn.Linear(d_model, d_model)\n",
        "        self.W_l2 = nn.Linear(d_model, d_model)\n",
        "        self.W_v2 = nn.Linear(d_model, d_model)\n",
        "        self.W_o2 = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.E2 = nn.Linear(len_seq, k)\n",
        "        self.F2 = nn.Linear(len_seq, k)\n",
        "        self.G2 = nn.Linear(len_seq, k)\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        B, L, D = x.size()\n",
        "        x = x.view(B, L, self.num_heads, self.head_dim)\n",
        "        return x.transpose(1, 2)  # [B, H, L, D]\n",
        "\n",
        "    def self_attention(self, q, k, l, v, mask):\n",
        "        scores = torch.einsum('bhid,bhjd,bhkd->bhijk', q, k, l) / (self.head_dim ** 0.5)\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "        attn_weights = softmax_5d(scores, axis=(-1, -2))\n",
        "\n",
        "        v_3d = torch.einsum('bhld,bhLd->bhlLd', v, v)\n",
        "        output = torch.einsum('bhijk,bhjkl->bhil', attn_weights, v_3d)\n",
        "        return output\n",
        "\n",
        "    def forward_path(self, q, k, l, v, W_q, W_k, W_l, W_v, W_o, E, F, G, mask):\n",
        "        Q = self.split_heads(W_q(q))\n",
        "\n",
        "        def project(x, linear_proj, W):\n",
        "            x = W(x).permute(0, 2, 1)\n",
        "            x = linear_proj(x).permute(0, 2, 1)\n",
        "            return self.split_heads(x)\n",
        "\n",
        "        K = project(k, E, W_k)\n",
        "        L = project(l, F, W_l)\n",
        "        V = project(v, G, W_v)\n",
        "\n",
        "        attn_out = self.self_attention(Q, K, L, V, mask)\n",
        "        B, H, L_seq, D = attn_out.shape\n",
        "        concat = attn_out.transpose(1, 2).contiguous().view(B, L_seq, self.d_model)\n",
        "        return W_o(concat)\n",
        "\n",
        "    def forward(self, q, mask=None):\n",
        "        k, l, v = q.clone(), q.clone(), q.clone()\n",
        "\n",
        "        out1 = self.forward_path(q, k, l, v, self.W_q1, self.W_k1, self.W_l1, self.W_v1, self.W_o1, self.E1, self.F1, self.G1, mask)\n",
        "        out2 = self.forward_path(q, k, l, v, self.W_q2, self.W_k2, self.W_l2, self.W_v2, self.W_o2, self.E2, self.F2, self.G2, mask)\n",
        "\n",
        "        return out1 + out2\n"
      ],
      "metadata": {
        "id": "r-Aryd7zN-Ye"
      },
      "execution_count": null,
      "outputs": [],
      "id": "r-Aryd7zN-Ye"
    },
    {
      "cell_type": "code",
      "source": [
        "class Attn3DLinformerMultiPed_Dual(nn.Module):\n",
        "    \"\"\"\n",
        "    Dual Attention version of MultiPed + Linformer 3D attention.\n",
        "    Runs two independent attention mechanisms (with separate weights)\n",
        "    and sums their outputs.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_heads, d_model, k, len_seq, block_size, stride):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        self.head_dim = d_model // num_heads\n",
        "        self.block_size = block_size\n",
        "        self.stride = stride\n",
        "\n",
        "        # First attention weights\n",
        "        self.W_q1 = nn.Linear(d_model, d_model)\n",
        "        self.W_k1 = nn.Linear(d_model, d_model)\n",
        "        self.W_l1 = nn.Linear(d_model, d_model)\n",
        "        self.W_v1 = nn.Linear(d_model, d_model)\n",
        "        self.W_o1 = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.E1 = nn.Linear(block_size, k)\n",
        "        self.F1 = nn.Linear(block_size, k)\n",
        "        self.G1 = nn.Linear(block_size, k)\n",
        "\n",
        "        # Second attention weights\n",
        "        self.W_q2 = nn.Linear(d_model, d_model)\n",
        "        self.W_k2 = nn.Linear(d_model, d_model)\n",
        "        self.W_l2 = nn.Linear(d_model, d_model)\n",
        "        self.W_v2 = nn.Linear(d_model, d_model)\n",
        "        self.W_o2 = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.E2 = nn.Linear(block_size, k)\n",
        "        self.F2 = nn.Linear(block_size, k)\n",
        "        self.G2 = nn.Linear(block_size, k)\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        bsz, num_peds, len_peds, d_model = x.size()\n",
        "        return x.reshape(bsz, num_peds, len_peds, self.num_heads, self.head_dim).transpose(2, 3)\n",
        "\n",
        "    def self_attention(self, q, k, l, v, mask=None):\n",
        "        mul_qkl = torch.einsum('abcid,abcjd,abckd -> abcijk', q, k, l) /(self.head_dim**0.5)\n",
        "        if mask is not None:\n",
        "            mul_qkl = mul_qkl.masked_fill(mask == 0, -1e9)\n",
        "        attention_scores = softmax_5d(mul_qkl, axis=(-1, -2))\n",
        "        vtilde = torch.einsum('abcid,abcjd->abcijd', v, v)\n",
        "        result = torch.einsum('abcijk,abcjkl->abcil', attention_scores, vtilde)\n",
        "        return result\n",
        "\n",
        "    def _sliding_blocks(self, x):\n",
        "        l = self.block_size\n",
        "        d = self.stride\n",
        "        B, L, D = x.shape\n",
        "        num_blocks = (L - l) // d + 1\n",
        "        shape = (B, num_blocks, l, D)\n",
        "        strides = (x.stride(0), d * x.stride(1), x.stride(1), x.stride(2))\n",
        "        blocks = x.as_strided(shape, strides)\n",
        "        return blocks.contiguous()\n",
        "\n",
        "    def _reconstruct_from_blocks(self, blocks, seq_len):\n",
        "        l = self.block_size\n",
        "        d = self.stride\n",
        "        B, num_blocks, _, D = blocks.shape\n",
        "        device = blocks.device\n",
        "\n",
        "        start_idx = torch.arange(num_blocks, device=device) * d\n",
        "        block_offsets = torch.arange(l, device=device)\n",
        "        positions = start_idx[:, None] + block_offsets[None, :]\n",
        "        positions = positions.unsqueeze(0).expand(B, -1, -1)\n",
        "\n",
        "        flat_pos = positions.reshape(B, -1)\n",
        "        flat_blocks = blocks.reshape(B, -1, D)\n",
        "\n",
        "        out = torch.zeros(B, seq_len, D, device=device)\n",
        "        counts = torch.zeros(B, seq_len, 1, device=device)\n",
        "\n",
        "        out.scatter_add_(1, flat_pos.unsqueeze(-1).expand(-1, -1, D), flat_blocks)\n",
        "        counts.scatter_add_(1, flat_pos.unsqueeze(-1), torch.ones_like(flat_pos, device=device).unsqueeze(-1).float())\n",
        "\n",
        "        return out / counts\n",
        "\n",
        "    def forward_path(self, q, W_q, W_k, W_l, W_v, W_o, E, F, G, mask):\n",
        "        k, l, v = q.clone(), q.clone(), q.clone()\n",
        "        Q = self.split_heads(self._sliding_blocks(W_q(q)))\n",
        "\n",
        "        def project(x, linear_proj, W):\n",
        "            x = self._sliding_blocks(W(x))\n",
        "            x = x.permute(0, 1, 3, 2)\n",
        "            x = linear_proj(x)\n",
        "            x = x.permute(0, 1, 3, 2)\n",
        "            return self.split_heads(x)\n",
        "\n",
        "        K = project(k, E, W_k)\n",
        "        L = project(l, F, W_l)\n",
        "        V = project(v, G, W_v)\n",
        "\n",
        "        attn_output = self.self_attention(Q, K, L, V, mask)\n",
        "        batch_size, num_peds, num_heads, len_ped, len_head = attn_output.shape\n",
        "        attn_output = attn_output.contiguous().view(batch_size, num_peds, len_ped, self.d_model)\n",
        "        attn_output = self._reconstruct_from_blocks(attn_output, q.shape[1])\n",
        "        return W_o(attn_output)\n",
        "\n",
        "    def forward(self, q, mask=None):\n",
        "        out1 = self.forward_path(q, self.W_q1, self.W_k1, self.W_l1, self.W_v1, self.W_o1, self.E1, self.F1, self.G1, mask)\n",
        "        out2 = self.forward_path(q, self.W_q2, self.W_k2, self.W_l2, self.W_v2, self.W_o2, self.E2, self.F2, self.G2, mask)\n",
        "        return out1 + out2\n"
      ],
      "metadata": {
        "id": "CDxv4ZtgN-Ye"
      },
      "execution_count": null,
      "outputs": [],
      "id": "CDxv4ZtgN-Ye"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Attn2DMultiPed3Dselected_Dual(nn.Module):\n",
        "    \"\"\"\n",
        "    Two completely separate Attention Mechanisms with 2D and selected 3D interactions.\n",
        "    Each branch has its own Q, K, V, L projections.\n",
        "    The outputs of the two branches are added together.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_heads: int, d_model: int, stride: int = 10, block_size: int = 40, k=4, k_prime=2):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "        self.stride = stride\n",
        "        self.block_size = block_size\n",
        "        self.k = k\n",
        "        self.k_prime = k_prime\n",
        "\n",
        "        # Branch 1 projections\n",
        "        self.W_q1 = nn.Linear(d_model, d_model)\n",
        "        self.W_k1 = nn.Linear(d_model, d_model)\n",
        "        self.W_v1 = nn.Linear(d_model, d_model)\n",
        "        self.W_l1 = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # Branch 2 projections\n",
        "        self.W_q2 = nn.Linear(d_model, d_model)\n",
        "        self.W_k2 = nn.Linear(d_model, d_model)\n",
        "        self.W_v2 = nn.Linear(d_model, d_model)\n",
        "        self.W_l2 = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # Output projection (shared after adding the two branches)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # Fusion layer (per-branch fusion of 2D and 3D outputs)\n",
        "        self.fusion_layer = nn.Linear(2 * self.head_dim, self.head_dim)\n",
        "\n",
        "    #### Helper functions (same as before) ####\n",
        "    def _split_heads(self, x):\n",
        "        B, Blk, L, D = x.size()\n",
        "        return x.view(B, Blk, L, self.num_heads, self.head_dim).transpose(2, 3)\n",
        "\n",
        "    def _sliding_blocks(self, x):\n",
        "        l, d = self.block_size, self.stride\n",
        "        B, L, D = x.shape\n",
        "        num_blocks = (L - l) // d + 1\n",
        "        shape = (B, num_blocks, l, D)\n",
        "        strides = (x.stride(0), d * x.stride(1), x.stride(1), x.stride(2))\n",
        "        return x.as_strided(shape, strides).contiguous()\n",
        "\n",
        "    def _topk_submats_adv(self, attn_scores: torch.Tensor, topk_idx: torch.Tensor):\n",
        "        B, Blk, H, L, _ = attn_scores.shape\n",
        "        K = topk_idx.shape[-1]\n",
        "        device = attn_scores.device\n",
        "        topk_idx = topk_idx.long().to(device)\n",
        "\n",
        "        # Select rows\n",
        "        b = torch.arange(B, device=device)[:, None, None, None, None].expand(B, Blk, H, L, K)\n",
        "        bl = torch.arange(Blk, device=device)[None, :, None, None, None].expand(B, Blk, H, L, K)\n",
        "        h = torch.arange(H, device=device)[None, None, :, None, None].expand(B, Blk, H, L, K)\n",
        "        q = torch.arange(L, device=device)[None, None, None, :, None].expand(B, Blk, H, L, K)\n",
        "        row_selected = attn_scores[b, bl, h, topk_idx, :]  # (B, Blk, H, L, K, L)\n",
        "\n",
        "        # Select cols\n",
        "        b2 = torch.arange(B, device=device)[:, None, None, None, None, None].expand(B, Blk, H, L, K, K)\n",
        "        bl2 = torch.arange(Blk, device=device)[None, :, None, None, None, None].expand(B, Blk, H, L, K, K)\n",
        "        h2 = torch.arange(H, device=device)[None, None, :, None, None, None].expand(B, Blk, H, L, K, K)\n",
        "        q2 = torch.arange(L, device=device)[None, None, None, :, None, None].expand(B, Blk, H, L, K, K)\n",
        "        cols = topk_idx.unsqueeze(-2).expand(B, Blk, H, L, K, K)\n",
        "\n",
        "        sub = row_selected[b2, bl2, h2, q2, torch.arange(K, device=device)[None, None, None, None, :, None], cols]\n",
        "        return sub\n",
        "\n",
        "    def _reconstruct_from_blocks(self, blocks, seq_len):\n",
        "        l, d = self.block_size, self.stride\n",
        "        B, num_blocks, _, D = blocks.shape\n",
        "        device = blocks.device\n",
        "        start_idx = torch.arange(num_blocks, device=device) * d\n",
        "        block_offsets = torch.arange(l, device=device)\n",
        "        positions = (start_idx[:, None] + block_offsets).unsqueeze(0).expand(B, -1, -1)\n",
        "        flat_pos, flat_blocks = positions.reshape(B, -1), blocks.reshape(B, -1, D)\n",
        "        out = torch.zeros(B, seq_len, D, device=device)\n",
        "        counts = torch.zeros(B, seq_len, 1, device=device)\n",
        "        out.scatter_add_(1, flat_pos.unsqueeze(-1).expand(-1, -1, D), flat_blocks)\n",
        "        counts.scatter_add_(1, flat_pos.unsqueeze(-1), torch.ones_like(flat_pos, device=device).unsqueeze(-1).float())\n",
        "        return out / counts\n",
        "\n",
        "    def _gather_partners(self, tensor, partner_idx):\n",
        "        return torch.gather(tensor, 4, partner_idx.unsqueeze(-1).expand(*partner_idx.shape, self.head_dim))\n",
        "\n",
        "    def _self_attention_2d(self, q, k, v, mask=None):\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask.unsqueeze(2).unsqueeze(4) == 0, -1e9)\n",
        "        attn_scores = torch.softmax(scores, dim=-1)\n",
        "        return attn_scores, torch.matmul(attn_scores, v)\n",
        "\n",
        "    def compute_selected_mask(self, mask, partner1, partner2, L):\n",
        "        if mask is None:\n",
        "            return None\n",
        "        mask_reshaped1 = mask.unsqueeze(2).expand(-1, -1, self.num_heads, -1)\n",
        "        mask_reshaped2 = mask_reshaped1.unsqueeze(-2).expand(-1, -1, -1, L, -1)\n",
        "        mask1_comp = torch.gather(mask_reshaped2, 4, partner1)\n",
        "        mask2_comp = torch.gather(mask_reshaped2, 4, partner2)\n",
        "        two_way_interac_mask = mask1_comp * mask2_comp\n",
        "        mask_comp3 = mask_reshaped1.unsqueeze(-1).expand(-1, -1, -1, -1, partner1.size(-1))\n",
        "        return mask_comp3 * two_way_interac_mask\n",
        "\n",
        "    #### Branch-specific computation ####\n",
        "    def _branch_attention(self, Q, Key, V, L_mat, mask):\n",
        "        attn_scores, attn_out_2d = self._self_attention_2d(Q, Key, V, mask=mask)\n",
        "\n",
        "        topk_scores, topk_idx = torch.topk(attn_scores, self.k, dim=-1)\n",
        "        outer = topk_scores[..., :, None] * topk_scores[..., None, :]\n",
        "        selected = self._topk_submats_adv(attn_scores, topk_idx)\n",
        "        res = outer * selected\n",
        "        B, Blk, H, L, K, _ = res.shape\n",
        "        _, topk_exp = torch.topk(res.view(B, Blk, H, L, -1), self.k_prime, dim=-1)\n",
        "        row_idx, col_idx = topk_exp // self.k, topk_exp % self.k_prime\n",
        "        partner1, partner2 = torch.gather(topk_idx, -1, row_idx), torch.gather(topk_idx, -1, col_idx)\n",
        "\n",
        "        K_exp = Key.unsqueeze(-3).expand(-1, -1, -1, L, -1, -1)\n",
        "        L_exp = L_mat.unsqueeze(-3).expand(-1, -1, -1, L, -1, -1)\n",
        "        V_exp = V.unsqueeze(-3).expand(-1, -1, -1, L, -1, -1)\n",
        "\n",
        "        k_sel = self._gather_partners(K_exp, partner1)\n",
        "        l_sel = self._gather_partners(L_exp, partner2)\n",
        "        v1 = self._gather_partners(V_exp, partner1)\n",
        "        v2 = self._gather_partners(V_exp, partner2)\n",
        "\n",
        "        q_exp = Q.unsqueeze(-2)\n",
        "        dot_qkl = (q_exp * k_sel * l_sel).sum(-1)\n",
        "        final_mask = self.compute_selected_mask(mask, partner1, partner2, L)\n",
        "        if final_mask is not None:\n",
        "            dot_qkl = dot_qkl.masked_fill(final_mask == 0, -1e9)\n",
        "\n",
        "        attn_weights_3d = torch.softmax(dot_qkl, dim=-1)\n",
        "        v_sq = v1 * v2\n",
        "        dot_qkl_exp = attn_weights_3d.unsqueeze(-1).expand(-1, -1, -1, -1, -1, self.head_dim)\n",
        "        res_3d = (dot_qkl_exp * v_sq).sum(-2)\n",
        "\n",
        "        res_combined = torch.cat([attn_out_2d, res_3d], dim=-1)\n",
        "        return self.fusion_layer(res_combined)\n",
        "\n",
        "    #### Forward ####\n",
        "    def forward(self, x, mask=None):\n",
        "        B, L, D = x.shape\n",
        "\n",
        "        # Branch 1\n",
        "        q1, k1, v1, l1 = self.W_q1(x), self.W_k1(x), self.W_v1(x), self.W_l1(x)\n",
        "        Q1, K1, V1, L1 = map(lambda t: self._split_heads(self._sliding_blocks(t)), (q1, k1, v1, l1))\n",
        "        out1 = self._branch_attention(Q1, K1, V1, L1, mask)\n",
        "\n",
        "        # Branch 2\n",
        "        q2, k2, v2, l2 = self.W_q2(x), self.W_k2(x), self.W_v2(x), self.W_l2(x)\n",
        "        Q2, K2, V2, L2 = map(lambda t: self._split_heads(self._sliding_blocks(t)), (q2, k2, v2, l2))\n",
        "        out2 = self._branch_attention(Q2, K2, V2, L2, mask)\n",
        "\n",
        "        # Add branches\n",
        "        summed = out1 + out2\n",
        "        B, Blk, H, L_blk, hd = summed.shape\n",
        "        out = summed.transpose(2, 3).contiguous().view(B, Blk, L_blk, self.d_model)\n",
        "\n",
        "        # Reconstruct and project\n",
        "        recon = self._reconstruct_from_blocks(out, x.shape[1])\n",
        "        return self.W_o(recon)\n"
      ],
      "metadata": {
        "id": "m9BndUHkN-Yf"
      },
      "execution_count": null,
      "outputs": [],
      "id": "m9BndUHkN-Yf"
    },
    {
      "cell_type": "markdown",
      "id": "lRi0YkXIKPjX",
      "metadata": {
        "id": "lRi0YkXIKPjX"
      },
      "source": [
        "#### Combined 2D with 3D MultiPed Linformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3hC34nsPK-Jq",
      "metadata": {
        "id": "3hC34nsPK-Jq"
      },
      "outputs": [],
      "source": [
        "class Attn2D3D_MultiPedLin(nn.Module):\n",
        "\n",
        "    \"\"\"\n",
        "    Combined 2D and 3D Multi-Ped Attention\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,num_heads, d_model, num_peds, len_seq, k):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_params = d_model//num_heads\n",
        "        self.num_peds = num_peds\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, d_model) # mapping to the same number of features\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_l = nn.Linear(d_model,d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # Linformer Projections\n",
        "        self.E = nn.Linear(len_seq, k)\n",
        "        self.F = nn.Linear(len_seq, k)\n",
        "        self.G = nn.Linear(len_seq, k)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(self.head_params)\n",
        "        self.norm2 = nn.LayerNorm(self.head_params)\n",
        "\n",
        "    def self_attention2D(self, q, k, v, mask = None):\n",
        "        # Calculate the dot produce of Q and K\n",
        "        dotqk = torch.matmul(q, k.transpose(-2,-1))/(self.head_params**0.5) # shape = (#samples, num_heads, len_seq, len_seq)\n",
        "        #print(f'shape of dotqk 2D Attn: {dotqk.shape}')\n",
        "        if mask is not None:\n",
        "            #print(f'shape of the mask applied: {mask.shape}')\n",
        "            dotqk = dotqk.masked_fill(mask == 0, -1e9)\n",
        "            #print(f'mask applied in 2D attn')\n",
        "            #print(f'mask applied to 2D attention. Shape of the mask: {mask.shape}')\n",
        "        # Apply the Softmax to the attention weights\n",
        "        attention_scores = torch.softmax(dotqk, dim = -1)\n",
        "        # Multiply to the value matrix\n",
        "        result = torch.matmul(attention_scores, v)\n",
        "        return(result)\n",
        "\n",
        "    def self_attention3D(self, q, k,l, v,mask = None):\n",
        "        # Save the length of the sequence\n",
        "        seq_len = k.shape[2]\n",
        "        # Calculate the dot produce of Q and K\n",
        "        mul_qkl = torch.einsum('abcid,abcjd,abckd -> abcijk', q, k, l) /(self.head_params**0.5)# Shape: (#samples,num_heads,len_seq,len_seq, len_seq)\n",
        "        #print(f'shape of mul_qkl: {mul_qkl.shape}')\n",
        "        if mask is not None:\n",
        "            #print(f'shape of the mask applied: {mask.shape}')\n",
        "            mul_qkl = mul_qkl.masked_fill(mask == 0, -1e9)\n",
        "            #print(f'mask applied to 3D multiped Linformer; shape of the mask: {mask.shape}')\n",
        "        # Apply the Softmax to the attention weights\n",
        "        attention_scores = softmax_5d(mul_qkl, axis = (-1,-2))\n",
        "        # Calculate V^2\n",
        "        vtilde = torch.einsum('abcil,abcjl->abcijl', v, v)\n",
        "        # Multiply to the value matrix\n",
        "        result = torch.einsum('abcijk,abcjkl->abcil', attention_scores, vtilde)\n",
        "        #print(f'shape of attention results: {result.shape}')\n",
        "        return(result)\n",
        "\n",
        "    def split_heads(self, q):\n",
        "\n",
        "        \"\"\"\n",
        "        Reshaping\n",
        "        Input of shape : (#samples, seq_len, d_model)\n",
        "\n",
        "        output of shape: (#samples, num_heads, seq_len, head_parameters)\n",
        "        \"\"\"\n",
        "        samples, seq_len, d_model = q.shape\n",
        "        # Check if dimensions are valid\n",
        "        assert d_model % self.num_heads == 0, \"d_model is not divisable by the number of heads\"\n",
        "        return q.view(samples, seq_len, self.num_heads, self.head_params).transpose(1,2)\n",
        "\n",
        "\n",
        "    def split_sequence(self,q):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            q (Tensor): Input tensor of shape (samples, num_heads, seq_len, d_model).\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Reshaped tensor of shape (samples, num_heads, num_peds, seq_len // num_peds, d_model).\n",
        "        \"\"\"\n",
        "\n",
        "        samples, num_heads, seq_len, head_params = q.shape\n",
        "        assert seq_len % self.num_peds == 0, \"Sequence length is not divisible by number of peds\"\n",
        "        #print(f'shape of q: {q.shape}')\n",
        "\n",
        "        return q.reshape(samples, self.num_peds, num_heads, seq_len // self.num_peds, head_params)\n",
        "\n",
        "\n",
        "    def forward(self, q, mask = None):\n",
        "        #print(f'combined 2D and 3D multiped is activated')\n",
        "\n",
        "        # Clone Query to define key, utinity, and value matrices\n",
        "        k = q.clone()\n",
        "        l = q.clone()\n",
        "        v = q.clone()\n",
        "        mask_2D = None\n",
        "        mask_3DMP = None\n",
        "\n",
        "        # Unpack the masks\n",
        "        if mask is not None:\n",
        "          mask_2D = mask[0]\n",
        "          #print(f'mask_2D{mask_2D}, shape = {mask_2D.shape}')\n",
        "          mask_3DMP = mask[1]\n",
        "          #print(f'mask_3DMP{mask_3DMP}, shape = {mask_3DMP.shape}')\n",
        "\n",
        "        Query_2D = self.split_heads(self.W_q(q))\n",
        "        Key_2D  = self.split_heads(self.W_k(k))\n",
        "        Value_2D = self.split_heads(self.W_v(v))\n",
        "\n",
        "        # Run the 2D self-attention\n",
        "        attn_outputs_2D = self.self_attention2D(Query_2D, Key_2D, Value_2D, mask = mask_2D) # (batch_size, num_heads, len_seq, head_params)\n",
        "\n",
        "        # Define the Query, Key, Utinity and Value matrices for Multi-Ped 3D\n",
        "        Query_3D = self.split_sequence(Query_2D)\n",
        "        Key_3D  = self.split_sequence(Key_2D)\n",
        "        Value_3D = self.split_sequence(Value_2D)\n",
        "        L_matrix = self.split_sequence(self.split_heads(self.W_l(l)))\n",
        "\n",
        "        # Run MultiPed 3D attention\n",
        "        attn_outputs_3DMultiPed= self.self_attention3D(Query_3D, Key_3D, L_matrix, Value_3D, mask = mask_3DMP) # (batch_size, num_peds, num_heads, len_ped, head_params)\n",
        "        #print(f'shape of attention 2D output: {attn_outputs_3DMultiPed.shape}')\n",
        "\n",
        "        # Combine the peds\n",
        "        batch_size, num_peds, num_heads, len_ped ,len_head = attn_outputs_3DMultiPed.shape\n",
        "        attn_outputs_3DMultiPed = attn_outputs_3DMultiPed.contiguous().view(batch_size, num_heads, num_peds*len_ped, len_head)\n",
        "        #print(f'shape of attention 3D multi-ped output after combining heads {attn_outputs_3DMultiPed.shape}')\n",
        "\n",
        "        # Combine the attentions\n",
        "        attn_outputs_combined = self.norm1(attn_outputs_2D) + self.norm2(attn_outputs_3DMultiPed)\n",
        "\n",
        "        # Concatenate the heads\n",
        "        batch_size, _,seq_len, _= attn_outputs_combined.shape\n",
        "        attn_outputs_combined = attn_outputs_combined.transpose(1,2).contiguous().view(batch_size, seq_len, self.d_model) #shape = (#samples, len_seq, d_model)\n",
        "\n",
        "        # Apply through the last linear layer\n",
        "        result_final = self.W_o(attn_outputs_combined) #shape = (#samples, len_seq, d_model)\n",
        "\n",
        "        return result_final"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Aa9qPp3zgTNo",
      "metadata": {
        "id": "Aa9qPp3zgTNo"
      },
      "source": [
        "#### Combined 2D with 3D Multi-Ped Linformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qaqDb3GdgWwl",
      "metadata": {
        "id": "qaqDb3GdgWwl"
      },
      "outputs": [],
      "source": [
        "class Attn2D3D_MultiPedLinAlter(nn.Module):\n",
        "\n",
        "    \"\"\"\n",
        "    Combined 2D and 3D Multi-Ped Attention\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,num_heads, d_model, num_peds, len_seq, k):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_params = d_model//num_heads\n",
        "        self.num_peds = num_peds\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, d_model) # mapping to the same number of features\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_l = nn.Linear(d_model,d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # Linformer Projections\n",
        "        self.E = nn.Linear(len_seq, k)\n",
        "        self.F = nn.Linear(len_seq, k)\n",
        "        self.G = nn.Linear(len_seq, k)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(self.head_params)\n",
        "        self.norm2 = nn.LayerNorm(self.head_params)\n",
        "\n",
        "        self.linear = nn.Linear(2*self.head_params, self.head_params)\n",
        "\n",
        "    def self_attention2D(self, q, k, v, mask = None):\n",
        "        # Calculate the dot produce of Q and K\n",
        "        dotqk = torch.matmul(q, k.transpose(-2,-1))/(self.head_params**0.5) # shape = (#samples, num_heads, len_seq, len_seq)\n",
        "        #print(f'shape of dotqk 2D Attn: {dotqk.shape}')\n",
        "        if mask is not None:\n",
        "            #print(f'shape of the mask applied: {mask.shape}')\n",
        "            dotqk = dotqk.masked_fill(mask == 0, -1e9)\n",
        "            #print(f'mask applied in 2D attn')\n",
        "            #print(f'mask applied to 2D attention. Shape of the mask: {mask.shape}')\n",
        "        # Apply the Softmax to the attention weights\n",
        "        attention_scores = torch.softmax(dotqk, dim = -1)\n",
        "        # Multiply to the value matrix\n",
        "        result = torch.matmul(attention_scores, v)\n",
        "        return(result)\n",
        "\n",
        "    def self_attention3D(self, q, k,l, v,mask = None):\n",
        "        # Save the length of the sequence\n",
        "        seq_len = k.shape[2]\n",
        "        # Calculate the dot produce of Q and K\n",
        "        mul_qkl = torch.einsum('abcid,abcjd,abckd -> abcijk', q, k, l) /(self.head_params**0.5)# Shape: (#samples,num_heads,len_seq,len_seq, len_seq)\n",
        "        #print(f'shape of mul_qkl: {mul_qkl.shape}')\n",
        "        if mask is not None:\n",
        "            #print(f'shape of the mask applied: {mask.shape}')\n",
        "            mul_qkl = mul_qkl.masked_fill(mask == 0, -1e9)\n",
        "            #print(f'mask applied to 3D multiped Linformer; shape of the mask: {mask.shape}')\n",
        "        # Apply the Softmax to the attention weights\n",
        "        attention_scores = softmax_5d(mul_qkl, axis = (-1,-2))\n",
        "        # Calculate V^2\n",
        "        vtilde = torch.einsum('abcil,abcjl->abcijl', v, v)\n",
        "        # Multiply to the value matrix\n",
        "        result = torch.einsum('abcijk,abcjkl->abcil', attention_scores, vtilde)\n",
        "        #print(f'shape of attention results: {result.shape}')\n",
        "        return(result)\n",
        "\n",
        "    def split_heads(self, q):\n",
        "\n",
        "        \"\"\"\n",
        "        Reshaping\n",
        "        Input of shape : (#samples, seq_len, d_model)\n",
        "\n",
        "        output of shape: (#samples, num_heads, seq_len, head_parameters)\n",
        "        \"\"\"\n",
        "        samples, seq_len, d_model = q.shape\n",
        "        # Check if dimensions are valid\n",
        "        assert d_model % self.num_heads == 0, \"d_model is not divisable by the number of heads\"\n",
        "        return q.view(samples, seq_len, self.num_heads, self.head_params).transpose(1,2)\n",
        "\n",
        "\n",
        "    def split_sequence(self,q):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            q (Tensor): Input tensor of shape (samples, num_heads, seq_len, d_model).\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Reshaped tensor of shape (samples, num_heads, num_peds, seq_len // num_peds, d_model).\n",
        "        \"\"\"\n",
        "\n",
        "        samples, num_heads, seq_len, head_params = q.shape\n",
        "        assert seq_len % self.num_peds == 0, \"Sequence length is not divisible by number of peds\"\n",
        "        #print(f'shape of q: {q.shape}')\n",
        "\n",
        "        return q.reshape(samples, self.num_peds, num_heads, seq_len // self.num_peds, head_params)\n",
        "\n",
        "\n",
        "    def forward(self, q, mask = None):\n",
        "        #print(f'combined 2D and 3D multiped is activated')\n",
        "\n",
        "        # Clone Query to define key, utinity, and value matrices\n",
        "        k = q.clone()\n",
        "        l = q.clone()\n",
        "        v = q.clone()\n",
        "        mask_2D = None\n",
        "        mask_3DMP = None\n",
        "\n",
        "        # Unpack the masks\n",
        "        if mask is not None:\n",
        "          mask_2D = mask[0]\n",
        "          #print(f'mask_2D{mask_2D}, shape = {mask_2D.shape}')\n",
        "          mask_3DMP = mask[1]\n",
        "          #print(f'mask_3DMP{mask_3DMP}, shape = {mask_3DMP.shape}')\n",
        "\n",
        "        Query_2D = self.split_heads(self.W_q(q))\n",
        "        Key_2D  = self.split_heads(self.W_k(k))\n",
        "        Value_2D = self.split_heads(self.W_v(v))\n",
        "\n",
        "        # Run the 2D self-attention\n",
        "        attn_outputs_2D = self.self_attention2D(Query_2D, Key_2D, Value_2D, mask = mask_2D) # (batch_size, num_heads, len_seq, head_params)\n",
        "\n",
        "        # Define the Query, Key, Utinity and Value matrices for Multi-Ped 3D\n",
        "        Query_3D = self.split_sequence(Query_2D)\n",
        "        Key_3D  = self.split_sequence(Key_2D)\n",
        "        Value_3D = self.split_sequence(Value_2D)\n",
        "        L_matrix = self.split_sequence(self.split_heads(self.W_l(l)))\n",
        "\n",
        "        # Run MultiPed 3D attention\n",
        "        attn_outputs_3DMultiPed= self.self_attention3D(Query_3D, Key_3D, L_matrix, Value_3D, mask = mask_3DMP) # (batch_size, num_peds, num_heads, len_ped, head_params)\n",
        "        #print(f'shape of attention 2D output: {attn_outputs_3DMultiPed.shape}')\n",
        "\n",
        "        # Combine the peds\n",
        "        batch_size, num_peds, num_heads, len_ped ,len_head = attn_outputs_3DMultiPed.shape\n",
        "        attn_outputs_3DMultiPed = attn_outputs_3DMultiPed.contiguous().view(batch_size, num_heads, num_peds*len_ped, len_head)\n",
        "        #print(f'shape of attention 3D multi-ped output after combining heads {attn_outputs_3DMultiPed.shape}')\n",
        "\n",
        "        # Concatenate the attentions\n",
        "        attn_outputs_concat = torch.cat((attn_outputs_2D, attn_outputs_3DMultiPed), dim = -1)\n",
        "\n",
        "        # Combine the attentions\n",
        "        attn_outputs_combined = self.linear(attn_outputs_concat)\n",
        "        print(f'shape of attn_outputs_combined: {attn_outputs_combined.shape}')\n",
        "\n",
        "        # Concatenate the heads\n",
        "        batch_size, _,seq_len, _= attn_outputs_combined.shape\n",
        "        attn_outputs_combined = attn_outputs_combined.transpose(1,2).contiguous().view(batch_size, seq_len, self.d_model) #shape = (#samples, len_seq, d_model)\n",
        "\n",
        "        # Apply through the last linear layer\n",
        "        result_final = self.W_o(attn_outputs_combined) #shape = (#samples, len_seq, d_model)\n",
        "\n",
        "        return result_final"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23iE0JhQyyqX",
      "metadata": {
        "id": "23iE0JhQyyqX"
      },
      "source": [
        "#### Attention 2D overlapping blocks and 3D selected"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9UeEITjpy4j7",
      "metadata": {
        "id": "9UeEITjpy4j7"
      },
      "outputs": [],
      "source": [
        "class Attn2DMultiPed3Dselected(nn.Module):\n",
        "    \"\"\"\n",
        "  Attention Mechanism with 2D and selected 3D interactions.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads: int, d_model: int, stride: int = 10, block_size: int = 40, k=4, k_prime=2):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "        self.stride = stride\n",
        "        self.block_size = block_size\n",
        "        self.k = k\n",
        "        self.k_prime = k_prime\n",
        "\n",
        "        # Projection layers\n",
        "        self.W_q, self.W_k, self.W_v, self.W_o, self.W_l = [nn.Linear(d_model, d_model) for _ in range(5)]\n",
        "\n",
        "        # Fusion layer for combining 2D & 3D attention\n",
        "        self.fusion_layer = nn.Linear(2 * self.head_dim, self.head_dim)\n",
        "\n",
        "    #### Helper functions\n",
        "\n",
        "    def _split_heads(self, x):\n",
        "        \"\"\"Split embeddings into multiple heads.\"\"\"\n",
        "        B, Blk, L, D = x.size()\n",
        "        return x.view(B, Blk, L, self.num_heads, self.head_dim).transpose(2, 3)\n",
        "\n",
        "    def _sliding_blocks(self, x):\n",
        "        \"\"\"Create sliding-window blocks from the sequence.\"\"\"\n",
        "        l, d, (B, L, D) = self.block_size, self.stride, x.shape\n",
        "        num_blocks = (L - l) // d + 1\n",
        "        shape = (B, num_blocks, l, D)\n",
        "        strides = (x.stride(0), d * x.stride(1), x.stride(1), x.stride(2))\n",
        "        return x.as_strided(shape, strides).contiguous()\n",
        "\n",
        "    def _topk_submats_adv(self, attn_scores: torch.Tensor, topk_idx: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Extract submatrices of attention for top-k indices.\n",
        "\n",
        "        Args:\n",
        "            attn_scores (Tensor): (B, Blk, H, L, L).\n",
        "            topk_idx (Tensor): (B, Blk, H, L, K).\n",
        "\n",
        "        Returns:\n",
        "            Tensor: (B, Blk, H, L, K, K).\n",
        "        \"\"\"\n",
        "        B, Blk, H, L, _ = attn_scores.shape\n",
        "        K = topk_idx.shape[-1]\n",
        "        device = attn_scores.device\n",
        "        topk_idx = topk_idx.long().to(device)\n",
        "\n",
        "        # Select rows\n",
        "        b = torch.arange(B, device=device)[:, None, None, None, None].expand(B, Blk, H, L, K)\n",
        "        bl = torch.arange(Blk, device=device)[None, :, None, None, None].expand(B, Blk, H, L, K)\n",
        "        h = torch.arange(H, device=device)[None, None, :, None, None].expand(B, Blk, H, L, K)\n",
        "        q = torch.arange(L, device=device)[None, None, None, :, None].expand(B, Blk, H, L, K)\n",
        "\n",
        "        row_selected = attn_scores[b, bl, h, topk_idx, :]  # (B, Blk, H, L, K, L)\n",
        "\n",
        "        # Select cols\n",
        "        b2 = torch.arange(B, device=device)[:, None, None, None, None, None].expand(B, Blk, H, L, K, K)\n",
        "        bl2 = torch.arange(Blk, device=device)[None, :, None, None, None, None].expand(B, Blk, H, L, K, K)\n",
        "        h2 = torch.arange(H, device=device)[None, None, :, None, None, None].expand(B, Blk, H, L, K, K)\n",
        "        q2 = torch.arange(L, device=device)[None, None, None, :, None, None].expand(B, Blk, H, L, K, K)\n",
        "\n",
        "        cols = topk_idx.unsqueeze(-2).expand(B, Blk, H, L, K, K)\n",
        "        sub = row_selected[b2, bl2, h2, q2, torch.arange(K, device=device)[None, None, None, None, :, None], cols]\n",
        "        return sub\n",
        "\n",
        "    def _reconstruct_from_blocks(self, blocks, seq_len):\n",
        "        \"\"\"Reconstruct sequence from overlapping sliding blocks.\"\"\"\n",
        "        l, d, (B, num_blocks, _, D) = self.block_size, self.stride, blocks.shape\n",
        "        device = blocks.device\n",
        "        start_idx = torch.arange(num_blocks, device=device) * d\n",
        "        block_offsets = torch.arange(l, device=device)\n",
        "        positions = (start_idx[:, None] + block_offsets).unsqueeze(0).expand(B, -1, -1)\n",
        "        flat_pos, flat_blocks = positions.reshape(B, -1), blocks.reshape(B, -1, D)\n",
        "        out, counts = torch.zeros(B, seq_len, D, device=device), torch.zeros(B, seq_len, 1, device=device)\n",
        "        out.scatter_add_(1, flat_pos.unsqueeze(-1).expand(-1, -1, D), flat_blocks)\n",
        "        counts.scatter_add_(1, flat_pos.unsqueeze(-1), torch.ones_like(flat_pos, device=device).unsqueeze(-1).float())\n",
        "        return out / counts\n",
        "\n",
        "    def _expand_for_gather(self, tensor, expand_dims, expand_sizes):\n",
        "        \"\"\"Utility for repeated unsqueeze + expand before gather.\"\"\"\n",
        "        for axis, size in zip(expand_dims, expand_sizes):\n",
        "            tensor = tensor.unsqueeze(axis).expand(*size)\n",
        "        return tensor\n",
        "\n",
        "    def _gather_partners(self, tensor, partner_idx):\n",
        "        \"\"\"Helper for gathering partner-specific selections (K, L, or V).\"\"\"\n",
        "        return torch.gather(tensor, 4, partner_idx.unsqueeze(-1).expand(*partner_idx.shape, self.head_dim))\n",
        "\n",
        "    def _self_attention_2d(self, q, k, v, mask=None):\n",
        "        \"\"\"Standard scaled dot-product attention (2D).\"\"\"\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask.unsqueeze(2).unsqueeze(4) == 0, -1e9)\n",
        "        attn_scores = torch.softmax(scores, dim=-1)\n",
        "        return attn_scores, torch.matmul(attn_scores, v)\n",
        "\n",
        "    def compute_selected_mask(self, mask, partner1, partner2, L):\n",
        "        \"\"\"Compute the final interaction mask for 3D attention.\"\"\"\n",
        "        if mask is None:\n",
        "            return None\n",
        "        mask_reshaped1 = mask.unsqueeze(2).expand(-1, -1, self.num_heads, -1)\n",
        "        mask_reshaped2 = mask_reshaped1.unsqueeze(-2).expand(-1, -1, -1, L, -1)\n",
        "        mask1_comp, mask2_comp = torch.gather(mask_reshaped2, 4, partner1), torch.gather(mask_reshaped2, 4, partner2)\n",
        "        two_way_interac_mask = mask1_comp * mask2_comp\n",
        "        mask_comp3 = mask_reshaped1.unsqueeze(-1).expand(-1, -1, -1, -1, partner1.size(-1))\n",
        "        return mask_comp3 * two_way_interac_mask\n",
        "\n",
        "    ####\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        \"\"\"Forward pass through Multi-Ped Attention.\"\"\"\n",
        "        #print(f'shape of x inputted: {x.shape}')\n",
        "        q, k, v, l = self.W_q(x), self.W_k(x), self.W_v(x), self.W_l(x)\n",
        "        Q, Key, V, L_mat = map(lambda t: self._split_heads(self._sliding_blocks(t)), (q, k, v, l))\n",
        "\n",
        "        # 2D Attention\n",
        "        attn_scores, attn_out_2d = self._self_attention_2d(Q, Key, V, mask=mask)\n",
        "\n",
        "        # Top-k selection\n",
        "        topk_scores, topk_idx = torch.topk(attn_scores, self.k, dim=-1)\n",
        "        outer = topk_scores[..., :, None] * topk_scores[..., None, :]\n",
        "        selected = self._topk_submats_adv(attn_scores, topk_idx)\n",
        "        res = outer * selected\n",
        "        B, Blk, H, L, K, _ = res.shape\n",
        "        _, topk_exp = torch.topk(res.view(B, Blk, H, L, -1), self.k_prime, dim=-1)\n",
        "        row_idx, col_idx = topk_exp // self.k, topk_exp % self.k_prime\n",
        "        partner1, partner2 = torch.gather(topk_idx, -1, row_idx), torch.gather(topk_idx, -1, col_idx)\n",
        "\n",
        "        # 3D Attention\n",
        "        K_exp, L_exp, V_exp = (Key, L_mat, V)\n",
        "        K_exp, L_exp, V_exp = (t.unsqueeze(-3).expand(-1, -1, -1, L, -1, -1) for t in (K_exp, L_exp, V_exp))\n",
        "        k_sel, l_sel, v1, v2 = (self._gather_partners(K_exp, partner1),\n",
        "                                self._gather_partners(L_exp, partner2),\n",
        "                                self._gather_partners(V_exp, partner1),\n",
        "                                self._gather_partners(V_exp, partner2))\n",
        "        q_exp = Q.unsqueeze(-2)\n",
        "        dot_qkl = (q_exp * k_sel * l_sel).sum(-1)\n",
        "        # print(f'shape of dotqkl: {dot_qkl.shape}')\n",
        "        final_mask = self.compute_selected_mask(mask, partner1, partner2, L)\n",
        "        if final_mask is not None:\n",
        "            dot_qkl = dot_qkl.masked_fill(final_mask == 0, -1e9)\n",
        "            # print(f'shape of mask: {final_mask.shape}')\n",
        "        attn_weights_3d = torch.softmax(dot_qkl, dim=-1)\n",
        "\n",
        "        # Apply to values\n",
        "        v_sq, dot_qkl_exp = v1 * v2, attn_weights_3d.unsqueeze(-1).expand(-1, -1, -1, -1, -1, self.head_dim)\n",
        "        res_3d = (dot_qkl_exp * v_sq).sum(-2)\n",
        "\n",
        "        # Fusion\n",
        "        res_combined = torch.cat([attn_out_2d, res_3d], dim=-1)\n",
        "        out = self.fusion_layer(res_combined).view(B, Blk, L, self.d_model)\n",
        "\n",
        "        # Reconstruct from blocks\n",
        "        return self.W_o(self._reconstruct_from_blocks(out, x.shape[1]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dp3kMTJk7A2g",
      "metadata": {
        "id": "dp3kMTJk7A2g"
      },
      "source": [
        "#### 3D Attention with Multi-Ped"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "w4YgDFhq1YB_",
      "metadata": {
        "id": "w4YgDFhq1YB_"
      },
      "outputs": [],
      "source": [
        "class Attn3D_MultiPed(nn.Module):\n",
        "    \"\"\"\n",
        "    num_peds defaults to 10\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,num_heads, d_model, num_peds):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_params = d_model//num_heads\n",
        "        self.num_peds = num_peds\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, d_model) # mapping to the same number of features\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_l = nn.Linear(d_model,d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def self_attention(self, q, k,l, v,mask = None):\n",
        "        # Save the length of the sequence\n",
        "        seq_len = k.shape[2]\n",
        "        # Calculate the dot produce of Q and K\n",
        "        mul_qkl = torch.einsum('abcid,abcjd,abckd -> abcijk', q, k, l) /(self.head_params**0.5)# Shape: (#samples,num_heads,len_seq,len_seq, len_seq)\n",
        "        #print(f'shape of mul_qkl: {mul_qkl.shape}')\n",
        "        if mask is not None:\n",
        "            #print(f'mask given to Multi3D: {mask.shape}')\n",
        "            mul_qkl = mul_qkl.masked_fill(mask == 0, -1e9) ######changed\n",
        "            #print(f'mask applied')\n",
        "        # Apply the Softmax to the attention weights\n",
        "        attention_scores = softmax_5d(mul_qkl, axis = (-1,-2))\n",
        "        # Calculate V^2\n",
        "        vtilde = torch.einsum('abcil,abcjl->abcijl', v, v)\n",
        "        # Multiply to the value matrix\n",
        "        result = torch.einsum('abcijk,abcjkl->abcil', attention_scores, vtilde)\n",
        "        #print(f'shape of attention results: {result.shape}')\n",
        "        return(result)\n",
        "\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        # Shape: (batch, seq_len, d_model) --> (batch, num_heads, seq_len, head_dim)\n",
        "        bsz, num_peds, len_peds, d_model = x.size()\n",
        "        return x.reshape(bsz, num_peds, len_peds, self.num_heads, self.head_params).transpose(2,3)\n",
        "\n",
        "    def split_sequence(self, q):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            q (Tensor): Input tensor of shape (samples, num_heads, seq_len, d_model).\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Reshaped tensor of shape (samples, num_heads, num_peds, seq_len // num_peds, d_model).\n",
        "        \"\"\"\n",
        "\n",
        "        samples, seq_len, d_model = q.shape\n",
        "        assert seq_len % self.num_peds == 0, \"Sequence length is not divisible by number of peds\"\n",
        "        #print(f'shape of q: {q.shape}')\n",
        "\n",
        "        return q.reshape(samples, self.num_peds, seq_len // self.num_peds, d_model)\n",
        "\n",
        "\n",
        "    def forward(self, q,mask = None):\n",
        "        k = q.clone()\n",
        "        l = q.clone()\n",
        "        v = q.clone()\n",
        "        #print(f'Multi-Ped is activated')\n",
        "        # Define Query, Key, Value\n",
        "        Query = self.split_heads(self.split_sequence(self.W_q(q)))\n",
        "        Key  = self.split_heads(self.split_sequence(self.W_k(k)))\n",
        "        L_matrix = self.split_heads(self.split_sequence(self.W_l(l)))\n",
        "        Value = self.split_heads(self.split_sequence(self.W_v(v)))\n",
        "        #print(f'shape of Query: {Query.shape}')\n",
        "        #print(f'shape of Key: {Key.shape}')\n",
        "        #print(f'shape of Value: {Value.shape}')\n",
        "\n",
        "        # Run the self-attention\n",
        "        attn_output = self.self_attention(Query, Key,L_matrix, Value, mask = mask)\n",
        "        #print(f'shape of the attn outputs received:{attn_output.shape}')\n",
        "\n",
        "        batch_size, num_peds, num_heads, len_ped ,len_head = attn_output.shape\n",
        "        attn_output = attn_output.contiguous().view(batch_size, num_peds, len_ped, self.d_model)\n",
        "        #print(f'shape of attention output after combining heads {attn_output.shape}')\n",
        "\n",
        "        # Combine peds -> (batch_size, len_peds * num_peds, d_model)\n",
        "        attn_output = attn_output.contiguous().view(batch_size, len_ped * num_peds, self.d_model)\n",
        "        #print(f'shape of attention output after combining the peds {attn_output.shape}')\n",
        "\n",
        "        # Apply through the last linear layer\n",
        "        result_final = self.W_o(attn_output) #shape = (#samples, len_seq, d_model)\n",
        "        #print(f'shape of final output: {result_final.shape}')\n",
        "\n",
        "        return result_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uRsKsM0nzB1h",
      "metadata": {
        "id": "uRsKsM0nzB1h"
      },
      "outputs": [],
      "source": [
        "class Attn3D_MultiPed(nn.Module):\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,num_heads, d_model, block_size, stride):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_params = d_model//num_heads\n",
        "        self.block_size = block_size\n",
        "        self.stride = stride\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, d_model) # mapping to the same number of features\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_l = nn.Linear(d_model,d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def self_attention(self, q, k,l, v,mask = None):\n",
        "        # Save the length of the sequence\n",
        "        seq_len = k.shape[2]\n",
        "        # Calculate the dot produce of Q and K\n",
        "        mul_qkl = torch.einsum('abcid,abcjd,abckd -> abcijk', q, k, l) /(self.head_params**0.5)# Shape: (#samples,num_heads,len_seq,len_seq, len_seq)\n",
        "        #print(f'shape of mul_qkl: {mul_qkl.shape}')\n",
        "        if mask is not None:\n",
        "            mul_qkl = mul_qkl.masked_fill(mask == 0, -1e9) ######changed\n",
        "            #print(f'mask applied in the sliding blocks Multiped 3D')\n",
        "            #print(f'shape of mask applied: {mask.shape}')\n",
        "        # Apply the Softmax to the attention weights\n",
        "        attention_scores = softmax_5d(mul_qkl, axis = (-1,-2))\n",
        "        # Calculate V^2\n",
        "        vtilde = torch.einsum('abcil,abcjl->abcijl', v, v)\n",
        "        # Multiply to the value matrix\n",
        "        result = torch.einsum('abcijk,abcjkl->abcil', attention_scores, vtilde)\n",
        "        #print(f'shape of attention results: {result.shape}')\n",
        "        return(result)\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        # Shape: (batch, seq_len, d_model) --> (batch, num_heads, seq_len, head_dim)\n",
        "        bsz, num_peds, len_peds, d_model = x.size()\n",
        "        return x.reshape(bsz, num_peds, len_peds, self.num_heads, self.head_params).transpose(2,3)\n",
        "\n",
        "    def _sliding_blocks(self, x):\n",
        "        \"\"\"\n",
        "        Create sliding-window blocks from the sequence.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): (B, L, D).\n",
        "\n",
        "        Returns:\n",
        "            Tensor: (B, num_blocks, block_size, D).\n",
        "        \"\"\"\n",
        "        l = self.block_size\n",
        "        d = self.stride\n",
        "        B, L, D = x.shape\n",
        "\n",
        "        num_blocks = (L - l) // d + 1\n",
        "\n",
        "        # Compute new shape and strides\n",
        "        shape = (B, num_blocks, l, D)\n",
        "        strides = (\n",
        "            x.stride(0),        # batch stride\n",
        "            d * x.stride(1),    # jump d steps for each block\n",
        "            x.stride(1),        # step 1 inside a block\n",
        "            x.stride(2),        # feature stride\n",
        "        )\n",
        "\n",
        "        blocks = x.as_strided(shape, strides)\n",
        "        return blocks.contiguous()\n",
        "\n",
        "    def _reconstruct_from_blocks(self, blocks, seq_len):\n",
        "        \"\"\"\n",
        "        Reconstruct sequence from overlapping sliding blocks.\n",
        "\n",
        "        Args:\n",
        "            blocks (Tensor): (B, num_blocks, block_size, D).\n",
        "            seq_len (int): Original sequence length.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: (B, seq_len, D).\n",
        "        \"\"\"\n",
        "        l = self.block_size\n",
        "        d = self.stride\n",
        "        B, num_blocks, _, D = blocks.shape\n",
        "\n",
        "        device = blocks.device\n",
        "\n",
        "        # Build index map: (num_blocks, block_size) -> positions in seq_len\n",
        "        start_idx = torch.arange(num_blocks, device=device) * d\n",
        "        block_offsets = torch.arange(l, device=device)\n",
        "        positions = start_idx[:, None] + block_offsets[None, :]  # (num_blocks, block_size)\n",
        "\n",
        "        # Expand to batch and feature dims\n",
        "        positions = positions.unsqueeze(0).expand(B, -1, -1)  # (B, num_blocks, block_size)\n",
        "\n",
        "        # Flatten everything for scatter\n",
        "        flat_pos = positions.reshape(B, -1)  # (B, num_blocks * block_size)\n",
        "        flat_blocks = blocks.reshape(B, -1, D)  # (B, num_blocks * block_size, D)\n",
        "\n",
        "        # Allocate output\n",
        "        out = torch.zeros(B, seq_len, D, device=device)\n",
        "        counts = torch.zeros(B, seq_len, 1, device=device)\n",
        "\n",
        "        # Scatter add the block values into the right positions\n",
        "        out.scatter_add_(1, flat_pos.unsqueeze(-1).expand(-1, -1, D), flat_blocks)\n",
        "        counts.scatter_add_(1, flat_pos.unsqueeze(-1), torch.ones_like(flat_pos, device=device).unsqueeze(-1).float())\n",
        "\n",
        "        return out / counts\n",
        "\n",
        "    def forward(self, q,mask = None):\n",
        "        k = q.clone()\n",
        "        l = q.clone()\n",
        "        v = q.clone()\n",
        "        # Define Query, Key, Value\n",
        "        Query = self.split_heads(self._sliding_blocks(self.W_q(q)))\n",
        "        Key  = self.split_heads(self._sliding_blocks(self.W_k(k)))\n",
        "        L_matrix = self.split_heads(self._sliding_blocks(self.W_l(l)))\n",
        "        Value = self.split_heads(self._sliding_blocks(self.W_v(v)))\n",
        "        #print(f'shape of Query: {Query.shape}')\n",
        "        #print(f'shape of Key: {Key.shape}')\n",
        "        #print(f'shape of Value: {Value.shape}')\n",
        "\n",
        "        # Run the self-attention\n",
        "        attn_output = self.self_attention(Query, Key,L_matrix, Value, mask = mask)\n",
        "        #print(f'shape of the attn outputs received:{attn_output.shape}')\n",
        "\n",
        "        batch_size, num_peds, num_heads, len_ped ,len_head = attn_output.shape\n",
        "        attn_output = attn_output.contiguous().view(batch_size, num_peds, len_ped, self.d_model)\n",
        "        #print(f'shape of attention output after combining heads {attn_output.shape}')\n",
        "\n",
        "        # Combine peds -> (batch_size, len_peds * num_peds, d_model)\n",
        "        len_seq = q.shape[1]\n",
        "        attn_output = self._reconstruct_from_blocks(attn_output,len_seq)\n",
        "        #print(f'shape of attention output after combining the peds {attn_output.shape}')\n",
        "\n",
        "        # Apply through the last linear layer\n",
        "        result_final = self.W_o(attn_output) #shape = (#samples, len_seq, d_model)\n",
        "        #print(f'shape of final output: {result_final.shape}')\n",
        "\n",
        "        return result_final"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xPorzgr-7bZF",
      "metadata": {
        "id": "xPorzgr-7bZF"
      },
      "source": [
        "#### 3D Linformer Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wvB1rkE27FYN",
      "metadata": {
        "id": "wvB1rkE27FYN"
      },
      "outputs": [],
      "source": [
        "class Attn3DLinformer(nn.Module):\n",
        "    \"\"\"\n",
        "    Modified Multi-Head Attention with 3D product using E, F, G projections.\n",
        "\n",
        "    Args:\n",
        "        num_heads (int): Number of attention heads\n",
        "        d_model (int): Input dimension\n",
        "        k (int): Latent projection size for E, F, G\n",
        "        len_seq (int): Sequence length\n",
        "\n",
        "        Note that the length of the sequence must be inputted correctly here.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_heads, d_model, k, len_seq):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        self.head_dim = d_model // num_heads\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_l = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.E = nn.Linear(len_seq, k)\n",
        "        self.F = nn.Linear(len_seq, k)\n",
        "        self.G = nn.Linear(len_seq, k)\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        B, L, D = x.size()\n",
        "        x = x.view(B, L, self.num_heads, self.head_dim)\n",
        "        return x.transpose(1, 2)  # [B, H, L, D]\n",
        "\n",
        "    def self_attention(self, q, k, l, v,mask=None):\n",
        "        # Compute trilinear attention scores\n",
        "        scores = torch.einsum('bhid,bhjd,bhkd->bhijk', q, k, l) / (self.head_dim ** 0.5)\n",
        "        if mask is not None:\n",
        "            #print(f'shape of mask given Linformer3D : {mask.shape}')\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "            #print(f'mask applied in linformer 3d')\n",
        "\n",
        "        attn_weights = softmax_5d(scores, axis=(-1, -2))\n",
        "\n",
        "        v_3d = torch.einsum('bhld,bhLd->bhlLd', v, v)  # Could be simplified\n",
        "        output = torch.einsum('bhijk,bhjkl->bhil', attn_weights, v_3d)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def forward(self, q,mask = None):\n",
        "        k = q.clone()\n",
        "        l = q.clone()\n",
        "        v = q.clone()\n",
        "        Q = self.split_heads(self.W_q(q))\n",
        "\n",
        "        def project(x, linear_proj, W):\n",
        "            x = W(x).permute(0, 2, 1)\n",
        "            x = linear_proj(x).permute(0, 2, 1)\n",
        "            return self.split_heads(x)\n",
        "\n",
        "        K = project(k, self.E, self.W_k)\n",
        "        L = project(l, self.F, self.W_l)\n",
        "        V = project(v, self.G, self.W_v)\n",
        "\n",
        "        attn_out = self.self_attention(Q, K, L, V,mask)\n",
        "\n",
        "        B, H, L, D = attn_out.shape\n",
        "        concat = attn_out.transpose(1, 2).contiguous().view(B, L, self.d_model)\n",
        "        return self.W_o(concat)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CXBpsWkwwzrX",
      "metadata": {
        "id": "CXBpsWkwwzrX"
      },
      "source": [
        "#### Multi-ped + Linformer 3D Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nk5aMkrYzQVo",
      "metadata": {
        "id": "nk5aMkrYzQVo"
      },
      "outputs": [],
      "source": [
        "class Attn3DLinformerMultiPed(nn.Module):\n",
        "    \"\"\"\n",
        "    Modified Multi-Head Attention with 3D product using E, F, G projections.\n",
        "\n",
        "    Args:\n",
        "        num_heads (int): Number of attention heads\n",
        "        d_model (int): Input dimension\n",
        "        k (int): Latent projection size for E, F, G\n",
        "        len_seq (int): Sequence length\n",
        "\n",
        "        Note that the length of the sequence must be inputted correctly here.\n",
        "        first multi-ped then linformer attention\n",
        "    \"\"\"\n",
        "    def __init__(self, num_heads, d_model, k, len_seq, block_size, stride):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        self.head_dim = d_model // num_heads\n",
        "        self.block_size = block_size\n",
        "        self.stride = stride\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_l = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.E = nn.Linear(block_size, k)\n",
        "        self.F = nn.Linear(block_size, k)\n",
        "        self.G = nn.Linear(block_size, k)\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        # Shape: (batch, seq_len, d_model) --> (batch, num_heads, seq_len, head_dim)\n",
        "        bsz, num_peds, len_peds, d_model = x.size()\n",
        "        return x.reshape(bsz, num_peds, len_peds, self.num_heads, self.head_dim).transpose(2,3)\n",
        "\n",
        "    def self_attention(self, q, k,l, v, mask = None):\n",
        "        # Save the length of the sequence\n",
        "        seq_len = k.shape[2]\n",
        "        # Calculate the dot produce of Q and K\n",
        "        mul_qkl = torch.einsum('abcid,abcjd,abckd -> abcijk', q, k, l) /(self.head_dim**0.5)# Shape: (#samples,num_heads,len_seq,k, k)\n",
        "        #print(f'shape of the attention scores: {mul_qkl.shape}')\n",
        "        if mask is not None:\n",
        "            mul_qkl = mul_qkl.masked_fill(mask == 0, -1e9)\n",
        "            #print(f'mask is implemented in MultiLin3D')\n",
        "            #print(f'shape of the mask: {mask.shape}')\n",
        "        # Apply the Softmax to the attention weights\n",
        "        attention_scores = softmax_5d(mul_qkl, axis = (-1,-2))\n",
        "        # Calculate V^2\n",
        "        vtilde = torch.einsum('abcid,abcjd->abcijd', v, v)\n",
        "        # Multiply to the value matrix\n",
        "        result = torch.einsum('abcijk,abcjkl->abcil', attention_scores, vtilde)\n",
        "        #print(f'shape of attention results: {result.shape}')\n",
        "        return(result)\n",
        "\n",
        "    def _sliding_blocks(self, x):\n",
        "        \"\"\"\n",
        "        Create sliding-window blocks from the sequence.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): (B, L, D).\n",
        "\n",
        "        Returns:\n",
        "            Tensor: (B, num_blocks, block_size, D).\n",
        "        \"\"\"\n",
        "        l = self.block_size\n",
        "        d = self.stride\n",
        "        B, L, D = x.shape\n",
        "\n",
        "        num_blocks = (L - l) // d + 1\n",
        "\n",
        "        # Compute new shape and strides\n",
        "        shape = (B, num_blocks, l, D)\n",
        "        strides = (\n",
        "            x.stride(0),        # batch stride\n",
        "            d * x.stride(1),    # jump d steps for each block\n",
        "            x.stride(1),        # step 1 inside a block\n",
        "            x.stride(2),        # feature stride\n",
        "        )\n",
        "\n",
        "        blocks = x.as_strided(shape, strides)\n",
        "        return blocks.contiguous()\n",
        "\n",
        "    def _reconstruct_from_blocks(self, blocks, seq_len):\n",
        "        \"\"\"\n",
        "        Reconstruct sequence from overlapping sliding blocks.\n",
        "\n",
        "        Args:\n",
        "            blocks (Tensor): (B, num_blocks, block_size, D).\n",
        "            seq_len (int): Original sequence length.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: (B, seq_len, D).\n",
        "        \"\"\"\n",
        "        l = self.block_size\n",
        "        d = self.stride\n",
        "        B, num_blocks, _, D = blocks.shape\n",
        "\n",
        "        device = blocks.device\n",
        "\n",
        "        # Build index map: (num_blocks, block_size) -> positions in seq_len\n",
        "        start_idx = torch.arange(num_blocks, device=device) * d\n",
        "        block_offsets = torch.arange(l, device=device)\n",
        "        positions = start_idx[:, None] + block_offsets[None, :]  # (num_blocks, block_size)\n",
        "\n",
        "        # Expand to batch and feature dims\n",
        "        positions = positions.unsqueeze(0).expand(B, -1, -1)  # (B, num_blocks, block_size)\n",
        "\n",
        "        # Flatten everything for scatter\n",
        "        flat_pos = positions.reshape(B, -1)  # (B, num_blocks * block_size)\n",
        "        flat_blocks = blocks.reshape(B, -1, D)  # (B, num_blocks * block_size, D)\n",
        "\n",
        "        # Allocate output\n",
        "        out = torch.zeros(B, seq_len, D, device=device)\n",
        "        counts = torch.zeros(B, seq_len, 1, device=device)\n",
        "\n",
        "        # Scatter add the block values into the right positions\n",
        "        out.scatter_add_(1, flat_pos.unsqueeze(-1).expand(-1, -1, D), flat_blocks)\n",
        "        counts.scatter_add_(1, flat_pos.unsqueeze(-1), torch.ones_like(flat_pos, device=device).unsqueeze(-1).float())\n",
        "\n",
        "        return out / counts\n",
        "\n",
        "    def forward(self, q,mask = None):\n",
        "        k = q.clone()\n",
        "        l = q.clone()\n",
        "        v = q.clone()\n",
        "        Q = self.split_heads(self._sliding_blocks(self.W_q(q)))\n",
        "        #print(f'shape of Q: {Q.shape}')\n",
        "\n",
        "        def project(x, linear_proj, W):\n",
        "            '''\n",
        "            Expected input shape: ([batch_size, len_seq, d_model])\n",
        "\n",
        "            '''\n",
        "            x = self._sliding_blocks(W(x)) #shape: ([batch_size, num_peds, len_ped, d_model])\n",
        "            x = x.permute(0, 1, 3, 2) #shape: ([batch_size, num_peds, d_model, len_ped])\n",
        "            x = linear_proj(x) # shape: torch.Size([batch_size, num_peds, d_model, k])\n",
        "            x = x.permute(0, 1, 3, 2) # shape: torch.Size([batch_size, num_peds, k, d_model])\n",
        "            return self.split_heads((x)) # shape: torch.Size([batch_size, num_peds, num_heads, k , head_dim])\n",
        "\n",
        "        K = project(k, self.E, self.W_k)\n",
        "        #print(f'shape of K: {K.shape}')\n",
        "        L = project(l, self.F, self.W_l)\n",
        "        V = project(v, self.G, self.W_v)\n",
        "\n",
        "        attn_output = self.self_attention(Q, K, L, V,mask)\n",
        "\n",
        "        batch_size, num_peds, num_heads, len_ped ,len_head = attn_output.shape\n",
        "        attn_output = attn_output.contiguous().view(batch_size, num_peds, len_ped, self.d_model)\n",
        "        #print(f'shape of attention output after combining heads {attn_output.shape}')\n",
        "\n",
        "        # Combine peds -> (batch_size, len_peds * num_peds, d_model)\n",
        "        attn_output = self._reconstruct_from_blocks(attn_output, q.shape[1])\n",
        "        #print(f'shape of attention output after combining the peds {attn_output.shape}')\n",
        "\n",
        "        # Apply through the last linear layer\n",
        "        result_final = self.W_o(attn_output) #shape = (#samples, len_seq, d_model)\n",
        "        #print(f'shape of final output: {result_final.shape}')\n",
        "\n",
        "        return result_final"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "huya0YlI84YY",
      "metadata": {
        "id": "huya0YlI84YY"
      },
      "source": [
        "#### Plain 3D Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Nfgusw1V89Rz",
      "metadata": {
        "id": "Nfgusw1V89Rz"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttnMod3D(nn.Module):\n",
        "    \"\"\"\n",
        "    Encoder Attention:\n",
        "    Input is mapped to Query, Key, and L-matrix, each matrix is (d_model x d_model).\n",
        "    Query, Key, and L-matrix are multiplied to get a len_seq x len_seq x len_seq matrix:\n",
        "        - shows correlation between 3 elements.\n",
        "    Softmax is taken across the last two dimensions (defined by 5D Softmax)\n",
        "    Value is calculated by multiplying input sequence by another matrix of d_model x d_model -> dim = len_seq x d_model\n",
        "    Attention scores after softmax are multiplied by Value\n",
        "            -> (#samples, num_heads, head_param, head_param, head_param) x (//, //, len_seq, head_param)\n",
        "            = (//,//, len_seq, head_param)\n",
        "    mask: padding mask\n",
        "\n",
        "\n",
        "    Decoder Attention\n",
        "    Decoder input mapped to Query\n",
        "    Encoder output mapped to Key, L-matrix and Value\n",
        "    mask: look-ahead mask and the padding mask\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,num_heads, d_model):\n",
        "        super(MultiHeadAttnMod3D,self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_params = d_model//num_heads\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, d_model) # mapping to the same number of features\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_l = nn.Linear(d_model,d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # Check if the number of len_emb (d_model) is divisable by num_heads\n",
        "        assert d_model % num_heads == 0, \"d_model is not divisable by the number of heads\"\n",
        "\n",
        "\n",
        "    def self_attention(self, q, k,l, v, mask):\n",
        "        # Save the length of the sequence\n",
        "        seq_len = k.shape[2]\n",
        "        # Calculate the dot produce of Q and K ## multiplication occurs across the last dim (embedding dim)\n",
        "        mul_qkl = torch.einsum('abcd,abed,abfd -> abcef', q, k, l) /(self.head_params**0.5) # Shape: (#samples,num_heads,len_seq,len_seq, len_seq)\n",
        "        #print(f'shape of mul_qkl: {mul_qkl.shape}')\n",
        "        if mask is not None:\n",
        "            #print(f'mask shape 3D attn : {mask.shape}')\n",
        "            mul_qkl = mul_qkl.masked_fill(mask == 0, -1e9)\n",
        "            print(f'mask applied in 3d plain')\n",
        "\n",
        "        # Apply the Softmax to the attention weights\n",
        "        attention_scores = softmax_5d(mul_qkl, axis = (-1,-2))\n",
        "        # Multiply v by v -> Dim = NxNxd\n",
        "        v_3d = torch.einsum('shld, shLd -> shlLd', v,v) # shape: torch.Size([#samples, #heads, len_seq, len_seq, d_model/#heads])\n",
        "        # Multiply to the value matrix\n",
        "        result = torch.einsum('shijk,shjkl->shil', attention_scores, v_3d)\n",
        "        #return(attention_scores,result)\n",
        "        return(result)\n",
        "\n",
        "    def split_heads(self, q):\n",
        "\n",
        "        \"\"\"\n",
        "        Reshaping\n",
        "        Input of shape : (#samples, seq_len, d_model)\n",
        "\n",
        "        to\n",
        "        output of shape: (#samples, num_heads, seq_len, head_parameters)\n",
        "        \"\"\"\n",
        "        samples, seq_len, _ = q.shape\n",
        "        return q.view(samples, seq_len, self.num_heads, self.head_params).transpose(1,2)\n",
        "\n",
        "    def forward(self, q, mask = None):\n",
        "        k = q.clone()\n",
        "        l = q.clone()\n",
        "        v = q.clone()\n",
        "\n",
        "        # Define Query, Key, Value\n",
        "        Query = self.split_heads(self.W_q(q))\n",
        "        Key  = self.split_heads(self.W_k(k))\n",
        "        L_matrix = self.split_heads(self.W_l(l))\n",
        "        Value = self.split_heads(self.W_v(v))\n",
        "\n",
        "        # Run the self-attention\n",
        "        attn_output = self.self_attention(Query, Key,L_matrix, Value, mask)\n",
        "\n",
        "        # Concatenate the heads\n",
        "        batch_size, _,seq_len, _= attn_output.shape\n",
        "        attn_output = attn_output.transpose(1,2).contiguous().view(batch_size, seq_len, self.d_model) #shape = (#samples, len_seq, d_model)\n",
        "\n",
        "        # Apply through the last linear layer\n",
        "        result_final = self.W_o(attn_output) #shape = (#samples, len_seq, d_model)\n",
        "\n",
        "        return result_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dZ67w3AC9Vpl",
      "metadata": {
        "id": "dZ67w3AC9Vpl"
      },
      "outputs": [],
      "source": [
        "def get_attention(attn_type, **kwargs):\n",
        "  \"\"\"\n",
        "  Attention options include:\n",
        "  - Plain2D\n",
        "  - Linformer2D\n",
        "  - Multiped2D\n",
        "\n",
        "  - Plain3D\n",
        "  - Linformer3D\n",
        "  - Multiped3D\n",
        "  - Combined3D\n",
        "  - Attn3DLinformerMultiPed\n",
        "\n",
        "  Combined2D3D\n",
        "  Combined2D3DLinformer\n",
        "  Combined2D3DMultiPed\n",
        "  Combined2D3DMultiPedLinformer\n",
        "\n",
        "  \"\"\"\n",
        "  if attn_type == \"Plain3D\":\n",
        "      print(f'Plain3D is activated')\n",
        "      return MultiHeadAttnMod3D(d_model=kwargs[\"d_model\"], num_heads=kwargs[\"num_heads\"])\n",
        "  elif attn_type == \"Linformer3D\":\n",
        "      print(f'Linformer3D is activated')\n",
        "      return Attn3DLinformer(num_heads = kwargs['num_heads'], d_model=kwargs[\"d_model\"], k=kwargs[\"k\"], len_seq=kwargs[\"len_seq\"])\n",
        "  elif attn_type == \"Combined2D3D\":\n",
        "      print(f'Combined2D3D is activated')\n",
        "      return Attn_2D3DC(num_heads=kwargs[\"num_heads\"], d_model=kwargs[\"d_model\"])\n",
        "  elif attn_type == \"Multiped3D\":\n",
        "      print(f'Multiped3D is activated')\n",
        "      return Attn3D_MultiPed(num_heads=kwargs[\"num_heads\"], d_model=kwargs[\"d_model\"], block_size = kwargs[\"block_size\"], stride=kwargs[\"stride\"])\n",
        "  elif attn_type == \"Attn3DLinformerMultiPed\":\n",
        "      print(f'Attn3DLinformerMultiPed is activated')\n",
        "      return Attn3DLinformerMultiPed(num_heads=kwargs[\"num_heads\"], d_model=kwargs[\"d_model\"], k=kwargs[\"k\"], len_seq=kwargs[\"len_seq\"], block_size = kwargs[\"block_size\"], stride=kwargs[\"stride\"])\n",
        "  elif attn_type == \"Plain2D\":\n",
        "      print(f'Plain2D is activated')\n",
        "      return MultiHeadAttn2D(d_model=kwargs[\"d_model\"], num_heads=kwargs[\"num_heads\"])\n",
        "  elif attn_type == \"Linformer2D\":\n",
        "      print(f'Linformer2D is activated')\n",
        "      return Attn2DLinformer(num_heads = kwargs['num_heads'], d_model=kwargs[\"d_model\"], k=kwargs[\"k\"], len_seq=kwargs[\"len_seq\"])\n",
        "  elif attn_type == \"Multiped2D\":\n",
        "      print(f'Multiped2D is activated')\n",
        "      return Attn2D_MultiPed(num_heads=kwargs[\"num_heads\"], d_model=kwargs[\"d_model\"], block_size = kwargs[\"block_size\"], stride = kwargs[\"stride\"])\n",
        "  elif attn_type == \"MultiLin2D\":\n",
        "      print(f'MultiLin2D is activated')\n",
        "      return Attn2DLinformerMultiPed(num_heads=kwargs[\"num_heads\"], d_model=kwargs[\"d_model\"],k=kwargs[\"k\"], len_seq=kwargs[\"len_seq\"], stride = kwargs[\"stride\"], block_size = kwargs[\"block_size\"])\n",
        "  elif attn_type == \"Attn2D3DLin\":\n",
        "      print(f'Attn2D3DLin is activated')\n",
        "      return Attn2D3DLin(num_heads = kwargs['num_heads'], d_model= kwargs['d_model'], k = kwargs['k'], len_seq = kwargs['len_seq'])\n",
        "  elif attn_type == 'Attn2D3D_MultiPed':\n",
        "      print(f'Attn2D3D_MultiPed is activated')\n",
        "      return Attn2D3D_MultiPed(num_heads=kwargs[\"num_heads\"], d_model=kwargs[\"d_model\"], num_peds=kwargs[\"num_peds\"])\n",
        "  elif attn_type == \"Attn2D3D_MultiPedLin\":\n",
        "      print(f'Attn2D3DMultiPed_Linformer is activated')\n",
        "      return Attn2D3D_MultiPedLin(num_heads= kwargs[\"num_heads\"], d_model= kwargs[\"d_model\"], num_peds=kwargs[\"num_peds\"], len_seq=kwargs[\"len_seq\"], k=kwargs[\"k\"])\n",
        "  elif attn_type ==\"Attn2DMultiPed3Dselected\":\n",
        "    return Attn2DMultiPed3Dselected(num_heads = kwargs[\"num_heads\"], d_model = kwargs[\"d_model\"], stride = kwargs[\"stride\"], block_size=kwargs[\"block_size\"])\n",
        "  elif attn_type == \"Linformer3D_Dual\":\n",
        "      print(f'Linformer3D Dual is activated')\n",
        "      return Attn3DLinformer_Dual(num_heads = kwargs['num_heads'], d_model=kwargs[\"d_model\"], k=kwargs[\"k\"], len_seq=kwargs[\"len_seq\"])\n",
        "  elif attn_type == \"Multiped3D_Dual\":\n",
        "      print(f'Multiped3D Dual is activated')\n",
        "      return Attn3D_MultiPed_Dual(num_heads=kwargs[\"num_heads\"], d_model=kwargs[\"d_model\"], block_size = kwargs[\"block_size\"], stride=kwargs[\"stride\"])\n",
        "  elif attn_type == \"MultiLin3D_Dual\":\n",
        "      print(f'MultiLin3D Dual is activated')\n",
        "      return Attn3DLinformerMultiPed_Dual(num_heads=kwargs[\"num_heads\"], d_model=kwargs[\"d_model\"], k=kwargs[\"k\"], len_seq=kwargs[\"len_seq\"], block_size = kwargs[\"block_size\"], stride=kwargs[\"stride\"])\n",
        "  elif attn_type == 'Attn2DMultiPed3Dselected_Dual':\n",
        "    print(f'Attn2DMultiPed3Dselected_Dual is activated')\n",
        "    return Attn2DMultiPed3Dselected_Dual(num_heads = kwargs[\"num_heads\"], d_model = kwargs[\"d_model\"], stride = kwargs[\"stride\"], block_size=kwargs[\"block_size\"])\n",
        "  else:\n",
        "      raise ValueError(f\"Unknown attention type: {attn_type}\")\n",
        "\n",
        "def softmax_5d(X, axis):\n",
        "    \"\"\"\n",
        "    Compute softmax for a 5D tensor along the specified axis.\n",
        "\n",
        "    Parameters:\n",
        "        X (numpy.ndarray or torch.Tensor): Input tensor of shape (sample_size, heads, n, n, n)\n",
        "        axis (int or tuple): Axis along which to apply softmax\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray or torch.Tensor: Softmax-applied tensor of the same shape as X.\n",
        "    \"\"\"\n",
        "    X_exp = torch.exp(X - torch.amax(X, dim=axis, keepdim=True))  # Stability trick\n",
        "    return X_exp / torch.sum(X_exp, dim=axis, keepdim=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GmEPv52MYpuk",
      "metadata": {
        "id": "GmEPv52MYpuk"
      },
      "source": [
        " Training Alternative Attentions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fn1vtP4TZGEn",
      "metadata": {
        "id": "fn1vtP4TZGEn"
      },
      "source": [
        "#### Training Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pHRYl8FgWO8-",
      "metadata": {
        "id": "pHRYl8FgWO8-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import contextlib\n",
        "import io\n",
        "import numpy as np\n",
        "\n",
        "def evaluate_model(model, eval_loader, class_report=False):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    total_loss = 0.0\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in eval_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            logits, labels = model(input_ids, labels)\n",
        "            loss = criterion(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            preds = logits.argmax(dim=-1)\n",
        "            mask = labels != -100\n",
        "            preds = preds[mask]\n",
        "            labels = labels[mask]\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    avg_loss = total_loss / len(eval_loader)\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    if eval_loader == test1_loader:\n",
        "      print(f'Evaluating the trained model on CB513 dataset')\n",
        "    elif eval_loader == test2_loader:\n",
        "      print(f'Evaluating the trained model on TS115 dataset')\n",
        "    elif eval_loader == test3_loader:\n",
        "      print(f'Evaluating the trained model on CASP12 dataset')\n",
        "\n",
        "    if class_report:\n",
        "        print(classification_report(all_labels, all_preds, digits=4))\n",
        "        print(f\"Average Validation Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "    else:\n",
        "      return avg_loss, accuracy\n",
        "\n",
        "\n",
        "def train_model(model_class, model_kwargs, train_loader, val_loader, epochs=10, lr=1e-4):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model_class(**model_kwargs).to(device)\n",
        "\n",
        "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"\\nTotal trainable parameters: {total_params:,}\\n\")\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "\n",
        "    epoch_losses = []\n",
        "    epoch_times = []\n",
        "    # Training Loop\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        epoch_start_time = time.time()\n",
        "\n",
        "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
        "        for batch in pbar:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            logits, labels = model(input_ids, labels)\n",
        "            loss = criterion(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            pbar.set_postfix(loss=loss.item())\n",
        "\n",
        "        epoch_duration = time.time() - epoch_start_time\n",
        "        epoch_times.append(epoch_duration)\n",
        "        avg_epoch_loss = total_loss / len(train_loader)\n",
        "\n",
        "        # Silent validation\n",
        "        with contextlib.redirect_stdout(io.StringIO()):\n",
        "            avg_val_loss, val_accuracy = evaluate_model(model, val_loader)\n",
        "\n",
        "        epoch_losses.append(avg_val_loss)\n",
        "\n",
        "        print(f\" Epoch {epoch+1} | Training Loss: {avg_epoch_loss:.4f} | \"\n",
        "              f\"Validation Loss: {avg_val_loss:.4f} | Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "\n",
        "    total_training_time = sum(epoch_times)\n",
        "    avg_epoch_time = np.mean(epoch_times)\n",
        "\n",
        "    print(f\"Average training time per epoch: {avg_epoch_time:.2f}s\")\n",
        "\n",
        "    return model, epoch_losses\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fKR4Q7BUXMu",
      "metadata": {
        "id": "0fKR4Q7BUXMu"
      },
      "source": [
        "## Secondary Structure Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "J9j5pO0NUVNY",
      "metadata": {
        "collapsed": true,
        "id": "J9j5pO0NUVNY"
      },
      "outputs": [],
      "source": [
        "# Step 1: Download all 5 datasets\n",
        "# !wget http://s3.amazonaws.com/songlabdata/proteindata/data_pytorch/secondary_structure.tar.gz\n",
        "# !wget http://s3.amazonaws.com/songlabdata/proteindata/data_pytorch/proteinnet.tar.gz\n",
        "# !wget http://s3.amazonaws.com/songlabdata/proteindata/data_pytorch/remote_homology.tar.gz\n",
        "# !wget http://s3.amazonaws.com/songlabdata/proteindata/data_pytorch/fluorescence.tar.gz\n",
        "# !wget http://s3.amazonaws.com/songlabdata/proteindata/data_pytorch/stability.tar.gz\n",
        "\n",
        "# Unzip the downloaded files\n",
        "# !tar -xzf secondary_structure.tar.gz\n",
        "# !tar -xzf proteinnet.tar.gz\n",
        "# !tar -xzf remote_homology.tar.gz\n",
        "# !tar -xzf fluorescence.tar.gz\n",
        "# !tar -xzf stability.tar.gz\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KgC2Vd_7Zao2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgC2Vd_7Zao2",
        "outputId": "10da04fc-7d4e-4437-d257-234404d5c966"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tape-proteins in /usr/local/lib/python3.12/dist-packages (0.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from tape-proteins) (4.67.1)\n",
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.12/dist-packages (from tape-proteins) (2.6.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from tape-proteins) (1.16.2)\n",
            "Requirement already satisfied: lmdb in /usr/local/lib/python3.12/dist-packages (from tape-proteins) (1.7.3)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.12/dist-packages (from tape-proteins) (1.40.43)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from tape-proteins) (2.32.4)\n",
            "Requirement already satisfied: biopython in /usr/local/lib/python3.12/dist-packages (from tape-proteins) (1.85)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from tape-proteins) (3.19.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from biopython->tape-proteins) (2.0.2)\n",
            "Requirement already satisfied: botocore<1.41.0,>=1.40.43 in /usr/local/lib/python3.12/dist-packages (from boto3->tape-proteins) (1.40.43)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from boto3->tape-proteins) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.15.0,>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from boto3->tape-proteins) (0.14.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->tape-proteins) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->tape-proteins) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->tape-proteins) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->tape-proteins) (2025.8.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorboardX->tape-proteins) (25.0)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.12/dist-packages (from tensorboardX->tape-proteins) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.12/dist-packages (from botocore<1.41.0,>=1.40.43->boto3->tape-proteins) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.41.0,>=1.40.43->boto3->tape-proteins) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install tape-proteins"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jF4y9nWaWz7n",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jF4y9nWaWz7n",
        "outputId": "b9941b80-a06d-46b2-8354-6572c604d2d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sequences for training set: 8678\n",
            "Number of sequences for validation set: 2170\n",
            "Number of sequences for test set 1: 513\n",
            "Number of sequences for test set 2: 115\n",
            "Number of sequences for test set 3: 21\n",
            "Amino Acids (tokens): AETVESCLAKSHTENSFTNVXKDDKTLDRYANYEGCLWNATGVVVCTGDETQCYGTWVPIGLAIPENEGGGSEGGGSEGGGSEGGGTKPPEYGDTPIPGYTYINPLDGTYPPGTEQNPANPNPSLEESQPLNTFMFQNNRFRNRQGALTVYTGTVTQGTDPVKTYYQYTPVSSKAMYDAYWNGKFRDCAFHSGFNEDIFVCEYQGQSSDLPQPPVNA, \n",
            "len_seq = 217\n",
            "Secondary Structure Labels (targets): [2 2 2 0 0 0 0 0 2 2 2 2 1 1 1 1 1 1 2 2 1 1 2 2 2 2 2 2 1 1 1 1 1 2 2 1 1\n",
            " 1 1 1 1 1 1 1 1 1 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2\n",
            " 1 2 2 2 2 1 2 2 2 2 2 2 1 1 1 2 2 2 2 2 2 2 1 1 1 2 2 1 1 1 1 1 1 2 2 1 1\n",
            " 1 1 1 1 1 1 1 1 1 2 2 2 2 2 1 1 1 1 1 1 1 1 2 2 2 0 0 0 0 0 0 0 0 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 2 2 2 2 2 2 1 1 1 1 2 2 2 2 2 2 2]\n"
          ]
        }
      ],
      "source": [
        "from tape.datasets import LMDBDataset\n",
        "\n",
        "# Change the path below to the dataset you want to inspect\n",
        "lmdb_path = \"/content/drive/My Drive/BioTransformer/TAPE /secondary_structure/secondary_structure_train.lmdb\"\n",
        "dataset_train = LMDBDataset(lmdb_path)\n",
        "dataset_val = LMDBDataset(\"/content/drive/My Drive/BioTransformer/TAPE /secondary_structure/secondary_structure_valid.lmdb\")\n",
        "dataset_test1 = LMDBDataset(\"/content/drive/My Drive/BioTransformer/TAPE /secondary_structure/secondary_structure_cb513.lmdb\")\n",
        "dataset_test2 = LMDBDataset(\"/content/drive/My Drive/BioTransformer/TAPE /secondary_structure/secondary_structure_ts115.lmdb\")\n",
        "dataset_test3 = LMDBDataset(\"/content/drive/My Drive/BioTransformer/TAPE /secondary_structure/secondary_structure_casp12.lmdb\")\n",
        "\n",
        "\n",
        "# Show size\n",
        "print(\"Number of sequences for training set:\", len(dataset_train))\n",
        "print(\"Number of sequences for validation set:\", len(dataset_val))\n",
        "print(\"Number of sequences for test set 1:\", len(dataset_test1))\n",
        "print(\"Number of sequences for test set 2:\", len(dataset_test2))\n",
        "print(\"Number of sequences for test set 3:\", len(dataset_test3))\n",
        "\n",
        "# Inspect the first sample\n",
        "sample = dataset_train[0]\n",
        "print(f\"Amino Acids (tokens): {sample['primary']}, \\nlen_seq = {len(sample['primary'])}\")\n",
        "print(\"Secondary Structure Labels (targets):\", sample['ss3'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vPXeh8zBelZu",
      "metadata": {
        "id": "vPXeh8zBelZu"
      },
      "source": [
        "The goal is that for every amino acid, we need to predict a secondary structure label. So we will not need any pooling or CLS token for this classification and we will need padding because the legnth of the sequences is fixed."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7vLIZTH5atXf",
      "metadata": {
        "id": "7vLIZTH5atXf"
      },
      "source": [
        "#### Keys:\n",
        "\n",
        "- Primary: primary sequence of amino acids (as a string or array of token IDs)\n",
        "- Protein_length\n",
        "- Evolutionary:\tEvolutionary information, usually a Position-Specific Scoring Matrix (PSSM) or HHblits profiles. Shape: (L, 20) where L = sequence length.\n",
        "- ss3:\tSecondary structure in 3 classes (H = Helix, E = Strand, C = Coil). Label format -> this will be our target\n",
        "- ss8: Secondary structure in 8 classes (more fine-grained).\n",
        "- phi, psi: Backbone torsion angles in radians for each residue.\n",
        "- asa_max: Max accessible surface area (used for normalization).\n",
        "- rsa: Relative solvent accessibility per residue (between 0 and 1).\n",
        "- interface:\tWhether each residue is at a proteinprotein interface (binary label).\n",
        "- disorder:\tBinary label for whether the residue is in a disordered region.\n",
        "- valid_mask:\tA mask indicating valid residues (e.g., for padding or missing data)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wP0XazRmZ896",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wP0XazRmZ896",
        "outputId": "a18a0116-c6da-431f-f59b-2c8884fb9573"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['asa_max', 'disorder', 'evolutionary', 'interface', 'phi', 'primary', 'psi', 'rsa', 'ss3', 'ss8', 'valid_mask', 'id', 'protein_length'])"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sample.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gQDuvjCcd0Kb",
      "metadata": {
        "id": "gQDuvjCcd0Kb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "from tape.tokenizers import TAPETokenizer\n",
        "# Define the Tokenizer\n",
        "tokenizer = TAPETokenizer(vocab='iupac')\n",
        "\n",
        "class SecondaryStructureDataset(Dataset):\n",
        "    def __init__(self, lmdb_dataset, tokenizer):\n",
        "        self.data = lmdb_dataset\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      sample = self.data[idx]\n",
        "      # Tokenize without special tokens\n",
        "      tokens = list(sample['primary'])  # List of single-letter amino acids\n",
        "      input_ids = torch.tensor(self.tokenizer.convert_tokens_to_ids(tokens), dtype=torch.long)\n",
        "\n",
        "      attention_mask = torch.tensor(sample['valid_mask'], dtype=torch.long)\n",
        "      labels = torch.tensor(sample['ss3'], dtype=torch.long)\n",
        "\n",
        "      return {\n",
        "          'input_ids': input_ids,\n",
        "          'attention_mask': attention_mask,\n",
        "          'labels': labels\n",
        "      }\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "FIXED_LEN_SEQ = 512\n",
        "\n",
        "def collate_fn(batch):\n",
        "    def pad_tensor(t, pad_value):\n",
        "        return torch.nn.functional.pad(t, (0, FIXED_LEN_SEQ - t.size(0)), value=pad_value)\n",
        "\n",
        "    input_ids = torch.stack([pad_tensor(x['input_ids'], 0) for x in batch])\n",
        "    attention_mask = torch.stack([pad_tensor(x['attention_mask'], 0) for x in batch])\n",
        "    labels = torch.stack([pad_tensor(x['labels'], -100) for x in batch])  # -100 for ignored tokens\n",
        "\n",
        "    return {\n",
        "        'input_ids': input_ids,\n",
        "        'attention_mask': attention_mask,\n",
        "        'labels': labels\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WZSIGHaCiGf3",
      "metadata": {
        "id": "WZSIGHaCiGf3"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "dataset_train = SecondaryStructureDataset(dataset_train, tokenizer)\n",
        "dataset_val = SecondaryStructureDataset(dataset_val, tokenizer)\n",
        "dataset_test1 = SecondaryStructureDataset(dataset_test1, tokenizer)\n",
        "dataset_test2 = SecondaryStructureDataset(dataset_test2, tokenizer)\n",
        "dataset_test3 = SecondaryStructureDataset(dataset_test3, tokenizer)\n",
        "\n",
        "\n",
        "train_loader = DataLoader(dataset_train, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(dataset_val, batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
        "test1_loader = DataLoader(dataset_test1, batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
        "test2_loader = DataLoader(dataset_test2, batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
        "test3_loader = DataLoader(dataset_test3, batch_size=16, shuffle=False, collate_fn=collate_fn)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZR5qiF0dxhiL",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZR5qiF0dxhiL",
        "outputId": "83c13606-fecf-42dd-8882-d9bd674817e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([16, 512])\n",
            "torch.Size([16, 512])\n",
            "torch.Size([16, 512])\n"
          ]
        }
      ],
      "source": [
        "sample = next(iter(train_loader))\n",
        "print(sample['input_ids'].shape)\n",
        "print(sample['attention_mask'].shape)\n",
        "print(sample['labels'].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EIP5pmvpuDnZ",
      "metadata": {
        "id": "EIP5pmvpuDnZ"
      },
      "source": [
        "### Try Models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kPftPWQasXoV",
      "metadata": {
        "id": "kPftPWQasXoV"
      },
      "source": [
        "### Training 2D models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Yf6DYjM5OGyP",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yf6DYjM5OGyP",
        "outputId": "b4b38b84-bae6-4e5d-f4e7-422d6f51503c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Plain2D is activated\n",
            "Plain2D is activated\n",
            "Plain2D is activated\n",
            "Plain2D is activated\n",
            "Plain2D is activated\n",
            "Plain2D is activated\n",
            "Plain2D is activated\n",
            "Plain2D is activated\n",
            "Plain2D is activated\n",
            "Plain2D is activated\n",
            "Plain2D is activated\n",
            "Plain2D is activated\n",
            "\n",
            "Total trainable parameters: 25,512,451\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|| 543/543 [01:18<00:00,  6.95it/s, loss=0.924]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 1 | Training Loss: 0.9967 | Validation Loss: 0.9602 | Accuracy: 0.5296\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|| 543/543 [01:18<00:00,  6.95it/s, loss=0.948]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 2 | Training Loss: 0.9638 | Validation Loss: 0.9736 | Accuracy: 0.5308\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|| 543/543 [01:18<00:00,  6.95it/s, loss=0.977]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 3 | Training Loss: 0.9579 | Validation Loss: 0.9524 | Accuracy: 0.5369\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|| 543/543 [01:18<00:00,  6.96it/s, loss=0.932]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 4 | Training Loss: 0.9538 | Validation Loss: 0.9829 | Accuracy: 0.5190\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5: 100%|| 543/543 [01:17<00:00,  6.96it/s, loss=0.994]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 5 | Training Loss: 0.9507 | Validation Loss: 0.9711 | Accuracy: 0.5323\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6: 100%|| 543/543 [01:18<00:00,  6.96it/s, loss=0.92]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 6 | Training Loss: 0.9457 | Validation Loss: 0.9614 | Accuracy: 0.5326\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7: 100%|| 543/543 [01:18<00:00,  6.96it/s, loss=0.935]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 7 | Training Loss: 0.9410 | Validation Loss: 0.9473 | Accuracy: 0.5382\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8: 100%|| 543/543 [01:18<00:00,  6.96it/s, loss=0.79]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 8 | Training Loss: 0.9351 | Validation Loss: 0.9488 | Accuracy: 0.5425\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9: 100%|| 543/543 [01:18<00:00,  6.96it/s, loss=0.935]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 9 | Training Loss: 0.9297 | Validation Loss: 0.9396 | Accuracy: 0.5453\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10: 100%|| 543/543 [01:18<00:00,  6.95it/s, loss=1]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 10 | Training Loss: 0.9230 | Validation Loss: 0.9406 | Accuracy: 0.5457\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11: 100%|| 543/543 [01:18<00:00,  6.95it/s, loss=0.854]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 11 | Training Loss: 0.9168 | Validation Loss: 0.9691 | Accuracy: 0.5452\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12: 100%|| 543/543 [01:18<00:00,  6.95it/s, loss=0.889]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 12 | Training Loss: 0.9090 | Validation Loss: 0.9435 | Accuracy: 0.5490\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 13: 100%|| 543/543 [01:18<00:00,  6.96it/s, loss=0.806]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 13 | Training Loss: 0.8983 | Validation Loss: 0.9291 | Accuracy: 0.5551\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 14: 100%|| 543/543 [01:18<00:00,  6.95it/s, loss=0.869]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 14 | Training Loss: 0.8859 | Validation Loss: 0.9393 | Accuracy: 0.5614\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 15: 100%|| 543/543 [01:18<00:00,  6.95it/s, loss=0.91]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 15 | Training Loss: 0.8715 | Validation Loss: 0.9285 | Accuracy: 0.5616\n",
            "Average training time per epoch: 78.08s\n",
            "Evaluating the trained model on CB513 dataset\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5480    0.4803    0.5119     46162\n",
            "           1     0.4294    0.4275    0.4284     29902\n",
            "           2     0.5909    0.6494    0.6188     58858\n",
            "\n",
            "    accuracy                         0.5424    134922\n",
            "   macro avg     0.5228    0.5191    0.5197    134922\n",
            "weighted avg     0.5404    0.5424    0.5400    134922\n",
            "\n",
            "Average Validation Loss: 0.9633, Accuracy: 0.5424\n",
            "Evaluating the trained model on TS115 dataset\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6154    0.6048    0.6100     11062\n",
            "           1     0.4547    0.4303    0.4421      5327\n",
            "           2     0.6193    0.6433    0.6311     12275\n",
            "\n",
            "    accuracy                         0.5889     28664\n",
            "   macro avg     0.5631    0.5595    0.5611     28664\n",
            "weighted avg     0.5872    0.5889    0.5878     28664\n",
            "\n",
            "Average Validation Loss: 0.8904, Accuracy: 0.5889\n",
            "Evaluating the trained model on CASP12 dataset\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5235    0.6500    0.5799      1837\n",
            "           1     0.5039    0.4131    0.4540      1249\n",
            "           2     0.6571    0.6069    0.6310      2867\n",
            "\n",
            "    accuracy                         0.5795      5953\n",
            "   macro avg     0.5615    0.5567    0.5550      5953\n",
            "weighted avg     0.5837    0.5795    0.5781      5953\n",
            "\n",
            "Average Validation Loss: 0.9061, Accuracy: 0.5795\n"
          ]
        }
      ],
      "source": [
        "# Start Training\n",
        "model, model_kwargs = ProteinTransformerSS3, {\n",
        "    'vocab_size': tokenizer.vocab_size,\n",
        "    'num_classes': 3,\n",
        "    'd_model': 512,\n",
        "    'num_layers': 12,\n",
        "    'dim_feedforward': 1024,\n",
        "    'p': 0.4,\n",
        "    'len_seq': 512,\n",
        "    'num_heads': 8,\n",
        "    'attn_mechanism': 'Plain2D',\n",
        "    'num_peds': 30\n",
        "}\n",
        "trained_model, avg_val_loss_Plain2D = train_model(model, model_kwargs, train_loader, val_loader, epochs=15, lr=1e-4)\n",
        "evaluate_model(trained_model, test1_loader, class_report = True)\n",
        "evaluate_model(trained_model, test2_loader, class_report = True)\n",
        "evaluate_model(trained_model, test3_loader, class_report = True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZnjzJDV1zs9h",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZnjzJDV1zs9h",
        "outputId": "54fe6d0d-68a4-41f4-d5b9-926730ba5c23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Multiped2D is activated\n",
            "Multiped2D is activated\n",
            "Multiped2D is activated\n",
            "Multiped2D is activated\n",
            "Multiped2D is activated\n",
            "Multiped2D is activated\n",
            "Multiped2D is activated\n",
            "Multiped2D is activated\n",
            "Multiped2D is activated\n",
            "Multiped2D is activated\n",
            "Multiped2D is activated\n",
            "Multiped2D is activated\n",
            "\n",
            "Total trainable parameters: 25,519,107\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|| 543/543 [01:11<00:00,  7.59it/s, loss=0.919]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 1 | Training Loss: 0.9488 | Validation Loss: 0.9124 | Accuracy: 0.5819\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|| 543/543 [01:11<00:00,  7.59it/s, loss=0.732]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 2 | Training Loss: 0.8798 | Validation Loss: 0.8553 | Accuracy: 0.6176\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|| 543/543 [01:11<00:00,  7.59it/s, loss=0.791]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 3 | Training Loss: 0.8229 | Validation Loss: 0.8103 | Accuracy: 0.6501\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|| 543/543 [01:11<00:00,  7.59it/s, loss=0.78]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 4 | Training Loss: 0.7789 | Validation Loss: 0.7890 | Accuracy: 0.6596\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5: 100%|| 543/543 [01:11<00:00,  7.59it/s, loss=0.783]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 5 | Training Loss: 0.7507 | Validation Loss: 0.7877 | Accuracy: 0.6699\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6: 100%|| 543/543 [01:11<00:00,  7.59it/s, loss=0.74]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 6 | Training Loss: 0.7263 | Validation Loss: 0.7709 | Accuracy: 0.6740\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7: 100%|| 543/543 [01:11<00:00,  7.59it/s, loss=0.679]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 7 | Training Loss: 0.7035 | Validation Loss: 0.8047 | Accuracy: 0.6750\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8: 100%|| 543/543 [01:11<00:00,  7.59it/s, loss=0.704]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 8 | Training Loss: 0.6812 | Validation Loss: 0.8027 | Accuracy: 0.6701\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9: 100%|| 543/543 [01:11<00:00,  7.59it/s, loss=0.6]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 9 | Training Loss: 0.6578 | Validation Loss: 0.8223 | Accuracy: 0.6759\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10: 100%|| 543/543 [01:11<00:00,  7.59it/s, loss=0.67]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 10 | Training Loss: 0.6338 | Validation Loss: 0.8789 | Accuracy: 0.6707\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11: 100%|| 543/543 [01:11<00:00,  7.59it/s, loss=0.652]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 11 | Training Loss: 0.6125 | Validation Loss: 0.8614 | Accuracy: 0.6731\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12: 100%|| 543/543 [01:11<00:00,  7.59it/s, loss=0.501]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 12 | Training Loss: 0.5917 | Validation Loss: 0.9134 | Accuracy: 0.6649\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 13: 100%|| 543/543 [01:11<00:00,  7.59it/s, loss=0.59]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 13 | Training Loss: 0.5730 | Validation Loss: 0.9341 | Accuracy: 0.6652\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 14: 100%|| 543/543 [01:11<00:00,  7.59it/s, loss=0.493]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 14 | Training Loss: 0.5567 | Validation Loss: 0.9705 | Accuracy: 0.6672\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 15: 100%|| 543/543 [01:11<00:00,  7.59it/s, loss=0.561]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 15 | Training Loss: 0.5400 | Validation Loss: 0.9731 | Accuracy: 0.6640\n",
            "Average training time per epoch: 71.55s\n",
            "Evaluating the trained model on CB513 dataset\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6633    0.7041    0.6831     46162\n",
            "           1     0.5435    0.5695    0.5562     29902\n",
            "           2     0.6893    0.6393    0.6634     58858\n",
            "\n",
            "    accuracy                         0.6460    134922\n",
            "   macro avg     0.6320    0.6376    0.6342    134922\n",
            "weighted avg     0.6481    0.6460    0.6464    134922\n",
            "\n",
            "Average Validation Loss: 1.0254, Accuracy: 0.6460\n",
            "Evaluating the trained model on TS115 dataset\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6910    0.7543    0.7212     11062\n",
            "           1     0.5470    0.5818    0.5639      5327\n",
            "           2     0.7166    0.6376    0.6748     12275\n",
            "\n",
            "    accuracy                         0.6723     28664\n",
            "   macro avg     0.6515    0.6579    0.6533     28664\n",
            "weighted avg     0.6752    0.6723    0.6721     28664\n",
            "\n",
            "Average Validation Loss: 0.9571, Accuracy: 0.6723\n",
            "Evaluating the trained model on CASP12 dataset\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5975    0.7823    0.6775      1837\n",
            "           1     0.5625    0.5332    0.5475      1249\n",
            "           2     0.7297    0.6017    0.6595      2867\n",
            "\n",
            "    accuracy                         0.6430      5953\n",
            "   macro avg     0.6299    0.6391    0.6282      5953\n",
            "weighted avg     0.6538    0.6430    0.6416      5953\n",
            "\n",
            "Average Validation Loss: 1.0038, Accuracy: 0.6430\n"
          ]
        }
      ],
      "source": [
        "# New MultiPed with sliding blocks\n",
        "# Start Training\n",
        "model, model_kwargs = ProteinTransformerSS3, {\n",
        "    'vocab_size': tokenizer.vocab_size,\n",
        "    'num_classes': 3,\n",
        "    'd_model': 512,\n",
        "    'num_layers': 12,\n",
        "    'dim_feedforward': 1024,\n",
        "    'p': 0.4,\n",
        "    'len_seq': 512,\n",
        "    'num_heads': 8,\n",
        "    'attn_mechanism': 'Multiped2D',\n",
        "    \"num_peds\": 30,\n",
        "    \"block_size\": 30,\n",
        "    \"stride\": 15\n",
        "}\n",
        "trained_model, avg_val_loss_Multi2D = train_model(model, model_kwargs, train_loader, val_loader, epochs=15, lr=1e-4)\n",
        "evaluate_model(trained_model, test1_loader, class_report = True)\n",
        "evaluate_model(trained_model, test2_loader, class_report = True)\n",
        "evaluate_model(trained_model, test3_loader, class_report = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "T-WaI8KOV_jl",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "T-WaI8KOV_jl",
        "outputId": "e135d054-39bc-4795-d887-72615b7de67b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "95"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gc\n",
        "#del model\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JVKRTFDcOPnA",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JVKRTFDcOPnA",
        "outputId": "63397c62-b756-405d-d435-ca043e303b83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Linformer2D is activated\n",
            "Linformer2D is activated\n",
            "Linformer2D is activated\n",
            "Linformer2D is activated\n",
            "Linformer2D is activated\n",
            "Linformer2D is activated\n",
            "Linformer2D is activated\n",
            "Linformer2D is activated\n",
            "Linformer2D is activated\n",
            "Linformer2D is activated\n",
            "Linformer2D is activated\n",
            "Linformer2D is activated\n",
            "\n",
            "Total trainable parameters: 26,128,051\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|| 543/543 [00:59<00:00,  9.20it/s, loss=0.94]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 1 | Training Loss: 1.0135 | Validation Loss: 1.0090 | Accuracy: 0.5013\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|| 543/543 [00:58<00:00,  9.21it/s, loss=1]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 2 | Training Loss: 0.9658 | Validation Loss: 0.9932 | Accuracy: 0.5290\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|| 543/543 [00:58<00:00,  9.21it/s, loss=0.938]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 3 | Training Loss: 0.9506 | Validation Loss: 0.9830 | Accuracy: 0.5314\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|| 543/543 [00:58<00:00,  9.21it/s, loss=0.895]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 4 | Training Loss: 0.9348 | Validation Loss: 1.0056 | Accuracy: 0.5234\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5: 100%|| 543/543 [00:58<00:00,  9.21it/s, loss=0.927]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 5 | Training Loss: 0.9231 | Validation Loss: 1.0138 | Accuracy: 0.5283\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6: 100%|| 543/543 [00:58<00:00,  9.21it/s, loss=0.849]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 6 | Training Loss: 0.9127 | Validation Loss: 0.9986 | Accuracy: 0.5254\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7: 100%|| 543/543 [00:58<00:00,  9.22it/s, loss=0.868]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 7 | Training Loss: 0.9027 | Validation Loss: 1.0019 | Accuracy: 0.5250\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8: 100%|| 543/543 [00:58<00:00,  9.21it/s, loss=0.901]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 8 | Training Loss: 0.8938 | Validation Loss: 0.9935 | Accuracy: 0.5268\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9: 100%|| 543/543 [00:59<00:00,  9.20it/s, loss=0.835]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 9 | Training Loss: 0.8850 | Validation Loss: 0.9964 | Accuracy: 0.5278\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10: 100%|| 543/543 [00:58<00:00,  9.21it/s, loss=0.943]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 10 | Training Loss: 0.8748 | Validation Loss: 1.0015 | Accuracy: 0.5270\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11: 100%|| 543/543 [00:59<00:00,  9.20it/s, loss=0.948]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 11 | Training Loss: 0.8637 | Validation Loss: 0.9970 | Accuracy: 0.5328\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12: 100%|| 543/543 [00:58<00:00,  9.20it/s, loss=0.849]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 12 | Training Loss: 0.8502 | Validation Loss: 0.9952 | Accuracy: 0.5325\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 13: 100%|| 543/543 [00:58<00:00,  9.21it/s, loss=0.931]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 13 | Training Loss: 0.8342 | Validation Loss: 1.0039 | Accuracy: 0.5336\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 14: 100%|| 543/543 [00:58<00:00,  9.20it/s, loss=0.851]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 14 | Training Loss: 0.8152 | Validation Loss: 1.0111 | Accuracy: 0.5334\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 15: 100%|| 543/543 [00:58<00:00,  9.20it/s, loss=0.83]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 15 | Training Loss: 0.7928 | Validation Loss: 1.0076 | Accuracy: 0.5371\n",
            "Average training time per epoch: 58.98s\n",
            "Evaluating the trained model on CB513 dataset\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5197    0.4899    0.5044     46162\n",
            "           1     0.4027    0.3626    0.3816     29902\n",
            "           2     0.5613    0.6149    0.5869     58858\n",
            "\n",
            "    accuracy                         0.5162    134922\n",
            "   macro avg     0.4946    0.4891    0.4910    134922\n",
            "weighted avg     0.5119    0.5162    0.5132    134922\n",
            "\n",
            "Average Validation Loss: 1.0427, Accuracy: 0.5162\n",
            "Evaluating the trained model on TS115 dataset\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5656    0.6185    0.5909     11062\n",
            "           1     0.4133    0.3407    0.3735      5327\n",
            "           2     0.5905    0.5858    0.5882     12275\n",
            "\n",
            "    accuracy                         0.5529     28664\n",
            "   macro avg     0.5232    0.5150    0.5175     28664\n",
            "weighted avg     0.5480    0.5529    0.5493     28664\n",
            "\n",
            "Average Validation Loss: 0.9714, Accuracy: 0.5529\n",
            "Evaluating the trained model on CASP12 dataset\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5106    0.5787    0.5425      1837\n",
            "           1     0.4171    0.4067    0.4118      1249\n",
            "           2     0.6325    0.5853    0.6080      2867\n",
            "\n",
            "    accuracy                         0.5458      5953\n",
            "   macro avg     0.5200    0.5236    0.5208      5953\n",
            "weighted avg     0.5497    0.5458    0.5466      5953\n",
            "\n",
            "Average Validation Loss: 0.9587, Accuracy: 0.5458\n"
          ]
        }
      ],
      "source": [
        "# Start Training\n",
        "model, model_kwargs = ProteinTransformerSS3, {\n",
        "    'vocab_size': tokenizer.vocab_size,\n",
        "    'num_classes': 3,\n",
        "    'd_model': 512,\n",
        "    'num_layers': 12,\n",
        "    'dim_feedforward': 1024,\n",
        "    'p': 0.4,\n",
        "    'len_seq': 512,\n",
        "    'num_heads': 8,\n",
        "    'attn_mechanism': 'Linformer2D',\n",
        "    'num_peds': 30,\n",
        "    'k': 50\n",
        "}\n",
        "trained_model, avg_val_loss_Lin2D = train_model(model, model_kwargs, train_loader, val_loader, epochs=15, lr=1e-4)\n",
        "evaluate_model(trained_model, test1_loader, class_report = True)\n",
        "evaluate_model(trained_model, test2_loader, class_report = True)\n",
        "evaluate_model(trained_model, test3_loader, class_report = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yZ-HQZR8UwDA",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "yZ-HQZR8UwDA",
        "outputId": "5f205a7d-8bf7-4ec0-ce4c-df7d2d1cbfc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MultiLin2D is activated\n",
            "MultiLin2D is activated\n",
            "MultiLin2D is activated\n",
            "MultiLin2D is activated\n",
            "MultiLin2D is activated\n",
            "MultiLin2D is activated\n",
            "MultiLin2D is activated\n",
            "MultiLin2D is activated\n",
            "MultiLin2D is activated\n",
            "MultiLin2D is activated\n",
            "MultiLin2D is activated\n",
            "MultiLin2D is activated\n",
            "\n",
            "Total trainable parameters: 25,526,547\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|| 543/543 [01:11<00:00,  7.62it/s, loss=0.792]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 1 | Training Loss: 0.9216 | Validation Loss: 0.8612 | Accuracy: 0.6226\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|| 543/543 [01:11<00:00,  7.62it/s, loss=0.778]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 2 | Training Loss: 0.8300 | Validation Loss: 0.8229 | Accuracy: 0.6393\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|| 543/543 [01:11<00:00,  7.62it/s, loss=0.77]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 3 | Training Loss: 0.7898 | Validation Loss: 0.8005 | Accuracy: 0.6598\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|| 543/543 [01:11<00:00,  7.61it/s, loss=0.786]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 4 | Training Loss: 0.7523 | Validation Loss: 0.7958 | Accuracy: 0.6628\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5: 100%|| 543/543 [01:11<00:00,  7.59it/s, loss=0.734]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 5 | Training Loss: 0.7231 | Validation Loss: 0.7756 | Accuracy: 0.6700\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6: 100%|| 543/543 [01:11<00:00,  7.61it/s, loss=0.774]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 6 | Training Loss: 0.6984 | Validation Loss: 0.7762 | Accuracy: 0.6776\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7: 100%|| 543/543 [01:11<00:00,  7.61it/s, loss=0.734]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 7 | Training Loss: 0.6722 | Validation Loss: 0.7877 | Accuracy: 0.6735\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8: 100%|| 543/543 [01:11<00:00,  7.61it/s, loss=0.665]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 8 | Training Loss: 0.6466 | Validation Loss: 0.8169 | Accuracy: 0.6755\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9: 100%|| 543/543 [01:11<00:00,  7.61it/s, loss=0.64]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 9 | Training Loss: 0.6223 | Validation Loss: 0.8405 | Accuracy: 0.6722\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10: 100%|| 543/543 [01:11<00:00,  7.61it/s, loss=0.538]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 10 | Training Loss: 0.5992 | Validation Loss: 0.8652 | Accuracy: 0.6697\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11: 100%|| 543/543 [01:11<00:00,  7.56it/s, loss=0.48]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 11 | Training Loss: 0.5784 | Validation Loss: 0.8898 | Accuracy: 0.6694\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12: 100%|| 543/543 [01:11<00:00,  7.57it/s, loss=0.576]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 12 | Training Loss: 0.5599 | Validation Loss: 0.9145 | Accuracy: 0.6646\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 13: 100%|| 543/543 [01:11<00:00,  7.60it/s, loss=0.584]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 13 | Training Loss: 0.5422 | Validation Loss: 0.9377 | Accuracy: 0.6667\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 14: 100%|| 543/543 [01:11<00:00,  7.59it/s, loss=0.552]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 14 | Training Loss: 0.5267 | Validation Loss: 0.9403 | Accuracy: 0.6622\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 15: 100%|| 543/543 [01:11<00:00,  7.60it/s, loss=0.528]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 15 | Training Loss: 0.5125 | Validation Loss: 0.9731 | Accuracy: 0.6603\n",
            "Average training time per epoch: 71.44s\n",
            "Evaluating the trained model on CB513 dataset\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6981    0.6378    0.6666     46162\n",
            "           1     0.5368    0.5705    0.5531     29902\n",
            "           2     0.6634    0.6872    0.6751     58858\n",
            "\n",
            "    accuracy                         0.6444    134922\n",
            "   macro avg     0.6328    0.6318    0.6316    134922\n",
            "weighted avg     0.6472    0.6444    0.6451    134922\n",
            "\n",
            "Average Validation Loss: 1.0072, Accuracy: 0.6444\n",
            "Evaluating the trained model on TS115 dataset\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7310    0.6862    0.7079     11062\n",
            "           1     0.5442    0.5921    0.5671      5327\n",
            "           2     0.6913    0.7031    0.6971     12275\n",
            "\n",
            "    accuracy                         0.6759     28664\n",
            "   macro avg     0.6555    0.6605    0.6574     28664\n",
            "weighted avg     0.6793    0.6759    0.6771     28664\n",
            "\n",
            "Average Validation Loss: 0.9349, Accuracy: 0.6759\n",
            "Evaluating the trained model on CASP12 dataset\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6346    0.6968    0.6642      1837\n",
            "           1     0.5591    0.5564    0.5578      1249\n",
            "           2     0.7059    0.6631    0.6838      2867\n",
            "\n",
            "    accuracy                         0.6511      5953\n",
            "   macro avg     0.6332    0.6388    0.6353      5953\n",
            "weighted avg     0.6531    0.6511    0.6513      5953\n",
            "\n",
            "Average Validation Loss: 0.9907, Accuracy: 0.6511\n"
          ]
        }
      ],
      "source": [
        "# sliding block multiped\n",
        "# Start Training\n",
        "model, model_kwargs = ProteinTransformerSS3, {\n",
        "    'vocab_size': tokenizer.vocab_size,\n",
        "    'num_classes': 3,\n",
        "    'd_model': 512,\n",
        "    'num_layers': 12,\n",
        "    'dim_feedforward': 1024,\n",
        "    'p': 0.4,\n",
        "    'len_seq': 512,\n",
        "    'num_heads': 8,\n",
        "    'attn_mechanism': 'MultiLin2D',\n",
        "    'k': 10,\n",
        "    'block_size': 30,\n",
        "    'stride': 15\n",
        "}\n",
        "trained_model, avg_val_loss_MultiLin2D = train_model(model, model_kwargs, train_loader, val_loader, epochs=15, lr=1e-4)\n",
        "evaluate_model(trained_model, test1_loader, class_report = True)\n",
        "evaluate_model(trained_model, test2_loader, class_report = True)\n",
        "evaluate_model(trained_model, test3_loader, class_report = True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3BwFqQcsbT0",
      "metadata": {
        "id": "d3BwFqQcsbT0"
      },
      "source": [
        "### Training 3D Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0YKupJn_seNc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "id": "0YKupJn_seNc",
        "outputId": "4880a1bf-8899-4f19-be53-1a2d3de51d4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Plain3D is activated\n",
            "Plain3D is activated\n",
            "Plain3D is activated\n",
            "Plain3D is activated\n",
            "Plain3D is activated\n",
            "Plain3D is activated\n",
            "Plain3D is activated\n",
            "Plain3D is activated\n",
            "Plain3D is activated\n",
            "Plain3D is activated\n",
            "Plain3D is activated\n",
            "Plain3D is activated\n",
            "\n",
            "Total trainable parameters: 28,664,323\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1:   0%|          | 0/543 [00:01<?, ?it/s]\n"
          ]
        },
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 79.32 GiB of which 51.26 GiB is free. Process 12139 has 28.05 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 4.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3820391917.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;34m'k'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m }\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mtrained_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_val_loss_Plain3D\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrained_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest1_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_report\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrained_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest2_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_report\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2422956785.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model_class, model_kwargs, train_loader, val_loader, epochs, lr)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3743950802.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, labels)\u001b[0m\n\u001b[1;32m    210\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3743950802.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mffn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mffn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3018590171.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, q, mask)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;31m# Run the self-attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQuery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mL_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# Concatenate the heads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3018590171.py\u001b[0m in \u001b[0;36mself_attention\u001b[0;34m(self, q, k, l, v, mask)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mseq_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# Calculate the dot produce of Q and K ## multiplication occurs across the last dim (embedding dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mmul_qkl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'abcd,abed,abfd -> abcef'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_params\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Shape: (#samples,num_heads,len_seq,len_seq, len_seq)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;31m#print(f'shape of mul_qkl: {mul_qkl.shape}')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/functional.py\u001b[0m in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[0;31m# flatten path for dispatching to C++\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtupled_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mequation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperands\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 79.32 GiB of which 51.26 GiB is free. Process 12139 has 28.05 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 4.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "# Start Training\n",
        "model, model_kwargs = ProteinTransformerSS3, {\n",
        "    'vocab_size': tokenizer.vocab_size,\n",
        "    'num_classes': 3,\n",
        "    'd_model': 512,\n",
        "    'num_layers': 12,\n",
        "    'dim_feedforward': 1024,\n",
        "    'p': 0.1,\n",
        "    'len_seq': 512,\n",
        "    'num_heads': 8,\n",
        "    'attn_mechanism': 'Plain3D',\n",
        "    'num_peds': 30,\n",
        "    'k': 50\n",
        "}\n",
        "trained_model, avg_val_loss_Plain3D = train_model(model, model_kwargs, train_loader, val_loader, epochs=30, lr=1e-4)\n",
        "evaluate_model(trained_model, test1_loader, class_report = True)\n",
        "evaluate_model(trained_model, test2_loader, class_report = True)\n",
        "evaluate_model(trained_model, test3_loader, class_report = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xbaPBVKcHcM2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbaPBVKcHcM2",
        "outputId": "ee4e0634-364f-4fb6-b4ab-7bfc1612634f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "import gc\n",
        "#del model\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "l_mh99egOVzf",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "l_mh99egOVzf",
        "outputId": "5218051d-2bde-4064-87ef-137d9289d754"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Linformer3D is activated\n",
            "Linformer3D is activated\n",
            "Linformer3D is activated\n",
            "Linformer3D is activated\n",
            "Linformer3D is activated\n",
            "Linformer3D is activated\n",
            "Linformer3D is activated\n",
            "Linformer3D is activated\n",
            "Linformer3D is activated\n",
            "Linformer3D is activated\n",
            "Linformer3D is activated\n",
            "Linformer3D is activated\n",
            "\n",
            "Total trainable parameters: 29,587,723\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|| 543/543 [04:39<00:00,  1.94it/s, loss=0.931]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 1 | Training Loss: 1.0119 | Validation Loss: 0.9868 | Accuracy: 0.5281\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|| 543/543 [04:39<00:00,  1.94it/s, loss=0.961]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 2 | Training Loss: 0.9706 | Validation Loss: 1.0122 | Accuracy: 0.5177\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|| 543/543 [04:40<00:00,  1.94it/s, loss=0.998]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 3 | Training Loss: 0.9543 | Validation Loss: 1.0075 | Accuracy: 0.5206\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|| 543/543 [04:40<00:00,  1.94it/s, loss=0.946]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 4 | Training Loss: 0.9380 | Validation Loss: 1.0171 | Accuracy: 0.5260\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5: 100%|| 543/543 [04:40<00:00,  1.93it/s, loss=0.948]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 5 | Training Loss: 0.9252 | Validation Loss: 0.9974 | Accuracy: 0.5290\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6: 100%|| 543/543 [04:41<00:00,  1.93it/s, loss=0.933]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 6 | Training Loss: 0.9147 | Validation Loss: 0.9993 | Accuracy: 0.5304\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7: 100%|| 543/543 [04:41<00:00,  1.93it/s, loss=0.79]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 7 | Training Loss: 0.9068 | Validation Loss: 1.0005 | Accuracy: 0.5259\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8: 100%|| 543/543 [04:41<00:00,  1.93it/s, loss=0.963]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 8 | Training Loss: 0.9003 | Validation Loss: 0.9889 | Accuracy: 0.5281\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9: 100%|| 543/543 [04:41<00:00,  1.93it/s, loss=0.961]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 9 | Training Loss: 0.8940 | Validation Loss: 1.0006 | Accuracy: 0.5284\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10: 100%|| 543/543 [04:41<00:00,  1.93it/s, loss=0.985]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 10 | Training Loss: 0.8880 | Validation Loss: 0.9932 | Accuracy: 0.5266\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11: 100%|| 543/543 [04:41<00:00,  1.93it/s, loss=0.9]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 11 | Training Loss: 0.8833 | Validation Loss: 1.0076 | Accuracy: 0.5236\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12: 100%|| 543/543 [04:41<00:00,  1.93it/s, loss=0.829]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 12 | Training Loss: 0.8764 | Validation Loss: 1.0059 | Accuracy: 0.5252\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 13: 100%|| 543/543 [04:41<00:00,  1.93it/s, loss=0.777]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 13 | Training Loss: 0.8686 | Validation Loss: 0.9838 | Accuracy: 0.5265\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 14: 100%|| 543/543 [04:41<00:00,  1.93it/s, loss=0.792]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 14 | Training Loss: 0.8614 | Validation Loss: 0.9990 | Accuracy: 0.5241\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 15: 100%|| 543/543 [04:41<00:00,  1.93it/s, loss=0.768]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 15 | Training Loss: 0.8534 | Validation Loss: 1.0033 | Accuracy: 0.5255\n",
            "Average training time per epoch: 280.83s\n",
            "Evaluating the trained model on CB513 dataset\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4757    0.4992    0.4872     46162\n",
            "           1     0.3937    0.3025    0.3421     29902\n",
            "           2     0.5566    0.6006    0.5778     58858\n",
            "\n",
            "    accuracy                         0.4998    134922\n",
            "   macro avg     0.4753    0.4674    0.4690    134922\n",
            "weighted avg     0.4928    0.4998    0.4945    134922\n",
            "\n",
            "Average Validation Loss: 1.0435, Accuracy: 0.4998\n",
            "Evaluating the trained model on TS115 dataset\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5425    0.6359    0.5855     11062\n",
            "           1     0.4308    0.2949    0.3501      5327\n",
            "           2     0.5778    0.5673    0.5725     12275\n",
            "\n",
            "    accuracy                         0.5432     28664\n",
            "   macro avg     0.5170    0.4994    0.5027     28664\n",
            "weighted avg     0.5369    0.5432    0.5362     28664\n",
            "\n",
            "Average Validation Loss: 0.9698, Accuracy: 0.5432\n",
            "Evaluating the trained model on CASP12 dataset\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5011    0.4905    0.4957      1837\n",
            "           1     0.4388    0.3907    0.4134      1249\n",
            "           2     0.5919    0.6282    0.6095      2867\n",
            "\n",
            "    accuracy                         0.5359      5953\n",
            "   macro avg     0.5106    0.5031    0.5062      5953\n",
            "weighted avg     0.5317    0.5359    0.5332      5953\n",
            "\n",
            "Average Validation Loss: 0.9692, Accuracy: 0.5359\n"
          ]
        }
      ],
      "source": [
        "# Start Training\n",
        "model, model_kwargs = ProteinTransformerSS3, {\n",
        "    'vocab_size': tokenizer.vocab_size,\n",
        "    'num_classes': 3,\n",
        "    'd_model': 512,\n",
        "    'num_layers': 12,\n",
        "    'dim_feedforward': 1024,\n",
        "    'p': 0.4,\n",
        "    'len_seq': 512,\n",
        "    'num_heads': 8,\n",
        "    'attn_mechanism': 'Linformer3D',\n",
        "    'num_peds': 30,\n",
        "    'k': 50\n",
        "}\n",
        "trained_model, avg_val_loss_Lin3D = train_model(model, model_kwargs, train_loader, val_loader, epochs=15, lr=1e-4)\n",
        "evaluate_model(trained_model, test1_loader, class_report = True)\n",
        "evaluate_model(trained_model, test2_loader, class_report = True)\n",
        "evaluate_model(trained_model, test3_loader, class_report = True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start Training\n",
        "model, model_kwargs = ProteinTransformerSS3, {\n",
        "    'vocab_size': tokenizer.vocab_size,\n",
        "    'num_classes': 3,\n",
        "    'd_model': 408,\n",
        "    'num_layers': 12,\n",
        "    'dim_feedforward': 1024,\n",
        "    'p': 0.4,\n",
        "    'len_seq': 512,\n",
        "    'num_heads': 8,\n",
        "    'attn_mechanism': 'Linformer3D_Dual',\n",
        "    'num_peds': 30,\n",
        "    'k': 50\n",
        "}\n",
        "trained_model, avg_val_loss_Lin3D = train_model(model, model_kwargs, train_loader, val_loader, epochs=15, lr=1e-4)\n",
        "evaluate_model(trained_model, test1_loader, class_report = True)\n",
        "evaluate_model(trained_model, test2_loader, class_report = True)\n",
        "evaluate_model(trained_model, test3_loader, class_report = True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwHiTmtISGGl",
        "outputId": "4b74c250-6b30-43ac-ba3a-7fdbb8aa057b"
      },
      "id": "YwHiTmtISGGl",
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Linformer3D Dual is activated\n",
            "Linformer3D Dual is activated\n",
            "Linformer3D Dual is activated\n",
            "Linformer3D Dual is activated\n",
            "Linformer3D Dual is activated\n",
            "Linformer3D Dual is activated\n",
            "Linformer3D Dual is activated\n",
            "Linformer3D Dual is activated\n",
            "Linformer3D Dual is activated\n",
            "Linformer3D Dual is activated\n",
            "Linformer3D Dual is activated\n",
            "Linformer3D Dual is activated\n",
            "\n",
            "Total trainable parameters: 32,157,579\n",
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|| 543/543 [08:39<00:00,  1.04it/s, loss=0.959]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 1 | Training Loss: 1.0031 | Validation Loss: 0.9994 | Accuracy: 0.5178\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|| 543/543 [08:26<00:00,  1.07it/s, loss=0.928]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 2 | Training Loss: 0.9650 | Validation Loss: 0.9839 | Accuracy: 0.5309\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|| 543/543 [08:26<00:00,  1.07it/s, loss=0.944]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 3 | Training Loss: 0.9489 | Validation Loss: 0.9960 | Accuracy: 0.5325\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|| 543/543 [08:27<00:00,  1.07it/s, loss=0.844]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 4 | Training Loss: 0.9330 | Validation Loss: 1.0010 | Accuracy: 0.5319\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5: 100%|| 543/543 [08:27<00:00,  1.07it/s, loss=0.875]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 5 | Training Loss: 0.9183 | Validation Loss: 0.9820 | Accuracy: 0.5364\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6: 100%|| 543/543 [08:27<00:00,  1.07it/s, loss=0.931]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 6 | Training Loss: 0.9068 | Validation Loss: 1.0014 | Accuracy: 0.5279\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7: 100%|| 543/543 [08:28<00:00,  1.07it/s, loss=0.839]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 7 | Training Loss: 0.8973 | Validation Loss: 0.9817 | Accuracy: 0.5340\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8: 100%|| 543/543 [08:28<00:00,  1.07it/s, loss=0.862]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 8 | Training Loss: 0.8885 | Validation Loss: 0.9823 | Accuracy: 0.5298\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9: 100%|| 543/543 [08:28<00:00,  1.07it/s, loss=0.817]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 9 | Training Loss: 0.8795 | Validation Loss: 0.9833 | Accuracy: 0.5306\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10: 100%|| 543/543 [08:29<00:00,  1.07it/s, loss=0.866]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 10 | Training Loss: 0.8698 | Validation Loss: 0.9910 | Accuracy: 0.5285\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11: 100%|| 543/543 [08:29<00:00,  1.07it/s, loss=0.73]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 11 | Training Loss: 0.8589 | Validation Loss: 1.0327 | Accuracy: 0.5273\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12: 100%|| 543/543 [08:29<00:00,  1.07it/s, loss=0.795]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 12 | Training Loss: 0.8466 | Validation Loss: 1.0019 | Accuracy: 0.5263\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 13: 100%|| 543/543 [08:29<00:00,  1.07it/s, loss=0.809]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 13 | Training Loss: 0.8329 | Validation Loss: 1.0174 | Accuracy: 0.5189\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 14: 100%|| 543/543 [08:29<00:00,  1.07it/s, loss=0.7]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 14 | Training Loss: 0.8165 | Validation Loss: 1.0335 | Accuracy: 0.5201\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15:  29%|       | 157/543 [02:27<06:02,  1.07it/s, loss=0.8]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wYEh5t37Phvd",
      "metadata": {
        "id": "wYEh5t37Phvd"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "#del model\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8Bm4f7BfZkuP",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "8Bm4f7BfZkuP",
        "outputId": "3c383255-576d-4341-c08f-ddafa4cb512b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Multiped3D is activated\n",
            "Multiped3D is activated\n",
            "Multiped3D is activated\n",
            "Multiped3D is activated\n",
            "Multiped3D is activated\n",
            "Multiped3D is activated\n",
            "Multiped3D is activated\n",
            "Multiped3D is activated\n",
            "Multiped3D is activated\n",
            "Multiped3D is activated\n",
            "Multiped3D is activated\n",
            "Multiped3D is activated\n",
            "\n",
            "Total trainable parameters: 20,303,763\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'sympy' has no attribute 'printing'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1543346654.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;34m'stride'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m }\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mtrained_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_val_loss_Mutli3D\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrained_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest1_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_report\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrained_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest2_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_report\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2422956785.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model_class, model_kwargs, train_loader, val_loader, epochs, lr)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nTotal trainable parameters: {total_params:,}\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize, capturable, differentiable, fused, decoupled_weight_decay)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0mdecoupled_weight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoupled_weight_decay\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         )\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfused\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, defaults)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam_group\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparam_groups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_param_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_group\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0;31m# Allows _cuda_graph_capture_health_check to rig a poor man's TORCH_WARN_ONCE in python,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_compile.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mdisable_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__dynamo_disable\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdisable_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                 \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0;31m# We can safely turn off functools.wraps here because the inner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume_execution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregistry\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlist_backends\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlookup_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregister_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcallback\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcallback_handler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_compile_end\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_compile_start\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCallbackTrigger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_compile_pg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymbolic_convert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensorifyState\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_guards\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompile_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCompileContext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCompileId\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logging\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstructured\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensorifyScalarRestartAnalysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_guards\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtracing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTracingContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstructured\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdump_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/exc.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcounters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_functorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymbolic_shapes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pytree\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpytree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/fx/experimental/symbolic_shapes.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ordered_set\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOrderedSet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_dispatch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_traceable_wrapper_subclass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m from torch.utils._sympy.functions import (\n\u001b[0m\u001b[1;32m     76\u001b[0m     \u001b[0mApplication\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mCeilToInt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_sympy/functions.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;31m# Right now, FloorDiv de facto changes behavior if arguments are negative or\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;31m# not, this can potentially cause correctness issues.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mFloorDiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msympy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFunction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m     \"\"\"\n\u001b[1;32m    185\u001b[0m     \u001b[0mWe\u001b[0m \u001b[0mmaintain\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mso\u001b[0m \u001b[0mthat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_sympy/functions.py\u001b[0m in \u001b[0;36mFloorDiv\u001b[0;34m()\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m_sympystr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprinter\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msympy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprinting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStrPrinter\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0mbase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparenthesize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPRECEDENCE\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Atom\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0mdivisor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparenthesize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdivisor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPRECEDENCE\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Atom\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'sympy' has no attribute 'printing'"
          ]
        }
      ],
      "source": [
        "## Multiped sliding block version\n",
        "# Start Training\n",
        "model, model_kwargs = ProteinTransformerSS3, {\n",
        "    'vocab_size': tokenizer.vocab_size,\n",
        "    'num_classes': 3,\n",
        "    'd_model': 408,\n",
        "    'num_layers': 12,\n",
        "    'dim_feedforward': 1024,\n",
        "    'p': 0.4,\n",
        "    'len_seq': 512,\n",
        "    'num_heads': 8,\n",
        "    'attn_mechanism': 'Multiped3D',\n",
        "    'k': 50,\n",
        "    'block_size': 30,\n",
        "    'stride': 15\n",
        "}\n",
        "trained_model, avg_val_loss_Mutli3D = train_model(model, model_kwargs, train_loader, val_loader, epochs=15, lr=1e-4)\n",
        "evaluate_model(trained_model, test1_loader, class_report = True)\n",
        "evaluate_model(trained_model, test2_loader, class_report = True)\n",
        "evaluate_model(trained_model, test3_loader, class_report = True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Multiped sliding block version\n",
        "# Start Training\n",
        "model, model_kwargs = ProteinTransformerSS3, {\n",
        "    'vocab_size': tokenizer.vocab_size,\n",
        "    'num_classes': 3,\n",
        "    'd_model': 408,\n",
        "    'num_layers': 12,\n",
        "    'dim_feedforward': 1024,\n",
        "    'p': 0.4,\n",
        "    'len_seq': 512,\n",
        "    'num_heads': 8,\n",
        "    'attn_mechanism': 'Multiped3D_Dual',\n",
        "    'k': 50,\n",
        "    'block_size': 30,\n",
        "    'stride': 15\n",
        "}\n",
        "trained_model, avg_val_loss_Mutli3D = train_model(model, model_kwargs, train_loader, val_loader, epochs=15, lr=1e-4)\n",
        "evaluate_model(trained_model, test1_loader, class_report = True)\n",
        "evaluate_model(trained_model, test2_loader, class_report = True)\n",
        "evaluate_model(trained_model, test3_loader, class_report = True)"
      ],
      "metadata": {
        "id": "l-3oeXpdXtjL"
      },
      "id": "l-3oeXpdXtjL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RLC3KXukWGQf",
      "metadata": {
        "id": "RLC3KXukWGQf"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "#del model\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8vTYuyza1nf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8vTYuyza1nf",
        "outputId": "09ae8299-78b6-46d0-ec99-e9e458a0cd12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attn3DLinformerMultiPed is activated\n",
            "Attn3DLinformerMultiPed is activated\n",
            "Attn3DLinformerMultiPed is activated\n",
            "Attn3DLinformerMultiPed is activated\n",
            "Attn3DLinformerMultiPed is activated\n",
            "Attn3DLinformerMultiPed is activated\n",
            "Attn3DLinformerMultiPed is activated\n",
            "Attn3DLinformerMultiPed is activated\n",
            "Attn3DLinformerMultiPed is activated\n",
            "Attn3DLinformerMultiPed is activated\n",
            "Attn3DLinformerMultiPed is activated\n",
            "Attn3DLinformerMultiPed is activated\n",
            "\n",
            "Total trainable parameters: 28,704,459\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|| 543/543 [07:25<00:00,  1.22it/s, loss=0.937]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 1 | Training Loss: 0.9447 | Validation Loss: 0.8795 | Accuracy: 0.6077\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|| 543/543 [07:25<00:00,  1.22it/s, loss=0.791]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 2 | Training Loss: 0.8304 | Validation Loss: 0.8532 | Accuracy: 0.6384\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|| 543/543 [07:25<00:00,  1.22it/s, loss=0.781]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 3 | Training Loss: 0.7836 | Validation Loss: 0.7832 | Accuracy: 0.6623\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|| 543/543 [07:25<00:00,  1.22it/s, loss=0.799]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 4 | Training Loss: 0.7419 | Validation Loss: 0.8134 | Accuracy: 0.6669\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5: 100%|| 543/543 [07:25<00:00,  1.22it/s, loss=0.649]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 5 | Training Loss: 0.6980 | Validation Loss: 0.8366 | Accuracy: 0.6664\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6: 100%|| 543/543 [07:25<00:00,  1.22it/s, loss=0.535]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 6 | Training Loss: 0.6545 | Validation Loss: 0.8725 | Accuracy: 0.6608\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7: 100%|| 543/543 [07:25<00:00,  1.22it/s, loss=0.511]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 7 | Training Loss: 0.6131 | Validation Loss: 0.8890 | Accuracy: 0.6599\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8: 100%|| 543/543 [07:25<00:00,  1.22it/s, loss=0.565]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 8 | Training Loss: 0.5778 | Validation Loss: 0.9662 | Accuracy: 0.6553\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9: 100%|| 543/543 [07:25<00:00,  1.22it/s, loss=0.54]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 9 | Training Loss: 0.5472 | Validation Loss: 0.9820 | Accuracy: 0.6549\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10: 100%|| 543/543 [07:25<00:00,  1.22it/s, loss=0.444]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 10 | Training Loss: 0.5213 | Validation Loss: 1.0287 | Accuracy: 0.6496\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11: 100%|| 543/543 [07:25<00:00,  1.22it/s, loss=0.509]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 11 | Training Loss: 0.4979 | Validation Loss: 1.0605 | Accuracy: 0.6486\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12: 100%|| 543/543 [07:25<00:00,  1.22it/s, loss=0.517]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 12 | Training Loss: 0.4747 | Validation Loss: 1.1364 | Accuracy: 0.6458\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 13: 100%|| 543/543 [07:25<00:00,  1.22it/s, loss=0.402]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 13 | Training Loss: 0.4540 | Validation Loss: 1.1381 | Accuracy: 0.6424\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 14: 100%|| 543/543 [07:25<00:00,  1.22it/s, loss=0.378]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 14 | Training Loss: 0.4352 | Validation Loss: 1.1760 | Accuracy: 0.6373\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 15: 100%|| 543/543 [07:25<00:00,  1.22it/s, loss=0.402]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 15 | Training Loss: 0.4144 | Validation Loss: 1.2119 | Accuracy: 0.6364\n",
            "Average training time per epoch: 445.63s\n",
            "Evaluating the trained model on CB513 dataset\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6408    0.6392    0.6400     46162\n",
            "           1     0.5279    0.4816    0.5037     29902\n",
            "           2     0.6444    0.6743    0.6590     58858\n",
            "\n",
            "    accuracy                         0.6196    134922\n",
            "   macro avg     0.6044    0.5984    0.6009    134922\n",
            "weighted avg     0.6173    0.6196    0.6181    134922\n",
            "\n",
            "Average Validation Loss: 1.2461, Accuracy: 0.6196\n",
            "Evaluating the trained model on TS115 dataset\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6789    0.6952    0.6869     11062\n",
            "           1     0.5510    0.4969    0.5226      5327\n",
            "           2     0.6635    0.6775    0.6704     12275\n",
            "\n",
            "    accuracy                         0.6507     28664\n",
            "   macro avg     0.6311    0.6232    0.6266     28664\n",
            "weighted avg     0.6486    0.6507    0.6493     28664\n",
            "\n",
            "Average Validation Loss: 1.1541, Accuracy: 0.6507\n",
            "Evaluating the trained model on CASP12 dataset\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5782    0.7006    0.6335      1837\n",
            "           1     0.5716    0.4540    0.5060      1249\n",
            "           2     0.6757    0.6446    0.6598      2867\n",
            "\n",
            "    accuracy                         0.6219      5953\n",
            "   macro avg     0.6085    0.5997    0.5998      5953\n",
            "weighted avg     0.6237    0.6219    0.6194      5953\n",
            "\n",
            "Average Validation Loss: 1.3157, Accuracy: 0.6219\n"
          ]
        }
      ],
      "source": [
        "#### Multiped Sliding window\n",
        "# Start Training\n",
        "model, model_kwargs = ProteinTransformerSS3, {\n",
        "    'vocab_size': tokenizer.vocab_size,\n",
        "    'num_classes': 3,\n",
        "    'd_model': 512,\n",
        "    'num_layers': 12,\n",
        "    'dim_feedforward': 1024,\n",
        "    'p': 0.4,\n",
        "    'len_seq': 512,\n",
        "    'num_heads': 8,\n",
        "    'attn_mechanism': 'Attn3DLinformerMultiPed',\n",
        "    'k': 30,\n",
        "    'block_size': 30,\n",
        "    'stride': 15\n",
        "}\n",
        "trained_model, avg_val_loss_MutliLin3D = train_model(model, model_kwargs, train_loader, val_loader, epochs=15, lr=1e-4)\n",
        "evaluate_model(trained_model, test1_loader, class_report = True)\n",
        "evaluate_model(trained_model, test2_loader, class_report = True)\n",
        "evaluate_model(trained_model, test3_loader, class_report = True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### Multiped Sliding window\n",
        "# Start Training\n",
        "model, model_kwargs = ProteinTransformerSS3, {\n",
        "    'vocab_size': tokenizer.vocab_size,\n",
        "    'num_classes': 3,\n",
        "    'd_model': 512,\n",
        "    'num_layers': 12,\n",
        "    'dim_feedforward': 1024,\n",
        "    'p': 0.4,\n",
        "    'len_seq': 512,\n",
        "    'num_heads': 8,\n",
        "    'attn_mechanism': 'MultiLin3D_Dual',\n",
        "    'k': 30,\n",
        "    'block_size': 30,\n",
        "    'stride': 15\n",
        "}\n",
        "trained_model, avg_val_loss_MutliLin3D = train_model(model, model_kwargs, train_loader, val_loader, epochs=15, lr=1e-4)\n",
        "evaluate_model(trained_model, test1_loader, class_report = True)\n",
        "evaluate_model(trained_model, test2_loader, class_report = True)\n",
        "evaluate_model(trained_model, test3_loader, class_report = True)"
      ],
      "metadata": {
        "id": "j78vHChWXyg4"
      },
      "id": "j78vHChWXyg4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "O0Socmk37vxM",
      "metadata": {
        "id": "O0Socmk37vxM"
      },
      "source": [
        "### Training Combined Attns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ub_lI0AVbObs",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ub_lI0AVbObs",
        "outputId": "c39003f8-7af7-4f75-812b-01c44df38a3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attn2DMultiPed3Dselected is activated\n",
            "Attn2DMultiPed3Dselected is activated\n",
            "Attn2DMultiPed3Dselected is activated\n",
            "Attn2DMultiPed3Dselected is activated\n",
            "Attn2DMultiPed3Dselected is activated\n",
            "Attn2DMultiPed3Dselected is activated\n",
            "Attn2DMultiPed3Dselected is activated\n",
            "Attn2DMultiPed3Dselected is activated\n",
            "Attn2DMultiPed3Dselected is activated\n",
            "Attn2DMultiPed3Dselected is activated\n",
            "Attn2DMultiPed3Dselected is activated\n",
            "Attn2DMultiPed3Dselected is activated\n",
            "\n",
            "Total trainable parameters: 28,770,051\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|| 543/543 [02:30<00:00,  3.60it/s, loss=0.857]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 1 | Training Loss: 0.9689 | Validation Loss: 0.9274 | Accuracy: 0.5689\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|| 543/543 [02:30<00:00,  3.60it/s, loss=0.89]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 2 | Training Loss: 0.9021 | Validation Loss: 0.9141 | Accuracy: 0.5704\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|| 543/543 [02:30<00:00,  3.61it/s, loss=0.864]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 3 | Training Loss: 0.8699 | Validation Loss: 0.8593 | Accuracy: 0.6153\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|| 543/543 [02:30<00:00,  3.60it/s, loss=0.822]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 4 | Training Loss: 0.8331 | Validation Loss: 0.8208 | Accuracy: 0.6383\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5: 100%|| 543/543 [02:30<00:00,  3.61it/s, loss=0.795]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 5 | Training Loss: 0.8014 | Validation Loss: 0.8251 | Accuracy: 0.6444\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6: 100%|| 543/543 [02:30<00:00,  3.61it/s, loss=0.803]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 6 | Training Loss: 0.7782 | Validation Loss: 0.7876 | Accuracy: 0.6629\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7: 100%|| 543/543 [02:30<00:00,  3.61it/s, loss=0.753]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 7 | Training Loss: 0.7538 | Validation Loss: 0.7957 | Accuracy: 0.6635\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8: 100%|| 543/543 [02:30<00:00,  3.61it/s, loss=0.824]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 8 | Training Loss: 0.7344 | Validation Loss: 0.7923 | Accuracy: 0.6652\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9: 100%|| 543/543 [02:30<00:00,  3.61it/s, loss=0.719]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 9 | Training Loss: 0.7181 | Validation Loss: 0.8453 | Accuracy: 0.6609\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10: 100%|| 543/543 [02:30<00:00,  3.61it/s, loss=0.764]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 10 | Training Loss: 0.7012 | Validation Loss: 0.8061 | Accuracy: 0.6708\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11: 100%|| 543/543 [02:30<00:00,  3.61it/s, loss=0.789]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 11 | Training Loss: 0.6860 | Validation Loss: 0.8130 | Accuracy: 0.6746\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12: 100%|| 543/543 [02:30<00:00,  3.61it/s, loss=0.721]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 12 | Training Loss: 0.6727 | Validation Loss: 0.8197 | Accuracy: 0.6740\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 13: 100%|| 543/543 [02:30<00:00,  3.61it/s, loss=0.688]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 13 | Training Loss: 0.6572 | Validation Loss: 0.8513 | Accuracy: 0.6703\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 14: 100%|| 543/543 [02:30<00:00,  3.61it/s, loss=0.671]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 14 | Training Loss: 0.6466 | Validation Loss: 0.8432 | Accuracy: 0.6741\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 15: 100%|| 543/543 [02:30<00:00,  3.61it/s, loss=0.59]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 15 | Training Loss: 0.6311 | Validation Loss: 0.8307 | Accuracy: 0.6761\n",
            "Average training time per epoch: 150.59s\n",
            "Evaluating the trained model on CB513 dataset\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6840    0.6854    0.6847     46162\n",
            "           1     0.5881    0.4976    0.5391     29902\n",
            "           2     0.6678    0.7190    0.6924     58858\n",
            "\n",
            "    accuracy                         0.6584    134922\n",
            "   macro avg     0.6467    0.6340    0.6387    134922\n",
            "weighted avg     0.6557    0.6584    0.6558    134922\n",
            "\n",
            "Average Validation Loss: 0.8760, Accuracy: 0.6584\n",
            "Evaluating the trained model on TS115 dataset\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7157    0.7361    0.7258     11062\n",
            "           1     0.6017    0.5132    0.5539      5327\n",
            "           2     0.6898    0.7161    0.7027     12275\n",
            "\n",
            "    accuracy                         0.6861     28664\n",
            "   macro avg     0.6691    0.6551    0.6608     28664\n",
            "weighted avg     0.6834    0.6861    0.6840     28664\n",
            "\n",
            "Average Validation Loss: 0.8090, Accuracy: 0.6861\n",
            "Evaluating the trained model on CASP12 dataset\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6391    0.7414    0.6865      1837\n",
            "           1     0.6018    0.4852    0.5372      1249\n",
            "           2     0.7080    0.6952    0.7015      2867\n",
            "\n",
            "    accuracy                         0.6654      5953\n",
            "   macro avg     0.6496    0.6406    0.6417      5953\n",
            "weighted avg     0.6645    0.6654    0.6624      5953\n",
            "\n",
            "Average Validation Loss: 0.9025, Accuracy: 0.6654\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Start Training\n",
        "model, model_kwargs = ProteinTransformerSS3, {\n",
        "    'vocab_size': tokenizer.vocab_size,\n",
        "    'num_classes': 3,\n",
        "    'd_model': 512,\n",
        "    'num_layers': 12,\n",
        "    'dim_feedforward': 1024,\n",
        "    'p': 0.4,\n",
        "    'len_seq': 512,\n",
        "    'num_heads': 8,\n",
        "    'attn_mechanism': 'Attn2DMultiPed3Dselected',\n",
        "    'k': 30,\n",
        "    'block_size': 30,\n",
        "    'stride': 15\n",
        "}b\n",
        "trained_model, avg_val_loss_MutliLin3D = train_model(model, model_kwargs, train_loader, val_loader, epochs=15, lr=1e-4)\n",
        "evaluate_model(trained_model, test1_loader, class_report = True)\n",
        "evaluate_model(trained_model, test2_loader, class_report = True)\n",
        "evaluate_model(trained_model, test3_loader, class_report = True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Start Training\n",
        "model, model_kwargs = ProteinTransformerSS3, {\n",
        "    'vocab_size': tokenizer.vocab_size,\n",
        "    'num_classes': 3,\n",
        "    'd_model': 408,\n",
        "    'num_layers': 12,\n",
        "    'dim_feedforward': 1024,\n",
        "    'p': 0.4,\n",
        "    'len_seq': 512,\n",
        "    'num_heads': 8,\n",
        "    'attn_mechanism': 'Attn2DMultiPed3Dselected_Dual',\n",
        "    'k': 30,\n",
        "    'block_size': 30,\n",
        "    'stride': 15\n",
        "}\n",
        "trained_model, avg_val_loss_MutliLin3D = train_model(model, model_kwargs, train_loader, val_loader, epochs=15, lr=1e-4)\n",
        "evaluate_model(trained_model, test1_loader, class_report = True)\n",
        "evaluate_model(trained_model, test2_loader, class_report = True)\n",
        "evaluate_model(trained_model, test3_loader, class_report = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9zdmEAIX8uQ",
        "outputId": "e3080790-db89-49c3-ea28-eeacad429b62"
      },
      "id": "q9zdmEAIX8uQ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attn2DMultiPed3Dselected_Dual is activated\n",
            "Attn2DMultiPed3Dselected_Dual is activated\n",
            "Attn2DMultiPed3Dselected_Dual is activated\n",
            "Attn2DMultiPed3Dselected_Dual is activated\n",
            "Attn2DMultiPed3Dselected_Dual is activated\n",
            "Attn2DMultiPed3Dselected_Dual is activated\n",
            "Attn2DMultiPed3Dselected_Dual is activated\n",
            "Attn2DMultiPed3Dselected_Dual is activated\n",
            "Attn2DMultiPed3Dselected_Dual is activated\n",
            "Attn2DMultiPed3Dselected_Dual is activated\n",
            "Attn2DMultiPed3Dselected_Dual is activated\n",
            "Attn2DMultiPed3Dselected_Dual is activated\n",
            "\n",
            "Total trainable parameters: 28,376,655\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|| 543/543 [03:47<00:00,  2.38it/s, loss=0.996]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 1 | Training Loss: 0.9744 | Validation Loss: 0.9369 | Accuracy: 0.5529\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|| 543/543 [03:48<00:00,  2.38it/s, loss=0.967]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 2 | Training Loss: 0.9370 | Validation Loss: 0.9413 | Accuracy: 0.5571\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|| 543/543 [03:47<00:00,  2.38it/s, loss=0.927]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 3 | Training Loss: 0.9279 | Validation Loss: 0.9216 | Accuracy: 0.5623\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|| 543/543 [03:47<00:00,  2.38it/s, loss=0.876]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 4 | Training Loss: 0.9154 | Validation Loss: 0.9537 | Accuracy: 0.5746\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|| 543/543 [03:47<00:00,  2.39it/s, loss=0.804]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 5 | Training Loss: 0.8857 | Validation Loss: 0.9076 | Accuracy: 0.5901\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6: 100%|| 543/543 [03:47<00:00,  2.39it/s, loss=0.875]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 6 | Training Loss: 0.8572 | Validation Loss: 0.8857 | Accuracy: 0.6051\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7: 100%|| 543/543 [03:47<00:00,  2.39it/s, loss=0.87]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 7 | Training Loss: 0.8380 | Validation Loss: 0.8613 | Accuracy: 0.6146\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8: 100%|| 543/543 [03:47<00:00,  2.39it/s, loss=0.768]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 8 | Training Loss: 0.8208 | Validation Loss: 0.8623 | Accuracy: 0.6190\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9: 100%|| 543/543 [03:47<00:00,  2.38it/s, loss=0.857]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 9 | Training Loss: 0.8082 | Validation Loss: 0.8616 | Accuracy: 0.6244\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10: 100%|| 543/543 [03:47<00:00,  2.39it/s, loss=0.839]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 10 | Training Loss: 0.7937 | Validation Loss: 0.8510 | Accuracy: 0.6174\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11: 100%|| 543/543 [03:47<00:00,  2.39it/s, loss=0.835]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 11 | Training Loss: 0.7811 | Validation Loss: 0.8464 | Accuracy: 0.6316\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12: 100%|| 543/543 [03:47<00:00,  2.39it/s, loss=0.85]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 12 | Training Loss: 0.7669 | Validation Loss: 0.8607 | Accuracy: 0.6280\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13: 100%|| 543/543 [03:47<00:00,  2.39it/s, loss=0.836]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 13 | Training Loss: 0.7544 | Validation Loss: 0.8536 | Accuracy: 0.6340\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14: 100%|| 543/543 [03:47<00:00,  2.39it/s, loss=0.753]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 14 | Training Loss: 0.7415 | Validation Loss: 0.8568 | Accuracy: 0.6299\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15: 100%|| 543/543 [03:47<00:00,  2.38it/s, loss=0.788]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 15 | Training Loss: 0.7312 | Validation Loss: 0.8593 | Accuracy: 0.6357\n",
            "Average training time per epoch: 227.67s\n",
            "Evaluating the trained model on CB513 dataset\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6200    0.6379    0.6288     46162\n",
            "           1     0.5594    0.4181    0.4786     29902\n",
            "           2     0.6448    0.7129    0.6771     58858\n",
            "\n",
            "    accuracy                         0.6219    134922\n",
            "   macro avg     0.6081    0.5896    0.5948    134922\n",
            "weighted avg     0.6174    0.6219    0.6166    134922\n",
            "\n",
            "Average Validation Loss: 0.8805, Accuracy: 0.6219\n",
            "Evaluating the trained model on TS115 dataset\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6717    0.7011    0.6861     11062\n",
            "           1     0.5631    0.4408    0.4945      5327\n",
            "           2     0.6714    0.7082    0.6893     12275\n",
            "\n",
            "    accuracy                         0.6558     28664\n",
            "   macro avg     0.6354    0.6167    0.6233     28664\n",
            "weighted avg     0.6514    0.6558    0.6519     28664\n",
            "\n",
            "Average Validation Loss: 0.8055, Accuracy: 0.6558\n",
            "Evaluating the trained model on CASP12 dataset\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5720    0.7175    0.6366      1837\n",
            "           1     0.5940    0.3795    0.4631      1249\n",
            "           2     0.6931    0.6892    0.6912      2867\n",
            "\n",
            "    accuracy                         0.6330      5953\n",
            "   macro avg     0.6197    0.5954    0.5969      5953\n",
            "weighted avg     0.6349    0.6330    0.6265      5953\n",
            "\n",
            "Average Validation Loss: 0.8842, Accuracy: 0.6330\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jiUNbG3gMoSM",
      "metadata": {
        "id": "jiUNbG3gMoSM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Z9ZsHxA77u49",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Z9ZsHxA77u49",
        "outputId": "1ede3366-9ba9-4290-c3c9-4567c28230bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Combined2D3D is activated\n",
            "Total trainable parameters: 4,103\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1:   0%|          | 2/543 [00:02<08:38,  1.04it/s, loss=1.09]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "shape of dotqk: torch.Size([16, 4, 20, 20])\n",
            "shape of the mask applied: torch.Size([16, 1, 1, 20])\n",
            "mask applied\n",
            "shape of mul_qkl: torch.Size([16, 4, 20, 20, 20])\n",
            "shape of the mask received: torch.Size([16, 1, 1, 1, 20])\n",
            "mask applied\n",
            "shape of dotqk: torch.Size([16, 4, 20, 20])\n",
            "shape of the mask applied: torch.Size([16, 1, 1, 20])\n",
            "mask applied\n",
            "shape of mul_qkl: torch.Size([16, 4, 20, 20, 20])\n",
            "shape of the mask received: torch.Size([16, 1, 1, 1, 20])\n",
            "mask applied\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1:   1%|          | 4/543 [00:04<08:13,  1.09it/s, loss=1.12]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "shape of dotqk: torch.Size([16, 4, 20, 20])\n",
            "shape of the mask applied: torch.Size([16, 1, 1, 20])\n",
            "mask applied\n",
            "shape of mul_qkl: torch.Size([16, 4, 20, 20, 20])\n",
            "shape of the mask received: torch.Size([16, 1, 1, 1, 20])\n",
            "mask applied\n",
            "shape of dotqk: torch.Size([16, 4, 20, 20])\n",
            "shape of the mask applied: torch.Size([16, 1, 1, 20])\n",
            "mask applied\n",
            "shape of mul_qkl: torch.Size([16, 4, 20, 20, 20])\n",
            "shape of the mask received: torch.Size([16, 1, 1, 1, 20])\n",
            "mask applied\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1:   1%|          | 6/543 [00:04<04:17,  2.08it/s, loss=1.17]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "shape of dotqk: torch.Size([16, 4, 20, 20])\n",
            "shape of the mask applied: torch.Size([16, 1, 1, 20])\n",
            "mask applied\n",
            "shape of mul_qkl: torch.Size([16, 4, 20, 20, 20])\n",
            "shape of the mask received: torch.Size([16, 1, 1, 1, 20])\n",
            "mask applied\n",
            "shape of dotqk: torch.Size([16, 4, 20, 20])\n",
            "shape of the mask applied: torch.Size([16, 1, 1, 20])\n",
            "mask applied\n",
            "shape of mul_qkl: torch.Size([16, 4, 20, 20, 20])\n",
            "shape of the mask received: torch.Size([16, 1, 1, 1, 20])\n",
            "mask applied\n",
            "shape of dotqk: torch.Size([16, 4, 20, 20])\n",
            "shape of the mask applied: torch.Size([16, 1, 1, 20])\n",
            "mask applied\n",
            "shape of mul_qkl: torch.Size([16, 4, 20, 20, 20])\n",
            "shape of the mask received: torch.Size([16, 1, 1, 1, 20])\n",
            "mask applied\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1:   2%|         | 10/543 [00:04<01:59,  4.45it/s, loss=1.05]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "shape of dotqk: torch.Size([16, 4, 20, 20])\n",
            "shape of the mask applied: torch.Size([16, 1, 1, 20])\n",
            "mask applied\n",
            "shape of mul_qkl: torch.Size([16, 4, 20, 20, 20])\n",
            "shape of the mask received: torch.Size([16, 1, 1, 1, 20])\n",
            "mask applied\n",
            "shape of dotqk: torch.Size([16, 4, 20, 20])\n",
            "shape of the mask applied: torch.Size([16, 1, 1, 20])\n",
            "mask applied\n",
            "shape of mul_qkl: torch.Size([16, 4, 20, 20, 20])\n",
            "shape of the mask received: torch.Size([16, 1, 1, 1, 20])\n",
            "mask applied\n",
            "shape of dotqk: torch.Size([16, 4, 20, 20])\n",
            "shape of the mask applied: torch.Size([16, 1, 1, 20])\n",
            "mask applied\n",
            "shape of mul_qkl: torch.Size([16, 4, 20, 20, 20])\n",
            "shape of the mask received: torch.Size([16, 1, 1, 1, 20])\n",
            "mask applied\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1:   2%|         | 13/543 [00:06<02:27,  3.59it/s, loss=1.03]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "shape of dotqk: torch.Size([16, 4, 20, 20])\n",
            "shape of the mask applied: torch.Size([16, 1, 1, 20])\n",
            "mask applied\n",
            "shape of mul_qkl: torch.Size([16, 4, 20, 20, 20])\n",
            "shape of the mask received: torch.Size([16, 1, 1, 1, 20])\n",
            "mask applied\n",
            "shape of dotqk: torch.Size([16, 4, 20, 20])\n",
            "shape of the mask applied: torch.Size([16, 1, 1, 20])\n",
            "mask applied\n",
            "shape of mul_qkl: torch.Size([16, 4, 20, 20, 20])\n",
            "shape of the mask received: torch.Size([16, 1, 1, 1, 20])\n",
            "mask applied\n",
            "shape of dotqk: torch.Size([16, 4, 20, 20])\n",
            "shape of the mask applied: torch.Size([16, 1, 1, 20])\n",
            "mask applied\n",
            "shape of mul_qkl: torch.Size([16, 4, 20, 20, 20])\n",
            "shape of the mask received: torch.Size([16, 1, 1, 1, 20])\n",
            "mask applied\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch 1:   2%|         | 13/543 [00:06<02:27,  3.59it/s, loss=1.11]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "shape of dotqk: torch.Size([16, 4, 20, 20])\n",
            "shape of the mask applied: torch.Size([16, 1, 1, 20])\n",
            "mask applied\n",
            "shape of mul_qkl: torch.Size([16, 4, 20, 20, 20])\n",
            "shape of the mask received: torch.Size([16, 1, 1, 1, 20])\n",
            "mask applied\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1:   3%|         | 15/543 [00:06<02:53,  3.04it/s, loss=1.09]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "shape of dotqk: torch.Size([16, 4, 20, 20])\n",
            "shape of the mask applied: torch.Size([16, 1, 1, 20])\n",
            "mask applied\n",
            "shape of mul_qkl: torch.Size([16, 4, 20, 20, 20])\n",
            "shape of the mask received: torch.Size([16, 1, 1, 1, 20])\n",
            "mask applied\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch 1:   3%|         | 15/543 [00:08<04:59,  1.76it/s, loss=1.09]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3506213086.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m }\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mtrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mss_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-3328863698.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model_class, model_kwargs, train_loader, epochs, lr)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mpbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Epoch {epoch+1}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    731\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    734\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-160387913.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m       \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m       \u001b[0;31m# Tokenize without special tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m       \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'primary'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# List of single-letter amino acids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tape/datasets.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtxn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m                 \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m'id'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                     \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Start Training\n",
        "model, model_kwargs = ProteinTransformerSS3, {\n",
        "    'vocab_size': tokenizer.vocab_size,\n",
        "    'num_classes': 3,\n",
        "    'd_model': 512,\n",
        "    'num_layers': 12,\n",
        "    'dim_feedforward': 1024,\n",
        "    'p': 0.1,\n",
        "    'len_seq': 512,\n",
        "    'num_heads': 8,\n",
        "    'attn_mechanism': 'Combined2D3D',\n",
        "    'num_peds': 30,\n",
        "    'k': 50\n",
        "}\n",
        "trained_model, avg_val_loss_Comb2D3D = train_model(model, model_kwargs, train_loader, val_loader, epochs=30, lr=1e-4)\n",
        "evaluate_model(trained_model, test1_loader, class_report = True)\n",
        "evaluate_model(trained_model, test2_loader, class_report = True)\n",
        "evaluate_model(trained_model, test3_loader, class_report = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zjJTKJbP95Ru",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjJTKJbP95Ru",
        "outputId": "d8b80e26-ead5-4969-ad50-9bbdee5c15c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attn2D3DLin is activated\n",
            "Attn2D3DLin is activated\n",
            "Attn2D3DLin is activated\n",
            "Attn2D3DLin is activated\n",
            "Attn2D3DLin is activated\n",
            "Attn2D3DLin is activated\n",
            "Attn2D3DLin is activated\n",
            "Attn2D3DLin is activated\n",
            "Attn2D3DLin is activated\n",
            "Attn2D3DLin is activated\n",
            "Attn2D3DLin is activated\n",
            "Attn2D3DLin is activated\n",
            "\n",
            "Total trainable parameters: 29,590,795\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|| 543/543 [05:59<00:00,  1.51it/s, loss=1]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 1 | Training Loss: 1.0905 | Validation Loss: 1.0270 | Accuracy: 0.4608\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|| 543/543 [05:59<00:00,  1.51it/s, loss=1.05]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 2 | Training Loss: 1.0497 | Validation Loss: 1.0226 | Accuracy: 0.4654\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|| 543/543 [05:59<00:00,  1.51it/s, loss=1.02]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 3 | Training Loss: 1.0368 | Validation Loss: 1.0281 | Accuracy: 0.4612\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|| 543/543 [05:59<00:00,  1.51it/s, loss=1.03]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 4 | Training Loss: 1.0306 | Validation Loss: 1.0169 | Accuracy: 0.4695\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5: 100%|| 543/543 [05:58<00:00,  1.51it/s, loss=0.971]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 5 | Training Loss: 1.0259 | Validation Loss: 1.0133 | Accuracy: 0.4698\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6: 100%|| 543/543 [05:58<00:00,  1.51it/s, loss=0.982]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 6 | Training Loss: 1.0224 | Validation Loss: 1.0188 | Accuracy: 0.4673\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7: 100%|| 543/543 [05:58<00:00,  1.51it/s, loss=1]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 7 | Training Loss: 1.0195 | Validation Loss: 1.0172 | Accuracy: 0.4661\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8: 100%|| 543/543 [05:58<00:00,  1.51it/s, loss=0.853]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 8 | Training Loss: 1.0160 | Validation Loss: 1.0134 | Accuracy: 0.4713\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9: 100%|| 543/543 [05:59<00:00,  1.51it/s, loss=1.08]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 9 | Training Loss: 1.0143 | Validation Loss: 1.0116 | Accuracy: 0.4725\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10: 100%|| 543/543 [05:59<00:00,  1.51it/s, loss=1.04]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 10 | Training Loss: 1.0111 | Validation Loss: 1.0134 | Accuracy: 0.4735\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11: 100%|| 543/543 [05:58<00:00,  1.51it/s, loss=0.948]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 11 | Training Loss: 1.0081 | Validation Loss: 1.0075 | Accuracy: 0.4753\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12: 100%|| 543/543 [05:58<00:00,  1.51it/s, loss=1.01]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 12 | Training Loss: 1.0057 | Validation Loss: 1.0206 | Accuracy: 0.4652\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 13: 100%|| 543/543 [05:59<00:00,  1.51it/s, loss=0.947]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 13 | Training Loss: 1.0026 | Validation Loss: 1.0190 | Accuracy: 0.4713\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 14: 100%|| 543/543 [05:59<00:00,  1.51it/s, loss=0.855]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 14 | Training Loss: 0.9998 | Validation Loss: 1.0157 | Accuracy: 0.4771\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 15: 100%|| 543/543 [05:59<00:00,  1.51it/s, loss=0.998]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 15 | Training Loss: 0.9971 | Validation Loss: 1.0112 | Accuracy: 0.4811\n",
            "Average training time per epoch: 359.03s\n",
            "Evaluating the trained model on CB513 dataset\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4555    0.5268    0.4886     46162\n",
            "           1     0.4335    0.0820    0.1380     29902\n",
            "           2     0.4830    0.6227    0.5440     58858\n",
            "\n",
            "    accuracy                         0.4701    134922\n",
            "   macro avg     0.4574    0.4105    0.3902    134922\n",
            "weighted avg     0.4626    0.4701    0.4351    134922\n",
            "\n",
            "Average Validation Loss: 1.0374, Accuracy: 0.4701\n",
            "Evaluating the trained model on TS115 dataset\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5344    0.7029    0.6072     11062\n",
            "           1     0.4454    0.0858    0.1439      5327\n",
            "           2     0.5060    0.5396    0.5222     12275\n",
            "\n",
            "    accuracy                         0.5182     28664\n",
            "   macro avg     0.4953    0.4427    0.4244     28664\n",
            "weighted avg     0.5057    0.5182    0.4847     28664\n",
            "\n",
            "Average Validation Loss: 0.9710, Accuracy: 0.5182\n",
            "Evaluating the trained model on CASP12 dataset\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4554    0.6396    0.5320      1837\n",
            "           1     0.0000    0.0000    0.0000      1249\n",
            "           2     0.5277    0.6209    0.5705      2867\n",
            "\n",
            "    accuracy                         0.4964      5953\n",
            "   macro avg     0.3277    0.4202    0.3675      5953\n",
            "weighted avg     0.3947    0.4964    0.4389      5953\n",
            "\n",
            "Average Validation Loss: 1.0410, Accuracy: 0.4964\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "# Start Training\n",
        "model, model_kwargs = ProteinTransformerSS3, {\n",
        "    'vocab_size': tokenizer.vocab_size,\n",
        "    'num_classes': 3,\n",
        "    'd_model': 512,\n",
        "    'num_layers': 12,\n",
        "    'dim_feedforward': 1024,\n",
        "    'p': 0.4,\n",
        "    'len_seq': 512,\n",
        "    'num_heads': 8,\n",
        "    'attn_mechanism': 'Attn2D3DLin',\n",
        "    'num_peds': 30,\n",
        "    'k': 50\n",
        "}\n",
        "trained_model, avg_val_loss_Comb2D3DLin = train_model(model, model_kwargs, train_loader, val_loader, epochs=15, lr=0.5e-5)\n",
        "evaluate_model(trained_model, test1_loader, class_report = True)\n",
        "evaluate_model(trained_model, test2_loader, class_report = True)\n",
        "evaluate_model(trained_model, test3_loader, class_report = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fcwWqRV5_zc3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcwWqRV5_zc3",
        "outputId": "36a75e79-ded8-49f1-bef8-c9e6589777bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attn2D3D_MultiPed is activated\n",
            "Attn2D3D_MultiPed is activated\n",
            "Attn2D3D_MultiPed is activated\n",
            "Attn2D3D_MultiPed is activated\n",
            "Attn2D3D_MultiPed is activated\n",
            "Attn2D3D_MultiPed is activated\n",
            "Attn2D3D_MultiPed is activated\n",
            "Attn2D3D_MultiPed is activated\n",
            "Attn2D3D_MultiPed is activated\n",
            "Attn2D3D_MultiPed is activated\n",
            "Attn2D3D_MultiPed is activated\n",
            "Attn2D3D_MultiPed is activated\n",
            "\n",
            "Total trainable parameters: 28,681,731\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|| 543/543 [04:04<00:00,  2.22it/s, loss=1.01]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 1 | Training Loss: 1.0630 | Validation Loss: 1.0078 | Accuracy: 0.4712\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|| 543/543 [04:05<00:00,  2.22it/s, loss=0.965]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 2 | Training Loss: 1.0328 | Validation Loss: 1.0043 | Accuracy: 0.4758\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|| 543/543 [04:05<00:00,  2.22it/s, loss=0.993]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 3 | Training Loss: 1.0223 | Validation Loss: 1.0014 | Accuracy: 0.4776\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|| 543/543 [04:05<00:00,  2.22it/s, loss=0.91]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 4 | Training Loss: 1.0163 | Validation Loss: 0.9988 | Accuracy: 0.4791\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5: 100%|| 543/543 [04:04<00:00,  2.22it/s, loss=1.03]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 5 | Training Loss: 1.0120 | Validation Loss: 1.0029 | Accuracy: 0.4779\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6: 100%|| 543/543 [04:04<00:00,  2.22it/s, loss=0.973]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 6 | Training Loss: 1.0074 | Validation Loss: 0.9990 | Accuracy: 0.4795\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7: 100%|| 543/543 [04:05<00:00,  2.22it/s, loss=1.05]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 7 | Training Loss: 1.0045 | Validation Loss: 0.9976 | Accuracy: 0.4809\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8: 100%|| 543/543 [04:04<00:00,  2.22it/s, loss=1.05]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 8 | Training Loss: 1.0018 | Validation Loss: 0.9979 | Accuracy: 0.4804\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9: 100%|| 543/543 [04:04<00:00,  2.22it/s, loss=0.966]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 9 | Training Loss: 0.9973 | Validation Loss: 0.9988 | Accuracy: 0.4802\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10: 100%|| 543/543 [04:04<00:00,  2.22it/s, loss=0.996]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 10 | Training Loss: 0.9951 | Validation Loss: 1.0133 | Accuracy: 0.4733\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11: 100%|| 543/543 [04:04<00:00,  2.22it/s, loss=1.02]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 11 | Training Loss: 0.9930 | Validation Loss: 0.9943 | Accuracy: 0.4829\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12: 100%|| 543/543 [04:04<00:00,  2.22it/s, loss=1.02]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 12 | Training Loss: 0.9889 | Validation Loss: 0.9929 | Accuracy: 0.4834\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 13: 100%|| 543/543 [04:05<00:00,  2.22it/s, loss=0.964]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 13 | Training Loss: 0.9861 | Validation Loss: 0.9899 | Accuracy: 0.4838\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 14: 100%|| 543/543 [04:05<00:00,  2.21it/s, loss=1.01]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 14 | Training Loss: 0.9821 | Validation Loss: 1.0049 | Accuracy: 0.4808\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 15: 100%|| 543/543 [04:05<00:00,  2.21it/s, loss=1.02]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 15 | Training Loss: 0.9800 | Validation Loss: 0.9887 | Accuracy: 0.4871\n",
            "Average training time per epoch: 245.03s\n",
            "Evaluating the trained model on CB513 dataset\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4541    0.5390    0.4929     46162\n",
            "           1     0.3728    0.1315    0.1944     29902\n",
            "           2     0.4854    0.5739    0.5260     58858\n",
            "\n",
            "    accuracy                         0.4639    134922\n",
            "   macro avg     0.4374    0.4148    0.4044    134922\n",
            "weighted avg     0.4498    0.4639    0.4412    134922\n",
            "\n",
            "Average Validation Loss: 1.0263, Accuracy: 0.4639\n",
            "Evaluating the trained model on TS115 dataset\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5240    0.7337    0.6113     11062\n",
            "           1     0.4318    0.1449    0.2170      5327\n",
            "           2     0.5308    0.4924    0.5109     12275\n",
            "\n",
            "    accuracy                         0.5209     28664\n",
            "   macro avg     0.4955    0.4570    0.4464     28664\n",
            "weighted avg     0.5098    0.5209    0.4950     28664\n",
            "\n",
            "Average Validation Loss: 0.9537, Accuracy: 0.5209\n",
            "Evaluating the trained model on CASP12 dataset\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4678    0.7082    0.5634      1837\n",
            "           1     0.3382    0.1849    0.2391      1249\n",
            "           2     0.5685    0.4935    0.5284      2867\n",
            "\n",
            "    accuracy                         0.4950      5953\n",
            "   macro avg     0.4582    0.4622    0.4437      5953\n",
            "weighted avg     0.4891    0.4950    0.4785      5953\n",
            "\n",
            "Average Validation Loss: 0.9976, Accuracy: 0.4950\n"
          ]
        }
      ],
      "source": [
        "# Start Training\n",
        "model, model_kwargs = ProteinTransformerSS3, {\n",
        "    'vocab_size': tokenizer.vocab_size,\n",
        "    'num_classes': 3,\n",
        "    'd_model': 512,\n",
        "    'num_layers': 12,\n",
        "    'dim_feedforward': 1024,\n",
        "    'p': 0.4,\n",
        "    'len_seq': 512,\n",
        "    'num_heads': 8,\n",
        "    'attn_mechanism': 'Attn2D3D_MultiPed',\n",
        "    'num_peds': 30,\n",
        "    'k': 50\n",
        "}\n",
        "trained_model, avg_val_loss_Comb2D3DMulti = train_model(model, model_kwargs, train_loader, val_loader, epochs=15, lr=0.5e-5)\n",
        "evaluate_model(trained_model, test1_loader, class_report = True)\n",
        "evaluate_model(trained_model, test2_loader, class_report = True)\n",
        "evaluate_model(trained_model, test3_loader, class_report = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xUtQzZv9A-f0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUtQzZv9A-f0",
        "outputId": "3a277b3f-b3ef-4145-c815-ae0ea5d18987"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attn2D3DMultiPed_Linformer is activated\n",
            "Attn2D3DMultiPed_Linformer is activated\n",
            "Attn2D3DMultiPed_Linformer is activated\n",
            "Attn2D3DMultiPed_Linformer is activated\n",
            "Attn2D3DMultiPed_Linformer is activated\n",
            "Attn2D3DMultiPed_Linformer is activated\n",
            "Attn2D3DMultiPed_Linformer is activated\n",
            "Attn2D3DMultiPed_Linformer is activated\n",
            "Attn2D3DMultiPed_Linformer is activated\n",
            "Attn2D3DMultiPed_Linformer is activated\n",
            "Attn2D3DMultiPed_Linformer is activated\n",
            "Attn2D3DMultiPed_Linformer is activated\n",
            "\n",
            "Total trainable parameters: 29,655,531\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|| 543/543 [03:39<00:00,  2.47it/s, loss=0.864]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 1 | Training Loss: 1.0378 | Validation Loss: 1.0046 | Accuracy: 0.5113\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|| 543/543 [03:39<00:00,  2.47it/s, loss=0.969]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 2 | Training Loss: 0.9936 | Validation Loss: 0.9729 | Accuracy: 0.5149\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|| 543/543 [03:39<00:00,  2.47it/s, loss=0.972]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 3 | Training Loss: 0.9795 | Validation Loss: 0.9648 | Accuracy: 0.5220\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|| 543/543 [03:39<00:00,  2.47it/s, loss=1.02]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 4 | Training Loss: 0.9694 | Validation Loss: 0.9647 | Accuracy: 0.5235\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5: 100%|| 543/543 [03:39<00:00,  2.48it/s, loss=0.87]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 5 | Training Loss: 0.9647 | Validation Loss: 0.9627 | Accuracy: 0.5253\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6: 100%|| 543/543 [03:39<00:00,  2.48it/s, loss=0.922]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 6 | Training Loss: 0.9608 | Validation Loss: 0.9564 | Accuracy: 0.5272\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7: 100%|| 543/543 [03:39<00:00,  2.48it/s, loss=1.02]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 7 | Training Loss: 0.9575 | Validation Loss: 0.9491 | Accuracy: 0.5315\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8: 100%|| 543/543 [03:39<00:00,  2.47it/s, loss=0.802]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 8 | Training Loss: 0.9545 | Validation Loss: 0.9545 | Accuracy: 0.5297\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9: 100%|| 543/543 [03:39<00:00,  2.48it/s, loss=0.883]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 9 | Training Loss: 0.9507 | Validation Loss: 0.9564 | Accuracy: 0.5308\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10: 100%|| 543/543 [03:39<00:00,  2.48it/s, loss=0.922]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 10 | Training Loss: 0.9484 | Validation Loss: 0.9523 | Accuracy: 0.5317\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11: 100%|| 543/543 [03:39<00:00,  2.48it/s, loss=0.95]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 11 | Training Loss: 0.9463 | Validation Loss: 0.9569 | Accuracy: 0.5299\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12: 100%|| 543/543 [03:39<00:00,  2.48it/s, loss=0.947]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 12 | Training Loss: 0.9441 | Validation Loss: 0.9566 | Accuracy: 0.5292\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 13: 100%|| 543/543 [03:39<00:00,  2.48it/s, loss=0.864]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 13 | Training Loss: 0.9410 | Validation Loss: 0.9633 | Accuracy: 0.5306\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 14: 100%|| 543/543 [03:39<00:00,  2.48it/s, loss=1.03]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 14 | Training Loss: 0.9384 | Validation Loss: 0.9628 | Accuracy: 0.5329\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 15: 100%|| 543/543 [03:39<00:00,  2.48it/s, loss=0.976]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 15 | Training Loss: 0.9360 | Validation Loss: 0.9503 | Accuracy: 0.5322\n",
            "Average training time per epoch: 219.36s\n",
            "Evaluating the trained model on CB513 dataset\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5347    0.5855    0.5589     46162\n",
            "           1     0.3951    0.2220    0.2843     29902\n",
            "           2     0.5364    0.6158    0.5733     58858\n",
            "\n",
            "    accuracy                         0.5181    134922\n",
            "   macro avg     0.4887    0.4744    0.4722    134922\n",
            "weighted avg     0.5045    0.5181    0.5043    134922\n",
            "\n",
            "Average Validation Loss: 0.9849, Accuracy: 0.5181\n",
            "Evaluating the trained model on TS115 dataset\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5844    0.7180    0.6443     11062\n",
            "           1     0.4518    0.2577    0.3282      5327\n",
            "           2     0.5809    0.5695    0.5751     12275\n",
            "\n",
            "    accuracy                         0.5688     28664\n",
            "   macro avg     0.5390    0.5150    0.5159     28664\n",
            "weighted avg     0.5582    0.5688    0.5559     28664\n",
            "\n",
            "Average Validation Loss: 0.9042, Accuracy: 0.5688\n",
            "Evaluating the trained model on CASP12 dataset\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5238    0.7665    0.6223      1837\n",
            "           1     0.4499    0.1978    0.2747      1249\n",
            "           2     0.6252    0.5923    0.6083      2867\n",
            "\n",
            "    accuracy                         0.5632      5953\n",
            "   macro avg     0.5330    0.5188    0.5018      5953\n",
            "weighted avg     0.5571    0.5632    0.5426      5953\n",
            "\n",
            "Average Validation Loss: 0.9310, Accuracy: 0.5632\n"
          ]
        }
      ],
      "source": [
        "# Start Training\n",
        "model, model_kwargs = ProteinTransformerSS3, {\n",
        "    'vocab_size': tokenizer.vocab_size,\n",
        "    'num_classes': 3,\n",
        "    'd_model': 512,\n",
        "    'num_layers': 12,\n",
        "    'dim_feedforward': 1024,\n",
        "    'p': 0.4,\n",
        "    'len_seq': 512,\n",
        "    'num_heads': 8,\n",
        "    'attn_mechanism': 'Attn2D3D_MultiPedLin',\n",
        "    'num_peds': 30,\n",
        "    'k': 50\n",
        "}\n",
        "trained_model, avg_val_loss_Comb2D3DMultiLin = train_model(model, model_kwargs, train_loader, val_loader, epochs=15, lr=0.5e-5)\n",
        "evaluate_model(trained_model, test1_loader, class_report = True)\n",
        "evaluate_model(trained_model, test2_loader, class_report = True)\n",
        "evaluate_model(trained_model, test3_loader, class_report = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JY59EJvGgbmj",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JY59EJvGgbmj",
        "outputId": "c24a9bba-54a3-411e-ae91-13ba3fb89e6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attn2D3DLinAlter is activated\n",
            "Attn2D3DLinAlter is activated\n",
            "Attn2D3DLinAlter is activated\n",
            "Attn2D3DLinAlter is activated\n",
            "Attn2D3DLinAlter is activated\n",
            "Attn2D3DLinAlter is activated\n",
            "Attn2D3DLinAlter is activated\n",
            "Attn2D3DLinAlter is activated\n",
            "Attn2D3DLinAlter is activated\n",
            "Attn2D3DLinAlter is activated\n",
            "Attn2D3DLinAlter is activated\n",
            "Attn2D3DLinAlter is activated\n",
            "\n",
            "Total trainable parameters: 29,689,867\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|| 543/543 [05:53<00:00,  1.54it/s, loss=1.06]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 1 | Training Loss: 1.0489 | Validation Loss: 0.9940 | Accuracy: 0.5188\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|| 543/543 [05:53<00:00,  1.54it/s, loss=0.942]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 2 | Training Loss: 0.9949 | Validation Loss: 0.9747 | Accuracy: 0.5290\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|| 543/543 [05:53<00:00,  1.54it/s, loss=0.973]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 3 | Training Loss: 0.9813 | Validation Loss: 0.9753 | Accuracy: 0.5299\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|| 543/543 [05:53<00:00,  1.54it/s, loss=0.934]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 4 | Training Loss: 0.9765 | Validation Loss: 0.9713 | Accuracy: 0.5307\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5: 100%|| 543/543 [05:53<00:00,  1.54it/s, loss=0.948]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 5 | Training Loss: 0.9718 | Validation Loss: 0.9669 | Accuracy: 0.5331\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6: 100%|| 543/543 [05:53<00:00,  1.54it/s, loss=0.979]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 6 | Training Loss: 0.9694 | Validation Loss: 0.9673 | Accuracy: 0.5310\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7: 100%|| 543/543 [05:53<00:00,  1.54it/s, loss=0.999]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 7 | Training Loss: 0.9665 | Validation Loss: 0.9661 | Accuracy: 0.5316\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8: 100%|| 543/543 [05:53<00:00,  1.54it/s, loss=0.93]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 8 | Training Loss: 0.9643 | Validation Loss: 0.9656 | Accuracy: 0.5317\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9: 100%|| 543/543 [05:53<00:00,  1.54it/s, loss=0.999]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 9 | Training Loss: 0.9633 | Validation Loss: 0.9609 | Accuracy: 0.5341\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10: 100%|| 543/543 [05:53<00:00,  1.54it/s, loss=1.08]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 10 | Training Loss: 0.9624 | Validation Loss: 0.9579 | Accuracy: 0.5346\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11: 100%|| 543/543 [05:53<00:00,  1.54it/s, loss=0.943]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 11 | Training Loss: 0.9605 | Validation Loss: 0.9629 | Accuracy: 0.5319\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12: 100%|| 543/543 [05:53<00:00,  1.54it/s, loss=0.897]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 12 | Training Loss: 0.9587 | Validation Loss: 0.9629 | Accuracy: 0.5338\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 13: 100%|| 543/543 [05:53<00:00,  1.54it/s, loss=0.968]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 13 | Training Loss: 0.9580 | Validation Loss: 0.9632 | Accuracy: 0.5316\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 14: 100%|| 543/543 [05:53<00:00,  1.54it/s, loss=0.907]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 14 | Training Loss: 0.9573 | Validation Loss: 0.9577 | Accuracy: 0.5359\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 15: 100%|| 543/543 [05:53<00:00,  1.54it/s, loss=0.898]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 15 | Training Loss: 0.9562 | Validation Loss: 0.9648 | Accuracy: 0.5348\n",
            "Average training time per epoch: 353.43s\n",
            "Evaluating the trained model on CB513 dataset\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4733    0.6396    0.5440     46162\n",
            "           1     0.4517    0.2278    0.3029     29902\n",
            "           2     0.5840    0.5701    0.5769     58858\n",
            "\n",
            "    accuracy                         0.5180    134922\n",
            "   macro avg     0.5030    0.4792    0.4746    134922\n",
            "weighted avg     0.5168    0.5180    0.5049    134922\n",
            "\n",
            "Average Validation Loss: 0.9919, Accuracy: 0.5180\n",
            "Evaluating the trained model on TS115 dataset\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5277    0.7500    0.6195     11062\n",
            "           1     0.5167    0.2270    0.3154      5327\n",
            "           2     0.6143    0.5306    0.5694     12275\n",
            "\n",
            "    accuracy                         0.5588     28664\n",
            "   macro avg     0.5529    0.5025    0.5014     28664\n",
            "weighted avg     0.5627    0.5588    0.5415     28664\n",
            "\n",
            "Average Validation Loss: 0.9292, Accuracy: 0.5588\n",
            "Evaluating the trained model on CASP12 dataset\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4396    0.7273    0.5480      1837\n",
            "           1     0.5431    0.2370    0.3300      1249\n",
            "           2     0.6340    0.5239    0.5737      2867\n",
            "\n",
            "    accuracy                         0.5265      5953\n",
            "   macro avg     0.5389    0.4961    0.4839      5953\n",
            "weighted avg     0.5550    0.5265    0.5146      5953\n",
            "\n",
            "Average Validation Loss: 0.9443, Accuracy: 0.5265\n"
          ]
        }
      ],
      "source": [
        "# Start Training\n",
        "model, model_kwargs = ProteinTransformerSS3, {\n",
        "    'vocab_size': tokenizer.vocab_size,\n",
        "    'num_classes': 3,\n",
        "    'd_model': 512,\n",
        "    'num_layers': 12,\n",
        "    'dim_feedforward': 1024,\n",
        "    'p': 0.4,\n",
        "    'len_seq': 512,\n",
        "    'num_heads': 8,\n",
        "    'attn_mechanism': 'Attn2D3DLinAlter',\n",
        "    'num_peds': 30,\n",
        "    'k': 50\n",
        "}\n",
        "trained_model, avg_val_loss_Comb2D3DMultiLinAlter = train_model(model, model_kwargs, train_loader, val_loader, epochs=15, lr=0.5e-5)\n",
        "evaluate_model(trained_model, test1_loader, class_report = True)\n",
        "evaluate_model(trained_model, test2_loader, class_report = True)\n",
        "evaluate_model(trained_model, test3_loader, class_report = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ofNx23FweLDz",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofNx23FweLDz",
        "outputId": "d7ee4d26-df70-4d10-f307-11f7b0239622"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attn2D3D_MultiPedAlter is activated\n",
            "Attn2D3D_MultiPedAlter is activated\n",
            "Attn2D3D_MultiPedAlter is activated\n",
            "Attn2D3D_MultiPedAlter is activated\n",
            "Attn2D3D_MultiPedAlter is activated\n",
            "Attn2D3D_MultiPedAlter is activated\n",
            "Attn2D3D_MultiPedAlter is activated\n",
            "Attn2D3D_MultiPedAlter is activated\n",
            "Attn2D3D_MultiPedAlter is activated\n",
            "Attn2D3D_MultiPedAlter is activated\n",
            "Attn2D3D_MultiPedAlter is activated\n",
            "Attn2D3D_MultiPedAlter is activated\n",
            "\n",
            "Total trainable parameters: 28,780,803\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|| 543/543 [03:58<00:00,  2.27it/s, loss=1.02]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 1 | Training Loss: 1.0494 | Validation Loss: 0.9961 | Accuracy: 0.5188\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|| 543/543 [03:58<00:00,  2.27it/s, loss=0.943]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 2 | Training Loss: 0.9965 | Validation Loss: 0.9752 | Accuracy: 0.5317\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|| 543/543 [03:58<00:00,  2.27it/s, loss=0.954]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 3 | Training Loss: 0.9809 | Validation Loss: 0.9740 | Accuracy: 0.5286\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|| 543/543 [03:58<00:00,  2.27it/s, loss=0.954]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 4 | Training Loss: 0.9747 | Validation Loss: 0.9658 | Accuracy: 0.5340\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5: 100%|| 543/543 [03:58<00:00,  2.27it/s, loss=0.943]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 5 | Training Loss: 0.9705 | Validation Loss: 0.9634 | Accuracy: 0.5346\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6: 100%|| 543/543 [03:58<00:00,  2.27it/s, loss=0.87]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 6 | Training Loss: 0.9671 | Validation Loss: 0.9688 | Accuracy: 0.5334\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7: 100%|| 543/543 [03:58<00:00,  2.27it/s, loss=0.94]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 7 | Training Loss: 0.9646 | Validation Loss: 0.9619 | Accuracy: 0.5350\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8: 100%|| 543/543 [03:59<00:00,  2.27it/s, loss=0.892]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 8 | Training Loss: 0.9619 | Validation Loss: 0.9615 | Accuracy: 0.5347\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9: 100%|| 543/543 [03:58<00:00,  2.27it/s, loss=0.873]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 9 | Training Loss: 0.9598 | Validation Loss: 0.9622 | Accuracy: 0.5369\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10: 100%|| 543/543 [03:58<00:00,  2.27it/s, loss=0.956]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 10 | Training Loss: 0.9578 | Validation Loss: 0.9623 | Accuracy: 0.5374\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11: 100%|| 543/543 [03:58<00:00,  2.27it/s, loss=0.899]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 11 | Training Loss: 0.9568 | Validation Loss: 0.9568 | Accuracy: 0.5363\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12: 100%|| 543/543 [03:58<00:00,  2.27it/s, loss=0.983]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 12 | Training Loss: 0.9554 | Validation Loss: 0.9552 | Accuracy: 0.5375\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 13: 100%|| 543/543 [03:58<00:00,  2.27it/s, loss=0.925]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 13 | Training Loss: 0.9534 | Validation Loss: 0.9556 | Accuracy: 0.5362\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 14: 100%|| 543/543 [03:58<00:00,  2.27it/s, loss=0.979]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 14 | Training Loss: 0.9529 | Validation Loss: 0.9567 | Accuracy: 0.5373\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 15: 100%|| 543/543 [03:58<00:00,  2.27it/s, loss=0.945]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 15 | Training Loss: 0.9522 | Validation Loss: 0.9546 | Accuracy: 0.5397\n",
            "Average training time per epoch: 238.83s\n",
            "Evaluating the trained model on CB513 dataset\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4946    0.5731    0.5310     46162\n",
            "           1     0.4137    0.3124    0.3560     29902\n",
            "           2     0.5805    0.5804    0.5805     58858\n",
            "\n",
            "    accuracy                         0.5185    134922\n",
            "   macro avg     0.4963    0.4887    0.4892    134922\n",
            "weighted avg     0.5142    0.5185    0.5138    134922\n",
            "\n",
            "Average Validation Loss: 0.9902, Accuracy: 0.5185\n",
            "Evaluating the trained model on TS115 dataset\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5549    0.7073    0.6219     11062\n",
            "           1     0.4689    0.3096    0.3729      5327\n",
            "           2     0.6131    0.5518    0.5808     12275\n",
            "\n",
            "    accuracy                         0.5668     28664\n",
            "   macro avg     0.5456    0.5229    0.5252     28664\n",
            "weighted avg     0.5638    0.5668    0.5580     28664\n",
            "\n",
            "Average Validation Loss: 0.9212, Accuracy: 0.5668\n",
            "Evaluating the trained model on CASP12 dataset\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4746    0.6674    0.5548      1837\n",
            "           1     0.4899    0.3291    0.3937      1249\n",
            "           2     0.6349    0.5605    0.5954      2867\n",
            "\n",
            "    accuracy                         0.5449      5953\n",
            "   macro avg     0.5331    0.5190    0.5146      5953\n",
            "weighted avg     0.5550    0.5449    0.5405      5953\n",
            "\n",
            "Average Validation Loss: 0.9357, Accuracy: 0.5449\n"
          ]
        }
      ],
      "source": [
        "# Start Training\n",
        "model, model_kwargs = ProteinTransformerSS3, {\n",
        "    'vocab_size': tokenizer.vocab_size,\n",
        "    'num_classes': 3,\n",
        "    'd_model': 512,\n",
        "    'num_layers': 12,\n",
        "    'dim_feedforward': 1024,\n",
        "    'p': 0.4,\n",
        "    'len_seq': 512,\n",
        "    'num_heads': 8,\n",
        "    'attn_mechanism': 'Attn2D3D_MultiPedAlter',\n",
        "    'num_peds': 30,\n",
        "    'k': 50\n",
        "}\n",
        "trained_model, avg_val_loss_Comb2D3DMultiAlter = train_model(model, model_kwargs, train_loader, val_loader, epochs=15, lr=0.5e-5)\n",
        "evaluate_model(trained_model, test1_loader, class_report = True)\n",
        "evaluate_model(trained_model, test2_loader, class_report = True)\n",
        "evaluate_model(trained_model, test3_loader, class_report = True) ## stop here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "K4k-6mh4f1R_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 860
        },
        "id": "K4k-6mh4f1R_",
        "outputId": "e2553de7-c4fc-40ab-a8f7-e551634779f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attn2D3D_MultiPedLinAlter is activated\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'Attn2D3D_MultiPedLinAlter' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-959776233.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;34m'k'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m }\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mtrained_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_val_loss_Comb2D3DMultiLinAlter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrained_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest1_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_report\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrained_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest2_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_report\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2422956785.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model_class, model_kwargs, train_loader, val_loader, epochs, lr)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mtotal_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-324776290.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_classes, vocab_size, num_layers, p, dim_feedforward, attn_mechanism, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m# Encoder and Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         self.encoder_layers = nn.ModuleList([\n\u001b[0m\u001b[1;32m     58\u001b[0m             \u001b[0mEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_feedforward\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdim_feedforward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mechanism\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_mechanism\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-324776290.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m# Encoder and Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         self.encoder_layers = nn.ModuleList([\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_feedforward\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdim_feedforward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mechanism\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_mechanism\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         ])\n",
            "\u001b[0;32m/tmp/ipython-input-324776290.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, p, dim_feedforward, attn_mechanism, **kwargs)\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0mlen_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen_seq\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpads_needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"len_seq\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen_seq\u001b[0m  \u001b[0;31m# Update the value in kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_mechanism\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3656969139.py\u001b[0m in \u001b[0;36mget_attention\u001b[0;34m(attn_type, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mattn_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"Attn2D3D_MultiPedLinAlter\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Attn2D3D_MultiPedLinAlter is activated'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mAttn2D3D_MultiPedLinAlter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_heads\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"d_model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_peds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_peds\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen_seq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"len_seq\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"k\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Unknown attention type: {attn_type}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Attn2D3D_MultiPedLinAlter' is not defined"
          ]
        }
      ],
      "source": [
        "# Start Training\n",
        "model, model_kwargs = ProteinTransformerSS3, {\n",
        "    'vocab_size': tokenizer.vocab_size,\n",
        "    'num_classes': 3,\n",
        "    'd_model': 512,\n",
        "    'num_layers': 12,\n",
        "    'dim_feedforward': 1024,\n",
        "    'p': 0.4,\n",
        "    'len_seq': 512,\n",
        "    'num_heads': 8,\n",
        "    'attn_mechanism': 'Attn2D3D_MultiPedLinAlter',\n",
        "    'num_peds': 30,\n",
        "    'k': 50\n",
        "}\n",
        "trained_model, avg_val_loss_Comb2D3DMultiLinAlter = train_model(model, model_kwargs, train_loader, val_loader, epochs=15, lr=0.5e-5)\n",
        "evaluate_model(trained_model, test1_loader, class_report = True)\n",
        "evaluate_model(trained_model, test2_loader, class_report = True)\n",
        "evaluate_model(trained_model, test3_loader, class_report = True) ## stop here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "R_0kBMc8eZq8",
      "metadata": {
        "id": "R_0kBMc8eZq8"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# Step 1: Load the existing dictionary\n",
        "with open('losses_Secondary_Structures.pkl', 'rb') as f:\n",
        "    losses = pickle.load(f)\n",
        "\n",
        "# Step 2: Add the new losses\n",
        "losses['Plain 2D Loss'] = avg_val_loss_Plain2D\n",
        "losses['Combined 2D and 3D Linformer Alter Loss'] = avg_val_loss_Comb2D3DMultiLinAlter\n",
        "losses['Combined 2D and 3D Multi-Ped Alter'] = Attn2D3D_MultiPedAlter\n",
        "#losses['Combined 2D and 3D Multi-Ped Linformer Alter'] = avg_val_loss_Comb2D3DMultiLinAlter\n",
        "\n",
        "# Step 3: Save the updated dictionary back to the same file\n",
        "with open('losses_Secondary_Structures.pkl', 'wb') as f:\n",
        "    pickle.dump(losses, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SeE60eY3Ki3J",
      "metadata": {
        "id": "SeE60eY3Ki3J"
      },
      "outputs": [],
      "source": [
        "# Dictionary of all losses\n",
        "losses = {\n",
        "    '2D MultiPed': avg_val_loss_Multi2D,\n",
        "    '2D Linformer': avg_val_loss_Lin2D,\n",
        "    '2D MultiPed Linformer': avg_val_loss_MultiLin2D,\n",
        "    '3D Linformer': avg_val_loss_Lin3D,\n",
        "    '3D MultiPed': avg_val_loss_Mutli3D,\n",
        "    '3D MultiPed Linformer': avg_val_loss_MutliLin3D,\n",
        "    '2D 3D Combined Linformer': avg_val_loss_Comb2D3DLin,\n",
        "    '2D 3D Combined MultiPed': avg_val_loss_Comb2D3DMulti,\n",
        "    '2D 3D Combined MultiPed Linformer': avg_val_loss_Comb2D3DMultiLin,\n",
        "\n",
        "}\n",
        "import pickle\n",
        "\n",
        "# Save the dictionary\n",
        "with open('losses_Secondary_Structures.pkl', 'wb') as f:\n",
        "    pickle.dump(losses, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wwk19GWjdff-",
      "metadata": {
        "id": "wwk19GWjdff-"
      },
      "outputs": [],
      "source": [
        "'2D 3D Combined Alter' : avg_val_loss_Comb2D3DMultiLinAlter"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nClnWMPTgRYt",
      "metadata": {
        "id": "nClnWMPTgRYt"
      },
      "source": [
        "'3D Plain': loss_3DPlain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FpNMfZmF2StI",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpNMfZmF2StI",
        "outputId": "f3556b01-8def-476a-b04f-75015ff79281"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['2D MultiPed', '2D Linformer', '2D MultiPed Linformer', '3D Linformer', '3D MultiPed', '3D MultiPed Linformer', '2D 3D Combined Linformer', '2D 3D Combined MultiPed', '2D 3D Combined MultiPed Linformer', 'Plain 2D Loss', 'Combined 2D and 3D Linformer Alter Loss', 'Combined 2D and 3D Multi-Ped Alter'])"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "losses.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "p65l2SjiKjfp",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 861
        },
        "id": "p65l2SjiKjfp",
        "outputId": "5268bb88-5953-4a85-8b9d-53fbc6412f93"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "float() argument must be a string or a real number, not 'type'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2637617885.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;34m'2D 3D'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlinestyle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m':'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'3D'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'-'\u001b[0m  \u001b[0;31m# Dotted if \"3D\" in name, solid otherwise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinestyle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlinestyle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3827\u001b[0m     \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3828\u001b[0m ) -> list[Line2D]:\n\u001b[0;32m-> 3829\u001b[0;31m     return gca().plot(\n\u001b[0m\u001b[1;32m   3830\u001b[0m         \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3831\u001b[0m         \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1777\u001b[0m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1778\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1779\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1780\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1781\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request_autoscale_view\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"x\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36madd_line\u001b[0;34m(self, line)\u001b[0m\n\u001b[1;32m   2369\u001b[0m             \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2371\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_line_limits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2372\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2373\u001b[0m             \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'_child{len(self._children)}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_update_line_limits\u001b[0;34m(self, line)\u001b[0m\n\u001b[1;32m   2392\u001b[0m         \u001b[0mFigures\u001b[0m \u001b[0mout\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdata\u001b[0m \u001b[0mlimit\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgiven\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdating\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataLim\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2393\u001b[0m         \"\"\"\n\u001b[0;32m-> 2394\u001b[0;31m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvertices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2396\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/lines.py\u001b[0m in \u001b[0;36mget_path\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0;34m\"\"\"Return the `~matplotlib.path.Path` associated with this line.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_invalidy\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_invalidx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/lines.py\u001b[0m in \u001b[0;36mrecache\u001b[0;34m(self, always)\u001b[0m\n\u001b[1;32m    678\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0malways\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_invalidy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m             \u001b[0myconv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_yunits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_yorig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_to_unmasked_float_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myconv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/cbook.py\u001b[0m in \u001b[0;36m_to_unmasked_float_array\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1343\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1344\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1345\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: float() argument must be a string or a real number, not 'type'"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA9UAAAH5CAYAAACPux17AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQABAABJREFUeJzs3XV8XfX9x/HXuR53l7q7G1a8uLv7BmPAth8yrOgYwzZsDBgM23CHUihQd3dLmkqSJk3jydXz++O2tw1taZumuZH38/G4j977zbnnfm6a3Nz3/ZphmqaJiIiIiIiIiBw0S7gLEBEREREREWmtFKpFREREREREGkmhWkRERERERKSRFKpFREREREREGkmhWkRERERERKSRFKpFREREREREGkmhWkRERERERKSRbOEu4EAEAgG2bNlCTEwMhmGEuxwRERERERFp40zTpKqqiszMTCyWffdHt4pQvWXLFnJycsJdhoiIiIiIiLQzGzduJDs7e59fbxWhOiYmBgg+mdjY2DBXIyIiIiIiIm1dZWUlOTk5oTy6L60iVO8c8h0bG6tQLSIiIiIiIs1mf1OQtVCZiIiIiIiISCMpVIuIiIiIiIg0kkK1iIiIiIiISCMpVIuIiIiIiIg0kkK1iIiIiIiISCMpVIuIiIiIiIg0kkK1iIiIiIiISCMpVIuIiIiIiIg0kkK1iIiIiIiISCMpVIuIiIiIiIg0kkK1iIiIiIiISCMpVIuIiIiIiIg0kkK1iIiIiIiISCMpVIuIiIiIiIg0kkK1iIiIiIiISCMpVIuIiIiIiIg0kkK1iIiIiIiISCMpVIuIiIiIiMhh5/W4w13CYaFQLSIiIiIiIoeN3+fj2xef4aXrL6OqrDTc5TQ5hWoRERERERFpUn6fN3TdarNRXlyEt76OtXNmhrGqw8MW7gJERERERESkbairquSH115i86rlXPv3V7HZ7QAceclVGIZBRrceYa6w6amnWkRERERERBot4PeHrjsjo9i8ajnVZdsoWLow1J7VoxeZ3XtiGEYYKjy81FMtIiIiIiIiB628uIjJb79O9fZtXPLIUwBYrFaOv+5mohMSSevcNcwVNg+FahERERERETkgZiCAYQkOeHZERLB+/mz8Ph/bNm8kKSsHgC5DhoezxGanUC0iIiIiIiK/amv+eqb97y2cUdGccssfAIiMjeP4628hvXPXUKBujzSnWkRERERERPZgmuau64EA6+fPYfXMqbhra0LtfY85nuTcjmGoruVQT7WIiIiIiIiEbFy+hFmfvE9m916MPv8SANI6d+Xoy66h85DhOCOjwlxhy6KeahERERERkXZu917pmvLtbFi8gCU/focZCITah55+DomZ2eEor0VTT7WIiIiIiEg7tW7ebOZ9+Qn9jj2RXkeOBaDrsFGMOu8Seh91bGhRMtk3hWoREREREZF2amv+OjYuX4KJGQrVNrs9NOxb9k+hWkREREREpB1YMfUnFkz4krFXXE9Gtx4A9B17AmbApO/YE8JcXeulvnwREREREZF2IH/hPApXr2TxDxNCbTGJyYw+/xJik1PCWFnrpp5qERERERGRNsQ0TZb8MIGlP03kzD/eS1R8AgCDTj6dxKwc+hxzfJgrbFvUUy0iIiIiItKGGIbB0h8nUrhmFct+/iHUnt61OyPOvoDohMQwVtf2qKdaRERERESklfJ5PCyZNIG1c2dxzl0PYrUFI96ws86joqiQ3kcfF+YK2z6FahERERERkVbKsFiY+fH/qK0oZ/382XQbPhqAbsNGhbmy9kOhWkREREREpBVw19ayZNIESvLXM+6WPwBgtdkYee5FmAGTnN79w1xh+6RQLSIiIiIi0gr4PG6mvPsGAb+fYWecS3JuRwAGnXRaeAtr5xSqRUREREREWpjaygqWTPoOb309R1x0OQBR8QkMO+NcYpJSiE1JDXOFspNCtYiIiIiISAtTXlTI1PfexOZwMvT0s3FFRQNwxEVXhLky+SWFahERERERkTCqKitl6aSJRMbFMeCEUwDI6NaD3kcdS07vfljt9jBXKL9GoVpERERERCSMNi5dzPQP3iE2JY3+x52MYbFgGAbjbr4j3KXJAVCoFhERERERaSaBgJ+1c2YSGRdPds8+AHQbOYZVM6bQc/RRmKaJEeYa5eAoVIuIiIiIiDST2Z98wLT33yandz8ueOBxAOwOJ2ff+UCYK5PGsoS7ABERERERkbaqpnw71dvLQrf7HHM8UfEJZPXqQyDgD2Nl0lTUUy0iIiIiInIYLJzwFT+99Sp9jj6OE66/BYCYpGRuePENLFZrmKuTpqKeahERERERkSZgmmaD3ufknA74vV62F27BDARC7QrUbYt6qkVERERERA5R/sJ5THv/bXqOOYYhp54JQFavPlz2+LOkde4a5urkcFJPtYiIiIiIyCGqLC2haN0aFn//DaZpAmAYhgJ1O6CeahERERERkYNQvb2MBd9+QW6fAXToPxCAXkeNpa6qkn7HnYRhaFOs9kShWkRERERE5CDM/fIT5n35CUVrV4VCtd3hZMTZF4S3MAkLDf8WERERERHZB9M0KVi6iIqtxaG2weNOJ7tXXwaefHpoqLe0X4bZCn4KKisriYuLo6KigtjY2HCXIyIiIiIi7cSPb7zC/G8+Z8CJp3L8tb8JdznSjA40h6qnWkREREREZAd3bS1ejzt0u+uwkdicTuxOZxirkpZMPdUiIiIiIiLAvK8+Y/oH7zDmwssZPO50IDj8211Tgys6OszVSXNTT7WIiIiIiMhBsNrteOpqyVs4N9RmGIYCtfwqrf4tIiIiIiLtTsHSxcz+7AOGnn4OHfsPAqDPMccRk5RM50FDw1xd21VVVk9EtB2bwxruUpqMeqpFRERERKTdWTd3JhsWL2Del5+E2uwOJ12GDMewKCYdLj6Pn+kfr2tTq6arp1pERERERNo0d20Ni7//lm4jxhCflg7A4FPOxMRkyClnhrm69iUhPYoug1Pw1vtxRLSNOKqPYEREREREpE379sVnmPzOv5n/9WehtrjUNI696kbiUtPDWFnb5/cFmPK/1ZRtqQm1ZXVPaDOBGhSqRURERESkjSnOW4fXXR+6PfDE00jKziWjW48wVtU+zfh0HYt/3MQ3/1yC3x8IdzmHRdv5eEBERERERNq9CS8/x9IfJ3LcNb9h4EmnApDbbwBX/u0FDMMIc3Xtz5CTOrBp5XZGntkZq7Vt9um2zWclIiIiIiLtgs/rbbDoVWqnLhgWC5XbSkJthmEoUDejipLa0PWIGAcX3jOMjv2Sw1jR4aWeahERERERaZXmf/0Zsz//iFNu+QO5fQcA0PeY4+kyZDixyalhrq79MQMmsz5fz4LvCjjt1gHk9EwEwLC07Q801FMtIiIiIiKt0vaiLdRsL2PpjxNDbXanS4E6XAyo3u4mEDApWlcR7mqazUGH6smTJ3P66aeTmZmJYRh8+umnv3r81KlTGTNmDElJSURERNCzZ0+eeeaZxtYrIiIiIiLtUNG6NXz53F+p2Focaht62tmMu/kOTvrN78NYmexkGAbHXNaDU37bn2Gndgp3Oc3moId/19TUMGDAAK655hrOOeec/R4fFRXFLbfcQv/+/YmKimLq1KnceOONREVFccMNNzSqaBERERERaV+mvPcmBUsWEhWfwNgrrwcgLjVdW2KFWcHybWxZU87IM7sAYLNb6dS/7c6f3puDDtXjxo1j3LhxB3z8oEGDGDRoUOh2x44d+fjjj5kyZYpCtYiIiIiI7MHn8bBy2s/0GHMUdocTgGFnnEt0fAJ9jj4uzNXJThUldXz1/GICAZPU3Fg6D0oJd0lh0ewLlS1YsIDp06fzyCOP7PMYt9uN2+0O3a6srGyO0kREREREpAV4/6G7KVyzCr/Px4ATgh16HfsPomP/Qfu5pzSnuJQIhp7akcpt9XTomxTucsKm2UJ1dnY2JSUl+Hw+HnzwQa677rp9Hvv4448zfvz45ipNRERERERakJ6jj6K6rAy70xnuUuQXasrd2JxWnBHBKDn0lI4A7XrLMsPcfVO3g72zYfDJJ59w1lln7ffYvLw8qqurmTlzJnfddRfPP/88F1988V6P3VtPdU5ODhUVFcTGxja2XBERERERaYGK16/F7nKRmJkNBPeeNgwDq007ALckxfmVfP3SYlJzYzjlN/3b/FZZlZWVxMXF7TeHNttPaadOwdXf+vXrR3FxMQ8++OA+Q7XT6cSpT6VERERERNq8vAVz+fypx4hNSeWSR5/GGRmJzW4Pd1myD+4aHxWl9dRVe4mMdYS7nBYhLB/9BAKBBj3RIiIiIiLSPqV26oIrJob49Ayg0YNopRmkdYzltFv6k9oxFodLowh2OujvRHV1NWvXrg3dzsvLY+HChSQmJpKbm8vdd9/N5s2b+c9//gPACy+8QG5uLj179gSC+1z/7W9/49Zbb22ipyAiIiIiIq2J3+cLDe2Oik/g4oefJCYxGcNiCXNlsrv6Gi9T/reakWd1ISbRBUB2z8QwV9XyHHSonjt3LmPHjg3dvuOOOwC48soreeONNygsLKSgoCD09UAgwN13301eXh42m40uXbrwxBNPcOONNzZB+SIiIiIi0poU563jy2f/wkk3/p7s3n0BiE1ODXNVsjc/vbOSdfNLqNpWz9l/HNyuFyP7NYe0UFlzOdAJ4iIiIiIi0rJ9+9KzLPvpezJ79Oai8U8oqLVgldvqmPDKUo65rCcpOTHhLqfZtbiFykRERERERI675iacEZGMOu8SBeoWxjRNygprSMqMBiA2KYLz7hqq/6f90KQFERERERE5bOqqq1gy6bvQbbvTxdirbsAVHR3GquSXfF4/E19bxgePz2XrhspQuwL1/qmnWkREREREDgtPfR3v3HM7FcVF2BwOeh1xTLhLkn2wWC143H5Mv8n2olpSO2ja7YFSqBYRERERkcPC4Yqg5+ijWDntZ5JzOoS7HPkVFovBCdf0oWxzNRld48NdTquihcpERERERKTJBAJ+/F4vdqcrdNtTV4crSsO9W5rlU7dQV+1hyMkdw11Ki6SFykREREREpFnVV1fz1d//it3p4vQ77sYwDCwWqwJ1C1S0voIf314JQFb3BNI7x4W5otZLoVpERERERJrE9qLNbFy2GMNiZdvGDSTndgx3SbIP6Z3j6D82m4gYO2mdNBr4UGj4t4iIiIiINJmV034mMSuH1I6dw12K/ML2ohpiklzY7FYguIWWVvfetwPNodpSS0REREREGsUMBJj1yftUlZWG2nqOOVqBugXKW1TC+4/P5ed3VrGzX1WBumkoVIuIiIiISKP8/PZrTP3vf/jiqccJ+P3hLkd+hc1hxe8NUF3uxu8NhLucNkVzqkVEREREpFEGnnQ6q6ZPYeDJp2GxWsNdjvyKnF6JnHnbQDK6xGGxqm+1KWlOtYiIiIiIHLCa8u1ExSeEbns9buwOZxgrkr2pLK1jyvtrOPaKnkREO8JdTqukOdUiIiIiItJkzECA6R+8w2u3Xs/W/PWhdgXqlsc0Tb57bRn5i0uZ/N7qcJfT5ilUi4iIiIjIfpmYbFm9Eq+7nvXz54S7HPkVhmEw9vKeZPWIZ8x5XcNdTpun4d8iIiIiInJA6qqr2LBoPj3HHB3uUuQX/L4A24tqSM6OCXcpbYaGf4uIiIiIyCFZP38O87/+LHQ7IjpGgboFqq/x8tmzC/jkqQVsL6oJdzntjlb/FhERERGRPRTnreOTvz4EQGrnrmT37BPmimRf7E4r7Bh/XFPuJiE9KrwFtTMK1SIiIiIisoe0Tl3of+xJGFYrGV27h7sc+RVWm4WTb+yHu9arQB0GCtUiIiIiIgJAxdYiohKSsNntABx/3W8xLJox2tKYAZNZn68nOsFJ36OzAYiMdRAZq62zwkG/ISIiIiIiQv7Cebx11+/58d//DLUpULdMa+dvZd63G5jyvzVUltaFu5x2Tz3VIiIiIiKCCbhraynduAGvux670xXukmQfug5JpWB5Gdk9EohNjgh3Oe2ettQSEREREREA8hbOI6dP/9Dwb2k5ivMqSekQg8VihLuUdkNbaomIiIiIyD6VFxXy6ZOPUFddFWrrNHCIAnULtOSnTXz017nM+HhtuEuRvVCoFhERERFpZ0zT5MvnnmDd3Jn8+MYr4S5H9iMixoFpQn2tDzPQ4gcatzuaUy0iIiIi0s4YhsGJN97Kz2+9ylGXXBXucmQ/ug5JJTphCGmdYjEMDf9uadRTLSIiIiLSDnjr6ynOWxe6ndqxM+ff9xjRiUlhrEr2pjivki/+vhBPvS/Ult45ToG6hVKoFhERERFp46q3l/He/X/iw4f/THlxUbjLkV/h9wf47rWlFCwvY/aXeeEuRw6AQrWIiIiISBvniorGardjsdmoq6wIdznyK6xWCydc04euQ1IZflqncJcjB0BbaomIiIiItEGmaTYYLlxVVooZMIlNTgljVbI39TVeqre7Sc6ODncpshttqSUiIiIi0k55PW6+ef4plkz6LtQWk5isQN0CVZbW8eETc/ni7wup3u4OdznSCArVIiIiIiJtzIrJP7Ji6k9MeuOf1Gq4d4vhqfeRv7iUlTMKQ22uaDtWmwWLzcBd6w1jddJY2lJLRERERKSN6XfsiRStX0OvMUcTGRsX7nLaJdM0qSytw2a3EhXvBKCssIavXlyMK8pOjxHpGBYDh8vGqTf3x2a3EhnrCHPV0hjqqRYRERERaeVM02TNnBkEAn4ADIuFE2/4HTl9+oe5svYj4A80uP3Tu6t4+76ZLJ+2JdSWkhNDUnY0nQYk4/X4Q+2xSREK1K2YeqpFRERERFq5H157iUUTv2bE2RdyxEWXh7ucdsXvDfDF8wspzq/iysdG44qyA5CcFY3FauCu2bXXtNVm4aJ7h4erVDlMFKpFRERERFq5rB69WPzDt0TExIS7lDatZGMVy6dsISLWEdruymq3UF3mxuf2U5xXSYe+SQD0HJVBrzEZ2OzWcJYszUChWkRERESkFQoE/FgswcDW68ixpHftTkJGVpirajtKN1WzZc12OvRNJi4lAoCacjdLJ28mLjWiwR7SYy/vSUS0g4T0yFCb3akw3V5oTrWIiIiISCtimiYLJ3zFe/f9Ca+7PtSuQN14nnofWzdUNmib/tEapvxvDQXLtoXaMrrEMeC4HEad3QXTNEPtWd0TSMyMwrAYSPujnmoRERERkVakvqaa6R++S11lBUt/+p5BJ50W7pJanYA/gMUa7F+sLK3j7ftmYLVZuO6Zo7Dagu25fZIwLEZo5W4AZ6SdI87vFpaapeVSqBYRERERaUUiomM44/a72bJmJQNPPDXc5bQqy6dtYe5X+XQZksqYc7sCEJPkwhlpx+60Ur29nriU4BDugcfnMvD43HCWK62EQrWIiIiISAu3edUKrDYb6V2CvaTZvfuS3btvmKtq2eZP2MDm1dsZe1lPohNcAFitBlVl9RStKw8dZxgGlz40MrRqt8jBUqgWEREREWnB8hfO45O/PkxkXByXPf4sUfEJ4S6pRXHXetm0cjueeh+9RmeG2tfO20pJQRWFayvoNiwYqnP7JHHG7weS1im2wTkUqOVQKFSLiIiIiLQwXo8buyM4lzejey/i09JJzumA3eUKc2XhZQZMthfV4oy0heY6b9tSw7evLCUi1kHPURkYRnCxsP5js/HU+xoE6IgYBzm9EsNSu7RdCtUiIiIiIi3E9qItTHjpWWrKt3PNs69gGAbOyEguHP8EETGxocDYXvh9gdDCYQA/vLmCVbOKGHlWZ4ac3BGA1A4xpOTGkNYxFr83gM0R3Mqq56iMcJQs7ZBCtYiIiIhImNRWVlBfXUViZjYA0fGJFK9fh8/jpmzzJpKycwCIjI0LZ5nNzuvx8/mzCyndWMXVTx6BwxWMLSm5MaybvxVPnT90rM1u5YJ7hoWrVBGFahERERGRcFj28w9MePk5Og4YzDl3PQiA3eXitNvuJLVjZ2KSksNbYBjZHVZqyt34vAGK8yvJ6Rkcst37yEz6HpOF1WrZzxlEmo9CtYiIiIjIYeaurWHtnJmkde5Kck4HANK7dscMBKivriIQ8GOxBIctdxkyPJylhoXX42fxpI0MOiE3tH/08Vf3JireQWxyROg4+46h3SItiUK1iIiIiMhhNun1l1k+5UcGn3ImY6+8HoCkrByuf/51YlNSw1xdeJmmyVfPL2Lz6nJqKjwcdWF3ADK7xYe3MJEDpHETIiIiIiJNxDRNVs2YwudPP0ZtZUWovdvII0jMyiE+Lb3B8e09UENwn+j+x+YQEWOn2xB9P6T1UU+1iIiIiMghMAMBDEuwr8owDOZ8/hHF69fSod9ABpxwChAc0t116IhwltniBAImFktwNfPOA1PI7pkQWpBMpDVRT7WIiIiISCPUVlbw9T/+xr/vuIlAYNdq1ANPPJXhZ55Hdu9+obb2thXW/qyaVcSHf5mLu84XalOgltZKP7kiIiIiIgfA7/NRU76d2OQUAJyRUeQtmEt9TTWbVy4nZ0eI7jv2hHCW2eJ56n3M+HgtNRUelv68KbTftEhrpVAtIiIiIrIfGxYv5MvnniAxK4eLH/orAFabjeOu/Q0xyalkdusR5gpbD4fLxqk3D2Ddgq0MPrFDuMsROWQK1SIiIiIiuwkE/GxesYyImFiSczsCkJidTX1NNRXFhXjqanFERALQc8zRYay09XDX+ajaVk9ydjQAKbkxpOTGhLkqkaahUC0iIiIispsp777J3C8+pu/YEzjppt8DEJOYzKWPPEVqpy5YrNor+WDUlLv5/O8LqavycO7/DSUuJWL/dxJpRbRQmYiIiIi0W4VrV/HTf/5FxdbiUFvnQUNxRkXhjIxqcGx61+4K1I1gd1mxWA0Mi4HX7dv/HURaGfVUi4iIiEi7NeXdN9m4bDGRcQkMP/M8ALJ79eU3r7yN1WYPc3Vtg8Nl47SbBxAImMQkusJdjkiTU6gWERERkTbP665n1ifvk7dwHhc/9CQ2hwOAPkcfR2RcPBldu4eONSwWrBYN6DwUS3/ehDPSTrdhaQBExTvDXJHI4aNQLSIiIiJtkru2JjSE22Z3sGzyJKq3lZK/aD5dh40EgqG6z9HHhbPMNidvcSk/v7cai80gOSeahPSo/d9JpBVTqBYRERGRNmVr/nq+ef4pMAyufPJ5INj7fMSFl2O12cjtNyDMFbZtHfom0XlQCik50cSnRYa7HJHDTqFaRERERFq1iq1F+LxekrJyAIhNTqVsy2YAqraVEpOUDKAe6cPIXefD4bJiGAYWi8HJ1/fFsBjhLkukWWiyiIiIiIi0WvO/+ZxXf3cd0//3dqjNFR3N2Xfez29eeTsUqOXwKS+u5f3H5jDny7xQmwK1tCcK1SIiIiLSaqyY+hNlWzaFbmf16I1hWPD5vJimGWrvOGAwrujocJTY7hSuK6eypI5Vs4rw1GvLLGl/NPxbRERERFqFWZ+8z9T//ocRZ1/AERddAUBqpy7c9M//EBkXH97i2rFeozPx+0w6D0zB4VK8kPZHPdUiIiIi0iqkduqC1WbD7ooItRmGoUDdzEzTZOXMQnxef6it71FZRMY6wliVSPgY5u7jZFqoyspK4uLiqKioIDY2NtzliIiIiEiYlBcXEZ+WHu4y2rVpH61l4cQCug5N5cRr+2AYmj8tbdOB5lD1VIuIiIhIixQI+Jnx0XvUVlaE2hSow69Dn0SsNgvpneMUqEVoRKiePHkyp59+OpmZmRiGwaeffvqrx3/88ceccMIJpKSkEBsby6hRo5gwYUJj6xURERGRdmLy268z/f13+OQvDxII+Pd/Bzlsdh/cmt0zkcsfGcWAY3PCWJFIy3HQobqmpoYBAwbwwgsvHNDxkydP5oQTTuDrr79m3rx5jB07ltNPP50FCxYcdLEiIiIi0n70O+5kohOTGHr6OVgs1nCX026Vbqrm06cXUFvpCbVFxTvDWJFIy3JIc6oNw+CTTz7hrLPOOqj79enThwsvvJD7779/r193u9243e7Q7crKSnJycjSnWkRERKSd8brrsTtd4S6j3TJNk/cfm0Ppxmq6j0jjhKv7hLskkWbTYudUBwIBqqqqSExM3Ocxjz/+OHFxcaFLTo6GloiIiIi0dd76er587q+UFuSH2hSow8swDE66ri9dBqVw5AXdw12OSIvU7KH6b3/7G9XV1VxwwQX7PObuu++moqIidNm4cWMzVigiIiIi4TD53TdYNX0ynz31KAG/5lCHixkwKS+uDd2OT4vk5Bv74Yqyh7EqkZarWXdnf/fddxk/fjyfffYZqamp+zzO6XTidGqehoiIiEh7MvqCSyndmM+RF1+Jxao51OHg9wb44T8r2LCklHP+NISkrOhwlyTS4jVbqP7vf//LddddxwcffMDxxx/fXA8rIiIiIi1YIOAPLUIWER3DBfc/rm2awqym3I3PE6CssEahWuQANMvw7/fee4+rr76a9957j1NPPbU5HlJEREREWrjq7WW8fddtrJs3K9SmQB1eVruFcTf144zbBtJtaFq4yxFpFQ46VFdXV7Nw4UIWLlwIQF5eHgsXLqSgoAAIzoe+4oorQse/++67XHHFFTz11FOMGDGCoqIiioqKqKioaJpnICIiIiKt0vyvP6NkQx4/vfkqfp833OW0W0XrK1gxfUvotivKTlb3hDBWJNK6HPTw77lz5zJ27NjQ7TvuuAOAK6+8kjfeeIPCwsJQwAZ45ZVX8Pl83Hzzzdx8882h9p3Hi4iIiEj7dMRFV+DzeBh8yplYbVoEKxzKCmv49JkFBHwBYpIiyO6hMC1ysA5pn+rmcqD7g4mIiIhIy1ZbWUFkbFy4y5AdTNPkx7dWUlvl4cRr++BwNes6xiIt2oHmUP3WiIiIiEizKNmQx4eP3seQU89i+JnnhbucdisQCPapWSwGhmFw9KU9MACLtdl32xVpE/SbIyIiIiLNomDpYmorylk1fQo+r+ZQh4PX7eebl5cw9f017BywarVaFKhFDoF6qkVERESkWQw59UzsLhfdR47BZtcc6nDYsrac/MWlWO0W+o/NJj4tMtwlibR6CtUiIiIicthszV9Pck4HLNbgXtT9jzspzBW1bx36JHHURd1JyY1RoBZpIhrnISIiIiKHRd7Cebx37x/55oWnCQT84S6n3SpcW46nzhe63e+YbNI7a7E4kaaiUC0iIiIih0XA7yMQ8OOpryPgD4S7nHZpzZxiPn16ARP+tRS//g9EDgsN/xYRERGRw6LLkBFccP/jpHftpn2owyQ2JQKLzcAZaYMAYA13RSJtj/apFhEREZEms2b2dLJ79yMiOibcpcgO27ZUk5gehWExwl2KSKtyoDlUw79FREREpEks/XEinz/1GB8//gDe+vpwl9Mu1dd4mfDqUipL60JtSZnRCtQih5FCtYiIiIg0ifQu3XDFxJLZrSc2pzPc5bRLP7+3irVzt/Lda8toBQNSRdoEzakWERERkSaRnNuRK574O9GJSRiGekbDYcy5XanaVs8xl/bQ/4FIM1FPtYiIiIg0immazPrkfbZt3hhqi0lKVphrZrWVntD16AQX5/7fEJKzNaddpLkoVIuIiIhIo8z/+nOm/vc/fPjIvbhra8NdTru0+MdNvHXvdIrWV4Ta9KGGSPNSqBYRERGRRul15DEk53RgxFkX4IyMDHc57Y4ZMNm4ogyfJ8D6BSXhLkek3dKWWiIiIiLSaD6vF5tde1CHi9ftZ83cYnqNzlAPtbQaswtn0z+lPy6bK9yl/CptqSUiIiIiTcrv8/HN80+Rt3BeqE2BunnVVnpYMb0wdNvutNJ7TKYCtbQKFe4K7p16L9d+dy0vLnwx3OU0Ga3+LSIiIiIHZME3n7N8yo+smzeb655/DVdUdLhLalfcdT4++utcKkvrsdoNug9LD3dJIgfs+w3f8+isRymtK8XAwGf6ME2zTXwgpFAtIiIiIgdk0LjTKVy7mj7HHKdAHQbOCBudB6WyfsFWUnK0ure0DqV1pTw26zEmbpgIQKe4Tjw0+iEGpg4Mb2FNSHOqRURERGSf/D4fVlvL6IfJX1LKjE/WMfrcrnTokwTAxhVlfPfaMlJyYzjj1oGhY7//93JKN1dzxHldye6ZCEDppmqmf7yWmCQXYy/tGTp2/ncb2F5US58jMknvHAdA9fZ6lvy0CVeUg0En5u6qYXEpldvqyeoRT1Jm8IMFT52PDcu2YXda6dgvOXTs9qIa6mt8xCa7iIpzAuD3B6guq8ditRCTuGs+qd8bwMTEYrVgsTTsudu9N88MmLjrfLiiNOxeWjbTNPls3Wf8dc5fqfJUYTNsXNvvWm7ofwMOqyPc5R0QzakWERERkUNSX1PN/x68k3lffRruUggETGZ8so6yLTUNQqfP46e+2ou71tfg+PKttWzbVI3X7Q+11Vd72Li8jKJ1FQ2OLVhWxsrphVRuqwu1VZe7mT+hgCU/b2pw7LKpW5jyv9UNzlFVVs93ry5j0n9WNDh2zlf5fPzkPNbO3Rpqq9nu5u37ZvLugzMbHPvzf1fxz9/9zPwJG0JtdVUe/vm7n5jwr6Xs7AczLIYCtbR4m6o2cePEG7lv2n1UearondSb/572X24ZdEurCdQHo2V87CgiIiIiLc7qmVMpXLOK7YVb6HXkWCJj45r18d11PhwuK4ZhYLEYHHlBNzYsKyMld9fQ56weCVx8/wgstoa9u0dd1J36Gi/J2buOTciI4viremF3NXwL3OfITHJ6JZCctevYyBgHA47LwRnZ8NiMLnFYbRbiUiJCbVa7hcxu8TgiGh4bEWMnNiUCZ9SudtM0sbus2OwN+7ZMfzA07/6BQcBv4vMGWDe/hILlZaHeeZGWyh/w897K9/j7gr9T56vDaXVy88Cbubz35dgsbTd6avi3iIiIiOyVaZrM/vQDOg8eRkqHTs362Cumb2H6R+s48sJudB/e9hfk8nsD+H0BLDYDm90KQMAfoLK0HrvLGho+LtJSrStfxwPTH2BRySIAhqYN5cHRD9IhtkOYK2u8A82hbffjAhERERE5aLUV5bhiYrBYgj3EI86+ICx11JR7qK/xsmpmUbsI1Va7Besveq8tVgvxaZFhqkjkwHj9Xl5f+jr/XPxPvAEvUfYo7hhyB+d1Pw+L0T5mGytUi4iIiAgAFVuL+ODhP5Pdqx8n3XQrhqX53hCXb63FMIzQsOqBx+cQGeug56i2H6hFWqtlpcu4f/r9rN6+GoCjs4/m3pH3kh7Vvn5vFapFREREBICSgg1UlpaweeUy6qqrmm0O9aqZhUx6ayU5vRI57ZYBANgcVnofkdksjy8iB6fOV8eLC1/kP8v/Q8AMkOBM4O4Rd3Nyx5PbxL7TB0uhWkREREQA6Dp0BGfccQ/pXbo166JkaZ2CjxUImHg9fuwOa7M9togcnDlFc3hg+gNsrNoIwCmdTuGu4XeR4EoIc2Xho4XKRERERNqxkoJ8YpNTcEZGNdtjbllTTkVJLb1G7+qJ3l5UQ3xaZLvs5RJpDao8VTw972k+XP0hAKmRqdw/8n6Ozjk6zJUdPlqoTERERER+VeGaVXz02P0kZedy7p8fwuGK2P+dDvUx15bzyVPzsTks5PRKJDrBBUBCevOFehE5OD9t/ImHZzzM1rrgnusXdL+A24fcTrQjOryFtRAK1SIiIiLtlNVuBwMMi4EZaJ7Bi+ld4sjsFk9CeuQeq12LSMtSVl/GX2b/hW/yvgGgQ2wHHhj1AMPSh4W5spZFw79FRERE2rGSgnziU9Oxu1xNfm4zYLJ6TjGrZhVx2s39sViDIdrvD2C1KlCLtFSmafJV3lc8MfsJyt3lWA0rV/a5kt8M+A0uW9O/VrRUGv4tIiIiIntYv2AOCRlZJKQH5zOn5HY8bI/lcfuZ+v4a6mu8rJxRFFrNW4FapOUqqinioRkPMWXzFAB6JPRg/Jjx9EnqE+bKWi6FahEREZF2Yv2COXz25CNEJSRyySNPEZ2Q2OSP4a7z4YwIvsV0RtgYfW4Xais9dB+R1uSPJSJNJ2AG+GDVBzwz/xlqvDXYLXZ+M+A3XNX3KuwWe7jLa9EUqkVERETaibROXYlLyyCtU5cm3zLLNE3mfbuB+d9u4MzbBpHWKThUcvcVvkWkZcqvyOeB6Q8wf+t8AAamDGT8mPF0jusc5spaB4VqERERkXYiKj6Bi8Y/gSs6GoulafeCNgyDiuJavG4/K2cWhkK1iLRcvoCPN5e9yYsLX8QT8BBhi+C2wbdxUc+LsBiapnGgFKpFRERE2rD533xBUnYOHfoNBGjSHuqSgipikly4ooJDQ0ee1YWc3ol0G6ah3iIt3cqyldw/7X5WlK0AYEzmGO4fdT+Z0RpdcrAUqkVERETaqNWzpvHjG//E5nRy1d9eIC41vcnOPffrfGZ9sZ7+Y7M58oLuAETFO+k+vOkeQ0Santvv5p+L/snrS1/Hb/qJdcRy5/A7Ob3z6RiGEe7yWiWFahEREZE2qvPg4XQcMJiMbj2JTWna3uO0jrFggrvWh2maejMu0gos2LqA+6fdT35lPgAndjiRu0fcTXJEcngLa+W0T7WIiIhIGxbw+7FYD23+tGma5C8uxbAYdOy368132ZYaEjOjDrVEETnMarw1PDf/Of678r+YmCRHJHPviHs5rsNx4S6tRdM+1SIiIiLtjBkIMOmNf5KUlcvAk04FOORADbBqZhE/vLmC6AQnWeMTsDuC51SgFmn5pm6eykMzHqKwphCAc7qdwx1D7iDO2bQ7ALRnCtUiIiIibcTaOTNZOOErDMNCh/4DScjIapLzdhmSyrxvN9B5UAq0+DGOIgJQXl/Ok3Of5PN1nwOQFZ3Fg6MfZGTGyDBX1vYoVIuIiIi0EV2Hj2Lo6eeQ0qFTowO13x9g6U+bKSmo4virewNgd1i56P7hWK3aYkekpTNNk+82fMdjsx6jrL4MA4PLel/GLQNvIdIeGe7y2iSFahEREZFWzO/zYrFYMSwWDMPg6MuuOaTzVZXWM/2jtQQCJr1GZ5DVIwFAgVqkFdhau5VHZz7KpI2TAOgS14XxY8YzIGVAmCtr2xSqRURERFqpuuoqvv7H34hNSuH4629u9Arcnjofjojg28L4tEiGntqRiBgHGd3im7BaETlcTNPkk7Wf8Lc5f6PKW4XNYuP6ftdzXb/rcFgd4S6vzVOoFhEREWmlPLU1bFi8AKvNzuBTzyQpK+eg7u/3Bpjx6TpWzijk4vtHEBXvBGDYqZ0OR7kichhsrNrI+OnjmVU0C4C+SX0ZP2Y83RO6h7my9kOhWkRERKSVKC8qpDhvHT1GHQFAXGo6o867mA79Bh50oAaw2AyK8ypw1/pYO28rA447+HOISHj4A37eWfEO/1jwD+r99bisLm4ZdAuX9boMq+XQV/2XA6dQLSIiItIKlGzI4607f4/VYSenTz8iY4Pb4Yw69+KDOk/h2nLSOsVisQbnYB91UQ/qqjzk9kk6HGWLyGGwZvsaHpj+AEtKlwAwPH04D456kJxYfTAWDgrVIiIiIi1UXXUVEdExACTndiS1Uxci4+Lw1NaGQvXB+PGtFSyfVsjRF3en79HZAKTkxjRpzSJy+Hj9Xl5d8iqvLHkFX8BHtD2aPw79I+d0O6fRayrIoVOoFhEREWlhKrYWM+GlZ6ncVsI1z/wTi9WKYRhc+ODj2J2uRp83OScGw1JETYWnCasVkeawpGQJ90+/n7XlawE4JucY7h1xL2lRaWGuTBSqRURERFqYyNg4SjZuwFNbQ9G6NWR27wlwUIHaDJisnFlIcnZMqDe6z5GZZPVIIDEj6rDULSJNr9ZbywsLX+DtFW8TMAMkuhK5e8TdnNThJPVOtxAK1SIiIiJh5Pf5WDV9MkXr1nDs1TcCYHe5OPV3fyQxK4fY5JRGnXf2l3nM/TqfjC5xnP3HwRiGgcVqUaAWaUVmFc7iwekPsql6EwCndz6d/xv2f8S74sNbmDSgUC0iIiISRlWlJXz74rOYZoA+xxxPWqcuAHQcMPiQztvnyCyWT9tCpwEpmAETw6oeLZHWotJTydNzn+ajNR8BkBGVwf2j7ueIrCPCXJnsjUK1iIiISDPy1NVStG4tuX37AxCfnsHAk08lKj6R+LT0Rp3T6/Yz/7sNmAGTkWcGQ3l0gpMrHh2N1WZpstpF5PD7oeAHHp35KCV1JQBc3PNifj/490TZNcqkpVKoFhEREWkm5UWFvH3PbQR8fq5/4XUiYmIBOPaqGw/pvIXrypn7VT4Wi0HvMZnEJkcAKFCLtCKldaU8PutxvtvwHQAdYzsyfvR4Bqcd2qgVOfwUqkVEREQOo4Dfj8VqBSAuLZ3YlDR8Hg+VpSWhUH3w5wxQUVJHQnqw5yq3dxJ9jsoiu0cCMUmNXx1cRJqfaZp8uf5LnpjzBBXuCqyGlav7Xs1NA27CaXWGuzw5AIZpmma4i9ifyspK4uLiqKioIDa2cX98RERERJpTbWUFU//7HwpXr+Tyv/4diyUYrKvKSomKTwjdPljbi2r48vlF+L0BLn9kNFa7eqNFWqvC6kLGzxzPtM3TAOiV2Ivxo8fTK6lXmCsTOPAcqp5qERERkcPA5nCwZtZ06qurKFiyKLTwWExi8iGdNzYpAr/PxO83KSuqISUnpinKFZFmFDADfLj6Q56e9zQ13hocFge/GfgbruxzJXaLPdzlyUFSqBYRERFpAltWr6BgySJGnnsRAA5XBMddcxPRSclk9+zTqHN66n0s+WkTJQVVnHxDPwCsdgun3tyf+NRI7M7G9XaLSPhsrNzIgzMeZHbRbAAGpgzkoTEP0SmuU5grk8ZSqBYRERE5RJWlJfz3/jsxzQCdhwwntWNnAHqOOfqQzuup8zP7yzwCPpPCdRVkdIkDUO+0SCvkD/h5b+V7/H3B36nz1RFhi+D3g3/PRT0uwtrI6SDSMihUi4iIiBwkMxCgbMsmkrJzAYhNTqHXEUdjsdlxRUU3+rw15W6K1lfQZXAqENwWa/hpnYhOcJHWUUFapLVaX7GeB6Y9wMKShQAMTx/Og6MfJCcmJ7yFSZNQqBYRERE5CNXby/jg4T9Tta2UG174N67oYIg++eY7MAyj0eetKKnj3fEzAbiicxxR8cFVf4ec3PGQaxaR8PAFfLyx7A1eWvgSnoCHKHsUfxj6B87rdt4hvV5Iy6JQLSIiInIQouITsFqtWCwWtuavJ7dvf4BGvUH21PtwuIJvx+JSIkjrGAsm1Nd6Q6FaRFqnVWWruH/6/SzfthyAI7KO4IFRD5AelR7myqSpaUstERERkX3wuutZOOEr8hbM5bz7Hgltg1W6cQMxSck4I6Madd7aSg+T/7uKrflVXPrQSKy24LZYnjofdpdVPVgirZjX7+VfS/7Fvxb/C5/pI9YRy13D7+K0zqfpd7uV0ZZaIiIiIofIDASY9en7uGtqWDdvNt2GjQIgOafDIZ3XEWGlcF0FtRUeNq/eTm7vpB3temsm0potK13GfdPvY832NQAcl3sc9468l+SIQ9tKT1o2vXKLiIiI7FBVVkr+wvn0O/ZEABwRkRx58ZXYHE46DxrWqHOapsmmldspWF7GmHO7AmCzWzn28l5EJzhJymr8wmYi0jK4/W5eXPgibyx7g4AZINGVyN0j7uakDiepd7odUKgWERERAWorK3j91hvweT1kdO1Ocm5HAAaccMohntfDl88vIuA36TwwJbQtVoe+SYdasoi0AAu2LuD+afeTX5kPwLhO47hr+F0kuhLDW5g0G8vB3mHy5MmcfvrpZGZmYhgGn3766a8eX1hYyCWXXEL37t2xWCzcdtttjSxVREREpGnVVlaErkfGxtF58DCye/XF7/c3+pyBgMnWDZWh21FxTvoclUW/sdnEJLoOqV4RaTlqvbU8MfsJrvzmSvIr80mJSOG5sc/x16P+qkDdzhx0T3VNTQ0DBgzgmmuu4Zxzztnv8W63m5SUFO69916eeeaZRhUpIiIi0pTqq6v55oWn2LxyOdc9/1pob+mTb7kDu6Pxq27XVXn46Ml5VJe5ufzRUUTFBc911IXdm6RuEWkZZhXO4oHpD7C5ejMAZ3c9mz8M/QNxzrgwVybhcNChety4cYwbN+6Aj+/YsSPPPfccAK+//vrBPpyIiIhIk3NGRlKxtRh3XS0bly6m24jRAI0K1KZphuZMuqLtRETbqa/xUra5JhSqRaRtqPZU8/S8p/lg9QcAZERl8OCoBxmdNTrMlUk4tcg51W63G7fbHbpdWVn5K0eLiIiI7JsZCLB2zkxWz5rGKbf8AcNiwbBYOPHG3+GKjiUxM6tR5/V5/Cz+cRPrFpRwzp8GY7VaMAyD467qTWSsI7T/tIi0DVM2TWH8jPEU1xYDcGGPC7l9yO1E2Ru3tZ60HS3y1f7xxx9n/Pjx4S5DRERE2gBPfR0TXn4Od20N3UcdEdoWK7N7r0M+98IfNlJX6WHt3K30GJEOQHxq5CGfV0Rajgp3BX+d81c+X/c5ADkxOYwfPZ5h6Y3bEUDanhYZqu+++27uuOOO0O3KykpycnLCWJGIiIi0Fl6Pm43LFoe2wHJGRjH8rPPx1NWR1aN3o89bW+lh/cIS+h4V7Nm2OayMPqcLZgC6Dk1tktpFpGX5YcMPPDzzYbbVb8PA4PLel3PLoFuIsEWEuzRpQVpkqHY6nTidmoMkIiIiB8ddW8u/77iJmu1lXPXUiyRl5wIw/MzzDum8njofb98/A2+9n+ScaNI7BRcj6jky45BrFpGWZ1vdNh6f/TgT8icA0DmuM+NHj2dg6sDwFiYtUosM1SIiIiKN4YyMJKNrD4rz1lJdVhYK1Y1RX+PFFWUHwBFho8ugFLYX1TZVqSLSApmmybf53/L4rMfZ7t6O1bByTd9ruHHAjTit6vSTvTvoUF1dXc3atWtDt/Py8li4cCGJiYnk5uZy9913s3nzZv7zn/+Ejlm4cGHoviUlJSxcuBCHw0Hv3o0fgiUiIiKyNydcfzPOqGistsb1HXjdfia9tYINS7Zx2cOjiIx1AHD0xT2w2i2hlb5FpG3ZWruVh2c+zE8bfwKge0J3Hh7zML2TlFnk1x30X5u5c+cyduzY0O2dc5+vvPJK3njjDQoLCykoKGhwn0GDBoWuz5s3j3fffZcOHTqQn5/fyLJFREREgorXr2X9/DmMPPciDMMgMi7+kM5nc1ioLK3H6/azYek2eo3O2NFubYJqRaSlMU2TT9d+ypNznqTKW4XNYuPG/jdybd9rsVvt4S5PWgHDNE0z3EXsT2VlJXFxcVRUVBAbGxvuckRERKSFqK+p5t+330RtRTljr7yewaeceVD3N02TzavLWTm9kGOv6InFagGgOL8Sq81Ccnb04ShbRFqILdVbGD9jPNO3TAegb1JfHhrzEN0SuoW5MmkJDjSHak61iIiItFquqGjGXHgZi7+fQJ9jTjjo+/t9Ab57dSl1VV5yeiXQY8fCY2kd9SG+SFsWMAN8sOoDnp73NLW+WhwWB7cMuoXLe1+OzaKIJAdHPzEiIiLSqvU/7mT6HnMCFuv+h2cHAiaFa8rJ6pEAgM1uZfBJHagoqSOjW/xhrlREWoKCygIemP4Ac4vnAjAodRDjR4+nU1ynMFcmrZVCtYiIiLQqPo+H2Z99yPAzz8PmCC4idiCB2u8L8P5jcyjbUsP5dw8ltUOwN3rg8Y1fIVxEWg9/wM87K97hHwv+Qb2/nghbBLcNvo2Lel6ExbCEuzxpxRSqRUREpFX59qVnWTV9Mlvz13HWn+771WNN0wyt1m21WUjJiaGm3E1FSV0oVItI27e+fD33Tb+PxSWLARiRPoIHRz9Idkx2mCuTtkChWkRERFqVAcefzMZlixly6ln7PCbgD7Bo0iaWTd7MuXcOISI62KM96pwuHHVRdxwRegsk0h54A17eXPYmLy58EW/AS5Q9ij8O/SPndjtX2+NJk9FfFBEREWlVcvr057q/v4rd5drnMYbFYM2cYipK6lg2ZQtDx3UEICrO2UxViki4rSpbxX3T7mNF2QoAjsw6kvtH3U96VHqYK5O2RqFaREREWrwV034mt09/ouKDC4ztLVB73X7szuDcasMwGHV2F6rK6ukxQm+gRdoTj9/DK4tf4bUlr+EzfcQ6Yrlr+F2c1vk09U7LYaFQLSIiIi3a6plT+foffyMuNY1LH3uGiOiYPY4pL67ls+cWMOyUTvQ+IhOAnF6JzV2qiITZkpIl3D/9ftaWrwXg+Nzj+fPIP5MckRzmyqQtU6gWERGRFi21YxfiUtPoPGgYrqjovR6zdl4x1WVuFv6wkR4j07HatJKvSHtS76vnxYUv8ubyNwmYARJdifx5xJ85seOJ4S5N2gGFahEREWnR4tMzuPTRp3FFRe9z6OaQcR2xWC30Gp2hQC3Szswvns/90+9nQ+UGAE7tfCp3DruTBFdCmCuT9kKhWkRERFqcsi2b8Hk8pHbsDEBEzJ7bX23bXE1iRhSGxcAwDAaf1KG5yxSRMKr11vLc/Od4b+V7mJikRqRy36j7OCbnmHCXJu2MQrWIiIi0KFVlpXz46H24a2o4956HyOzec49j1i8sYcKrS+lzZBZHXtBNiw+JtDMzC2fy4PQH2Vy9GYBzup3DH4b+gViH9p+X5qdQLSIiIi2KwxVBXGoaNdu3E5+esddjvG4/AZ9JTbmbQMDEalWoFmkPqjxVPDX3KT5a8xEAGVEZPDjqQUZnjQ5zZdKeGaZpmuEuYn8qKyuJi4ujoqKC2Fh9+iQiItLW+Twe6qoriUnc94q9G1eWkdUtHotVc6hF2oPJmyYzfsZ4ttZuBeCiHhdx25DbiLJHhbkyaasONIeqp1pERETCzu/zUbhmJdm9+gJgczj2CNQrZxbSZVBqaC/qnJ7aMkukPahwV/DE7Cf4Yv0XAOTG5DJ+9HiGpg8Nc2UiQfpoV0RERMLKNE0mvvI874+/h8U/TNjrMXO/zuOHN1bwzcuLCfgDzVyhiITLxA0TOfPTM/li/RdYDAtX9r6SD8/4UIFaWhT1VIuIiEhYmWYAi80KBkTFx+/1mOyeicyfUEB2z0QN9xZpB0rrSnls1mNM3DARgM5xnXlozEMMSBkQ5srkUPgDJpPXlPDOzALOG5LFyX33vm5Ga6M51SIiIhJ2pmmyNW8daZ277vOYmgo3UXHOZqxKRJqbaZp8lfcVT8x+gnJ3OVbDyjV9r+GmATfhsDrCXZ400tbKet6fu5H3Zm9kc3kdAGO6JvHOdSPDXNmv05xqERERadFKCvJJzumAYQT3md49UPu8fqZ9sJbBJ3cgJtEFoEAt0sYVVBbw6KxHmb5lOgA9Enrw8JiH6ZXUK8yVSWMEAiZT15by7qwCvl9RjC8Q7MuNi7Bz3pBsLh6eG+YKm45CtYiIiDS79Qvm8OlfH2bACadw7FU3YFgaDume+v4alk3ZQuG6Ci748zAsFm2ZJdJWefweXl/6Ov9a/C88AQ8Oi4Mb+t/ANf2uwW6xh7s8OUil1W4+mLuJ92YXUFBWG2of2iGBS0bkckq/DFx2axgrbHoK1SIiItLsKrduxQwEcNfW7PXrQ8Z1pHBdBUdc0E2BWqQNm104m4dnPkx+ZT4AIzNGcu/Ie+kQ2yG8hclBMU2TGeu28c7sAr5bVoTXH+yVjnHZOHdwsFe6R3pMmKs8fDSnWkRERMJiw5KFZPfqi9UW/IzfDJgYuwXoQMBUoBZpo7bVbeOpuU+FtslKciXxf8P+j3GdxmEY+r1vLcpqPHw0bxPvzi4gr3TXh6QDc+K5ZEQup/fPJMLRenulNadaREREWpSa8u24oqOx2oLDOTv0Gxj6WlVZPV+/tJijLupBRpc4AAVqkTYoYAb4eM3HPDPvGSo9lRgYXNDjAm4dfCuxDnWetQamaTI7r4x3ZxfwzZIiPDu2OYx22jhrUCaXDO9A78z29X+pUC0iIiKHXW1lBe+Pv5uY5BTO+MM9OFwRDb4+58s8SjdW8/N7q7jwnmENeqxFpG1YVbaKh2c+zKKSRQD0TOzJfSPvo39K/zBXJgeiotbLR/ODvdJrt1aH2vtlxXHJiFzOGJBJlLN9xsv2+axFRESkWZVt2UTVtlI87nrcNTV7hOojL+yOGTAZfkZnBWqRNqbWW8tLi17ireVv4Tf9RNoiuWXQLVzc82JsFsWRlsw0TeYXbOedWQV8tbgQty/YKx3psHLmwGCvdL/suDBXGX6aUy0iIiLNonDtKhyuSJKycwCoq/IQEaN9Z0Xash8LfuTx2Y9TWFMIwPG5x3Pn8DtJj0oPc2XyayrrvXy6YDPvzipgZVFVqL1XRiyXjMjlrIGZxLja/srsmlMtIiIiYWUGArjranFFRQOQ0bVH6GubVpbx9ctLOOrC7vQclRGuEkXkMCmsLuTx2Y/z48YfAciMyuSeEfdwdM7RYa5M9sU0TRZtquDdWRv4YlEhdV4/AC67hdP7Z3LJiFwG5sRrIbm9UKgWERGRJmeaJj+99Rp5C+dx3j0PEZuS2uDrBcvK8Nb7WTO3mB4j0/UmTaSN8Aa8vLviXV5Y+AJ1vjpsho0r+lzBjf1vJNIeGe7yZC+q3b5Qr/TywspQe/e0aC4ZnsvZg7OJi2j7vdKHQqFaREREmlx9dRVrZk+nqrSELWtW7hGqR53dhdhkF71GZypQi7QRi0oW8dCMh1i9fTUAg1IHcd/I++iW0C3MlcneLN1cwTuzCvh84WZqPMFeaYfNwmn9MrhkRC5DOiTo9fkAaU61iIiIHBZV20opWLqIPkcfB8Dm1dvJ7KahgyJtTYW7gufmP8eHqz/ExCTOGccdQ+7grK5nYTEs4S5PdlPr8fH5wi28O7uAxZsqQu2dU6K4ZHgu5w7OJiFKa13spDnVIiIi0ux8Hg82R/ANWUxScihQz/9uAzM+XsfA43MYfW5XBWuRNsA0Tb7K+4on5zxJWX0ZAGd2OZM7ht5BoisxzNXJ7lYUVvLurAI+XbCZKrcPALvVYFzfYK/0iE6Jel0+BArVIiIi0iQ2rVjKV8/9ldNuv5usHr0afM0VFZyPZ7Gp10qkLcivyOeRWY8wq3AWAJ3jOnPvyHsZlj4szJXJTnUeP18uDvZKLygoD7V3TIrk4uG5nDckm6RoZ7PUYvr91C9bhr+8nKgxYzCs1mZ53OaiUC0iIiJNYvZnH1K9vYz533y+R6juPSaT5OxoUjtoGpdIa+b2u3ltyWu8uuRVvAEvTquTG/vfyFV9rsJu1WJWLcGa4iremVXAx/M3UVkf7JW2WQxO6pPOJSNyGdU5CYvl4HulzUAA0+fDsmM0kun1UvHlV/i3byfxissxbMFoWfbuu5S/9x4x48aR8tvf7rizSf4FFwLQbfo0bIltaySDQrWIiIg0idNvv4uZH/+PkedcSMAfYOH3G+k/NhubI9gjoUAt0rrN2DKDR2c9yobKDQCMyRrDn0f8mZyYnDBXJvVeP98sLeTdWQXMyd8eas9OiODi4bmcPzSb1BhXg/v4q6vxbd2KJSoKe1oaAAG3m9J//ANfeTkZDz4YCsolf/8HpS+/TOKVV5J25/8FT2AYFN59NwBxZ56BLSkpeI7qGtxr1uLquyn0WIbNhrNbN7DbMOvrD9v3IVwUqkVERKTRAgE/FkswNNudLo68+EoAfnhzOStnFFG4roJTftNPc/VEWrHSulKenPMkX+d9DUBKRAp3Dr+TEzucqN/tMFtXUs17swr4cP4mymu9ZFWXcERVMR37duWEM4/iyK7JUF/H5j/cQf727XR46z8Y9uCIgtKXXqLstddJvOoq0u66EwDDamXba6+DaZJ6++2hoGy4XBAI4N++K7AbNhsxJ5yA4XTCbmtfx447mYh+fbFnZzeotfMXnx/ub0fYKFSLiIhIo3jqavnw0fsYcMIpoQXJduo1OpO8RaX0HpOhN90irVTADPDBqg94bv5zVHmrMDC4uOfF3DLoFmIcMeEur00yPR78FRXYUlJCbTWzZ1M3fz4R/fsTNXo0bp+fiXPWk3Dbddhrq3hj3AP4LDYy41zcW72ajrM+IbHXVaR1Pzd4TqeT6p9+AtMMnjs5GQBbQgKWmBiw7FrrwrDZSLr+eiyREaHwDZBw4QXEnXkm1oT4BvVm/+PvezwHR04Ojpz2NXpBoVpEREQaZfH331K4ZhXlxUV0HTYSZ2RU6GuZ3eK5/NHROCP0VkOkNVpZtpKHZzzM4tLFAPRK7MUDox6gT3KfMFfWNvjKyqhbtAh7Rgaunj0B8JeXs3rkKAB6Ll6EsWPucs3kyWx79TWsF17CR+UJfDBvE+XV9XxRUYIFk1M6RHLWcf05unsqlR+VUVGahy09LfRYhtVKxmOPYYmOwhK163U68dprSbruuj1qS73j9j3arHFxWOOa9FvQpugvnYiIiDTKkFPPor6mmq5DR+L32fnm5SUceWF3ohOCq8kqUIu0PjXeGl5Y+ALvrHiHgBkgyh7F7wb9jot6XITV0rZWbG4ugZoa6lesIGLIkNDIndKXXmb7W2+RcMXlpN9zD0Cw19gwwDDwV1ZiS07G6w+wPD6Xwr5H8s06C5Pd6wFIi4tg1h1PcMLwLjzXt3to7nPC+eeTcP75e9QQf/ZZe7RpFFHT0V87EREROWCmaYbeiBkWC0dcdAUAX72wiPwl23DXeTnr9sHhLFFEGsE0TSYVTOLx2Y9TXFsMwIkdTuTO4XeSGpka5upaD9PnCw6x3jEX2fT5WD3mCMz6erpM/C40LDpiwABqZkzHlpgUuq9htdJ95gwsMTFsKq/nvW9X8v7cTZRWR0HXMzEMOLpbCpeOyOXYnqnYrNqisKVQqBYREZEDNvPj/+Kpq+OoS69u0Mtx5IXdqa9ZzjGX9AxjdSLSGFuqt/DYrMf4edPPAGRFZ/HnEX/myOwjw1xZy2aaJpgmxo45yZXffsuWu+4masQIcv75MrBj1evu3fFt3YqvuDgUquNOO5W4005tcD6fP8APm+p4d9YqJq8pCa39lRLj5IKh2Vw0LJecxMjme4JywBSqRURE5ICUbMhj+vvvANCh30By+gzEagu+mYxNjuCcPw3WcEKRVsQb8PLW8rd4edHL1PnqsFlsXN3naq7vfz0Rtohwl9eiFd7/AFXffUfm3/5G9BFjALBnZGDW1+POy8Pt8+Pzm3j9ASL/8RI+ZwRbfQG8xVV4/AG8O77m9QXw+APMLyjnf3MKKK50hx7jyG7JXDI8l+N7p2FXr3SLZpjmbuuft1CVlZXExcVRUVFBbKz2uBQREQmXpT9OpKJkK12GncZ3/1rGSTf0Ja2j/jaLtDYLti7goRkPsbZ8LQBD0oZw38j76BLfJcyV7V0gYOLxB/AFTLy+AF5/oEE49exoC93eEVhD4dXf8Hrw67+4vVvbztuR27Yy5qf3cdZV88ZZd4Qe69KJ/2Lourl8PPA0Pul7Ih5fANPrJbGylILIJEyjcSE4KcrBeUOzuXhYLh2To/Z/BzmsDjSHqqdaREREDljfsScA8PVLi6kqq2fOV3mcdvOAMFclIgeqvL6cZ+c/y0drPgIg3hnPH4f+kTO6nNEiRposKNjOA58vY/P2ul1B12/iDxz+fsDBxasYU7iE+SndmZbVH4A4dzU3rJwFwIq1W6i1B3vwX889ireyR5MXm4m32rPjDAZVUSl7nNcwwGG14LBasNss2K0GNosFx47rdquF1Bgn5wzO5sQ+aThtWhCutVGoFhERkX0qzlvH3C8+5sQbf4fd6Qq1H391b2Z/mcfw0zqFsToROVCmafL5us95au5TbHdvB+Ccbudw++DbiXfFh7c4gj3Rr03N44lvV+I7gABttRihQOqwWrBbLdhtwdt2y27XQ1/fcdtmwYlJ19VzSd2ynmXjLsHudGC3GvT8ejZdZ8xkcGY0x59+Uei+xak348/tyDN9+uNwuXac18BuC37d9ss6dvua3WrBagn/hxUths8DJSugcBEkdYUOo8NdUZNQqBYREZG98vu8fP7UY1SWFBMVH8+gcZcRlxLspXG4bBxxXrcwVygiB2J9+Xoenvkwc4vnAtA1viv3jbyPwWktY6X+7TUe/vDBIiat3ArAKf3S+d2x3XDadoRi225h9SCDqr+qirqFi8BiED0mOPfZDARYPeI3BKqqOO32q3D16gVAbepZVOfGkjt6NGNH7faB4bBbmvYJtxfeeiheBoULgyG6cBFsXQ7+HT37g69UqBYREZG2zWqzM+7m25n+/jvEZ47lnQdmcvzVveg+LD3cpYnIAaj31fPK4lf497J/4wv4cFld3DTgJq7ofQV2qz3c5QEwN7+M3723gMKKehw2C/ed1pvLRuQ2aih6oL6e+hUrcHbujDUuDoCq73+g8O67iRg6JBSqDYuFuNNPxzQDGLuNwIkcNIjIQYOa5om1N54aKFqyKzwXLoKtK8D073msKw4yBkBan+av8zBRqBYREZF9yu7Vl/Pvf4zJ/12NGTApWl+pUC3SCkzbPI1HZj7CpupNAByVfRT3jLiHrOisMFcWFAiYvDx5HU99txp/wKRTchTPXzKIPplxB3R/0+/HV1KCPX3X69GGy6+gfskSsp59ltiTTwIgon8/HB064OzUucH90++/r+meTHtTXwGFixsG6NLVwF6G7UcmQcZAyBwYDNIZAyC+Q3CieRuiUC0iIiIhPo+HH998hZHnXkRMYjIAhmFw1IXdyewWT9chqWGuUER+TUltCU/MeYIJ+RMASI1M5e7hd3Nc7nEtYiEygNJqN3e8v4jJq0sAOHNgJo+e3Y9o596jiWma4Pdj2IJfr1+1mg2XXIIlKopuk38OHefq3Rvvli0EaqpDbc4uXegy4dvD+GzauNqyhsO3CxdB2fq9HxuTsSM4D9wVoGMz21yA3httqSUiIiIh37/6AosmfkNyTkdGX3QvXYektZg34iKyb/6An/+t+h//WPAPqr3VWAwLl/S8hFsG3UKUveVszTRz/TZufW8BW6vcOG0Wxp/RhwuH5YReZ8xAAAwjdHvba69R9sabJFxxOcnXXw9AoLaWVcOGYzgcdJ30A7aEhGB7fT2G06nXrMaq3gpbFu4IzwuDvdEVBXs/Ni4XMvrv6oVO7w8xac1XazPRlloiIiJy0IadcR6bV60gPuMkvnt1Ods21zDyzJa5b62IBC3ftpyHZjzEsm3LAOib1Jf7R91Pr6ReYa4s2MscqKnFdLl48ef1PPv9arqWFXBuTT5nnHkEvYbnho5bO/ZYfCUldP1xEvbUXaNifCUl1C9ZGrptiYyk85df4MjJCfVeA1hcu+ZHy68wTajcsmcPdFXh3o9P7Lyr53lnL3RkYnNW3OIpVIuIiEhIXGoalz/xHMunFrFl7WoS0iLDXZKI7EO1p5rnFz7PeyvfI2AGiLZH8/vBv+f87udjtRy+vY4Dbjf+sjJsqakY1uDj1M6ZQ9VPP+Hq2Yu4008DgkF59dBhBGpqeO7Gp/m2OADAxa7tjPz5c2LTA3D+6UBwmonp84Hfj3/btlCojh03johBg0IrdO/k7KTt/A6IaUL5ht16oHdcakv3crAByd13BejMgZDeL7iwmPwqhWoREZF2bsmk70jJ7Uh61+4AWCxW+h6VRXbPBOJTFapFWhrTNJm4YSJPzH6CrXXBbajGdRzHn4b9iZTIlEadz79tG75tZTi7dA71/lZPm0bVtxNw9e9Hwvnnh45dNWQo+Hx0/fkn7GnBIb91S5dR9trrxJ56aihUG4aBz+HEUlPD6lUbiUjJ5eGz+jLOm0xFdM0eK213ePMNLNHR2JKSQm32zEzsmZkH/01qjwKB4HznwoUNe6HrK/Y81rBCaq+GPdBpfcAZ3cxFtw0K1SIiIu1Y3oK5fPfKP7A7nPQ97o8cdckwbPZgz5MCtUjLs7FqI4/Neoypm6cCkBOTw70j7mV0VsP9fk3TxJOXh3/bNiIGDsSwB7fQqpo0iYrPvyBy8GASr7g8dPyaY8YGg/JPP4ZW1HavWUP5Bx8QW1MTCtWGYWBNiMdfXoG/vCIUqiMGDCDxqqtw9esLgD9g8tz3q3l3xM1U2Vx0zErm80sG0S0tBsgmauSIPZ6bs4ummhwwvw+2rWnYA120GDzVex5rdUBq74Y90Km9wR7R3FW3WQrVIiIi7VhWz97k9h1AebGD5dNrCJirOP6q3uEuS0R+wev38uayN5j49YtEVHtwdrVz1YDruK7fdXgm/kjBfdcQOWpUaDEvgPVnngVeL10n/RDq7fVu3EjVt99iWAzYEaoNw8CWnIzp8RCo3hXKIocMIfnW3+Hq2XDodZdvvsUSFdlgQbDIwYOIHBzseS6urOfW9xYwK68MXHFcNCyHB07vQ4Tj8A1Jb9N8HihZ2bD3uWgp+Or2PNYWAel9G/ZAp/QEm6O5q25XFKpFRETaMUdEJOfc9QCb11Ty41srGXBcTrhLEhGgesoUKj77nIjBg1h/bHcenvkw68vX8c5bfux+cHz2Ml16jASgtqSEmukzsMTtmvtqGAaO3Fzw+QjU14faI0eMIO3Pf8bZrVuDx+s66QcMi6VBW0S/fkT067dHbdbofa8m/vPqEm7/30LKajxEOaw8dk4/zhzYMvbGbhW89bB1WcMe6K3Lwe/Z81hHdHDV7VCAHhCcE21VxGtu+o6LiIi0M2VbNlNakEe3EWOCQzltdnJ7JXHZ+FFY7Zb9n0BEmoxpmhTeey/uVavJefklbMnB/eE9+Ruo/PJLlmycw5+82wBIjEjC381FjD2OLGd66BxRY8aQ+cRfcPxi8a4uX325x+O5evbE1bPnHu2/DNQHy+cP8NTE1bz00zoAemXE8sIlg+icojm6ezBNqNsO2/ODi4ht3wClq3cE6BVg+ve8jytuzxW4E7vAIf6/SdNQqBYREWlHaisr+Oix+6gsLSG183mcc+dFRMU5ARSoRZqYaZoAoWHS1VOmUvrCCzi7dSXj4YdDX6udOxfvhgLca9ZgS06mrL6M/0YtpfgYG6sySgAL53U/j9sG30bchXuuxOzs0iWs85G3lNdx63sLmLthOwCXjczl3lN747K34+He7qpgWC4v2BWcQ/8WgKdq3/eNTNoVnHdeEjqC9t9usRSqRURE2hFXdDSdBg1j2U8zqChNYtqHaznx2j7hLkuk1QvU1zfYJ3nzHXdQM2062S+9SOTgwcFGM0DdwoUEahouJpX6+9+D1UagSy6vLH6F15e+To23BkbBqIwxvD/k9hax5/TeTFpZzB3vL6K81ku008Zfzu3Haf3bwWrd3nqo2LgrLP8yONeV7f8c0WkQ3wESOjTcCzo2SwG6lVGoFhERaUcsFivHXXMT/Y8/l7lfF3HURd3DXZJIq+KvqsKwWLBEBecV186bx6bbbsOemkanjz7cdVxlFf6KCtxr1oZCtatfP7Kefgpn94a/d5Enn8inaz/lxZ8up6SuBIBeib24bchtjM5suKp3S+H1B/jrtyv515Q8APplxfH8JYPokLTv+datit8HlZsbhuXygl3Xqwr3f46IhGBojs8NBuf4DsEe5/gOEJ+j1bfbEMPcOS6lBausrCQuLo6KigpiY2PDXc6+Lf4AIhOg87Ga3yAiIi1GwO9n1YwpdB4yBmeEPdzliLQKgdpavEVFODt3DrVtuu12qr79loy/PE78WWcB4MnPZ93J4zAiIugxb25obnLd0mUYFgNH584NerB3Z5omkzZO4rn5z5FXEQynWdFZ3DLoFk7pdAoWo2W+n9xYVsvv3lvAwo3lAFw1uiN3n9ITp60VDfcOBKBm6y+GZefvGp5dsWnvc5t3Z4/aLSzvCM+h6x3A1YJzixyQA82h6qluKj4PTLgbakogoRMMvRoGXgZRSfu/r4iIyGFimiYT//U8S3+ciCP6Ry564I+k5MaEuyyRFiPgduPJy8MaE4M9K7hKtXv9etafehqW6Gi6z54VmhNtSwq+r/Nu3hy6vz0nh47v/w9nly4NFvuK6Pvr0yoWbl3I0/OeZsHWBQDEO+O5of8NXNjjQhzWlrv90YRlRfzpg0VU1vuIddn463kDOLlv+v7v2Nx2Lga2x3zmHf9WbARf/a+fw+qAuJxfBOfdrkcmaZi2AArVTcdbC33OgUXvwfY8mHg/THoEep8Fw66FnBH6pRMRkWZnGAZpnbqx9KdJmGYWK6ZtISW3R7jLEml2pt+PJy8P9/r1xJ54Yqi9+JFHKP/gQ5J/+xtSbr0VAHt2NlgsGA4H/vJybAkJACT/5iZSfn8r1t16rAyrlYj+/Q+4jvUV63lu3nNM2jgJAJfVxWW9L+OavtcQ42i5H3i5fX7+8s1K/j0tH4ABOfE8f/EgchIjw1hUdcPFv34ZnH9tMTAAwwKx2b8Ynr3bv9HpGn0qB0ShuqlExMMpf4XjH4ClH8Gc14IbtC95P3hJ7QPDroH+F4Kz5b5giohI61dfXU3ppg1k9wz2lA086RSy+wyiYJmHgSfkhrk6kcPPW7yV+iWLsSYkEDlkCACm2836004HIHLG9FBQdnbtiiUuDtMfCN3f4nDQfdpUrPHxDc67c7urxiipLeHFRS/yyZpP8Jt+LIaFs7qexW8H/Ja0qLRGn7c5FGyr5eZ357NkcwUA1x/ZiT+d1BOH7TAHTp8byjfuNiz7F/Oaa7ft/xy7Lwb2y+HZcdlg1ZQYOXSaU304bZ4Hc16HpR/uGl7iiIb+F8DQayG9b3jrExGRNmdr/nreu/9P2B1OTvvDs+T2Sg13SSKHjWmaVP/0E+61a0m4+BKs0cFFskpf+RclTz9N7KmnkvXU30LHrz/rbAyng8y//AXnjj2dTa8XbLbQEO+mVu2p5vWlr/P2irep89UBcEzOMdw2+Da6xIdvG6wD9fWSQu78cDFVbh/xkXb+dt4Aju99kB8CBALBXuO6cqgvh/qK/V+v2LxjMbD9RBVX/L6HZ8fnajEwOSQHmkMVqptD3XZY+B7MfR22rdnVnjMiGK57nwn2vS9gISIi8mtM06S+uoqImODfx4Dfz6u3Xo+nzoJpG8epvz2KzgNTwlylyKFz5+VR/cMPWOLiSDj//FD7miOPwldSQsf/vkfEwIFAcD/okmeeIeroo4LbVYWB1+/l/dXv889F/2S7O7h/c/+U/twx5A6GpA0JS00Ho97r59GvVvDWzA0ADM+N4bmzOpHhcAfDb/32HQG4IhiCf+26uxLMwL4e6teFFgPL3XN4dnwuuPbct1ukqShUt0SmCflTgkPDV34JAV+wPSIRBl0KQ68J7lEnIiJyAEoK8vn2hWcwLAaXPvZMqKetclspc78uYdWsIo6/sjfdhrXsoaUiv7Tt9X9Tt2gRKbfcjLNbNwAqJ3zH5t//Hlf//nR6/3+hYwsffJBAVTVJ112Lq1f493IOmAEm5E/g7/P/zqbqTQB0jO3I7wf/nuNyjztsPeL7ZZrgrdt/AK6voKaylLyNm7F6Kokzaki21uEI1B16DTZXsGfZFRecOrnP63EQk6nFwCTsDtvq35MnT+bJJ59k3rx5FBYW8sknn3DWji0F9uWnn37ijjvuYNmyZeTk5HDvvfdy1VVXHexDt36GAZ2OCl6qimD+WzDvDajcBNP/Ebx0OTbYe939ZLBqyruIiOxbVHwCZZs3gmFQUVxEfHoGALFJyRxzSSL9js7WSt/S4piBQGiV7Prly9n6zLNYoqPIfuaZ0DHVkyZRO3cuMccfFwrVrl49iT3lFFx9G06fy3jwwWarfX9mFs7kmXnPsHzbcgCSI5L5zYDfcE63c7BZmuB9XSAQ7PU90CHUv7zu9xzQw0QBfQF2TpnevZPZGRsMva74XQF4r9fj9wzMGpkpbdRB/3bX1NQwYMAArrnmGs4555z9Hp+Xl8epp57KTTfdxDvvvMMPP/zAddddR0ZGBieddFKjim4TYtLh6D/BEbfDmu9g7muw9gdYNyl4ic2CwVfC4CsgNiPc1YqISJhVlmxl9mcfEPD7OfHG4ArFkbFxnPGHe0jv2p3y4gArPlnLyLO6YBgGFqtFgVrCxvT78ZeVYUvZNfWg8MEHqfp2AukP3E/suHHBRsOgZsoUrHFxmKYZ6sWNv+B8oo8/DlefXQHakZtL1tNPNevzOFCrylbxzLxnmLZlGgCRtkiu7ns1V/S+gkj7Aa6OXV8R3Bu5YlNwu6fQ9c1QtSU4nbC+kv3OMd4fw7rXMOxzxDF1k5eZW/xUEEVKShpXHTuQxKSUXcHYGatOH5G9OOjfinHjxjFu5wvhAXj55Zfp1KkTTz0VfBHs1asXU6dO5ZlnnmnfoXonqw16nhK8lOXBvH/DgrehcjP89Bj8/ETwa0OvhU5Ha1l/EZF2qq6qkkUTv8FitTL6gsuITkgEoNOgodRUuPn8uRn4vAES0qPoOUofxkrzCLjdePI3YI2Pw54WnGZQv3o1+edfgCU2hu5TpoSONT1e/OXluNetD7U5OnUi/aHxODs3nP4Wd8YZzfMEDtGW6i08v+B5vlz/JSYmNsPGBT0u4Ib+N5AUkbTrQJ8nGIx3huQGoXnHZX/bP+3OFrGfXuJfue6I3mM49dqtVdz8zgJWFVdhGHDL2K7celw3bFa97xQ5EIf9o6YZM2Zw/PHHN2g76aSTuO222/Z5H7fbjdvtDt2urKw8XOW1LImd4ISHYOyfYfnnwd7rghmw4ovgJbFLcN71wEsgMjHc1YqIyGHi83pZOe1nzECAfscG99NN69yVEWdfSIf+A4mKT2hwfFSck5FndWHz6u10GfKL1b6rS4JDLrWdoxwC0+OhbukyvJs3E3f6aaH2wvvuo/LzL0i5/XaSb7wBAHtmJqbbTaA8gL+6JrQid9K115B4+WU4OnYM3d/icpFwwQXN+lyaQoW7gn8t/hfvrnwXb8ALwMlpI7g17Uhy3HUw+elgB8nOwFxVxAH1MEckBrd5isvZ8e+OS2xm8Gs7g7HN2WTP5aN5m7j306XUef0kRzt59sKBHNGt8VuHibRHhz1UFxUVkZbWcIGUtLQ0KisrqaurIyJiz2XuH3/8ccaPH3+4S2u5bE7of37wUrwsuGr4ov9B2Tr47s8w6WHocw4MuxayhmjxBhGRNmbd3JlMeOlZIuPi6XXEMdgcDgCOuOjy0DEBfwCfN4DDFfxTPuC4HPqPzcaoLYXVUyBvcnBxzG1rwbBAWh/IGQm5I4O7T8TnhOW5SctXv3IltbNn4+jShegxYwAI1Nay4ZJLAIg5diyWqGBQdnbujCUmJrgt1Q7W6Gi6TPwOe2YmhtUaand2afnbR+2Vty7Yu1y5ifrtebyzaRKvlS+hCj8Aw+s93L6tjL55HwAf7Ps8VmfDoNzgkhOc+uc4wKHiTaDW4+P+z5bx4bzgYmqjuyTx7EUDSY3RvGeRg9UiJ0Xcfffd3HHHHaHblZWV5OS00z/+aX3g1Kfg+AdhyQfBfa+Ll8Cid4OX9P7BcN3vfHBEhbtaEZEWw71uHZ6CApydO+Po0AEAb/FWSp55BlevnsSffz6WyOZ7A/trtm0qwFNfR0bXHgB0HTaKjO496Tp0JOZetqHx1Pv47tVl+H0BTruuA9ZNMyBvMkbeZNi6/BdHG8GtbIqWBC9z/hVsjs2G3BE7gvYISOsLFusejyVtl+n1su2113CvW0/Gww9hcQXDVNXE7yl94QXizjs3FKqt8fE4e/fCFp+Av6oqFKqTrr2WpBtv3GNFa0dred8WCEDN1t3mMW/ec05zbSl+4PPoKF5IiKPYFnz73M3j4fayco6oq8cAiE5rGJLjsoNBeeftqOQW0xGyqqiKm9+dz9qt1VgM+P1x3bnl2K5YLS2jPpHW5rCH6vT0dIqLixu0FRcXExsbu9deagCn04nT2XTDWtoEZ0xw6PeQq2HTnOC2XMs+gaLF8MXv4bv7oP+FwYCdGv7tJEREDoVpmph1dfirq7Gn7hrOXPXTT7hXrCBq9GgiBgwAwLNxIwXXXYeBQZcJ34aOLX35n1R+8QWpf/oTSddeE2qv+PRTKr92EH/xxaG22nnzML0+Igb0x7KPv02Hy/LJk/jmhadJ79qdSx55CsMwsNpsXPLw3/Z+B3cVVfNnsmWliek3KX38etLsaxoek9YPOh0Z3G2iw2jw1MLGmVAwK/hv4eLgzhNLN8HSj4L3cURD9rBdPdnZQzVkvA2pnjKV7f/9L64+vUn57W+DjTYb217/N4HKyuB2VD2CH+pEDBxAzAnHE9G/f4NzdP744z3Oa9jth732Q+Ku2ntQrty8K0QHvPu8uwlMiXDxTFIia+3Bt83phpPfJQ3l1Mwjscbn7hqe3YRDsg8X0zR5f+5GHvh8GfXeAKkxTp67aBCjuiTt/84isk+HPVSPGjWKr7/+ukHbxIkTGTVq1OF+6LD4/vvvsVgsDBo0iISEhP3f4WAZBuQMD15OfhwWvhMcHl62Ptj7MOdfkDs6GK57nd4qXuBFpH0I1NZSv2IFpttN1OjRofayt96mdvYs4s87j+ijjwbAs3Yt608/A2t8PN1nzggdW/n111R+/gWGwxEK1YbTiXdDAVgsDbbqcXTqiKtPH6zxcaH72xLiSb7lFsz6Oiw7hlRDMIDXTJlC2j33kHhFcIh1wOPBrK3FGh/fpN8Hn8eDp66WyLjgeTsOGIzN6SQmMRmf243d9Yuhl9462Dg7OJw7bzJsmU9SwMfJMQNxGDXBQJ3cfdeWjR2OgKhfvEF2xUGfs4MXAHc1bJ4HG2dBwczgh7XuSlj/Y/ACO4aM990VsnNHQVxWk34v5NCYpolZW4vp92Pdbf/UjTffQv2SJeT++/XQkGt/2Taqf/iBQFUV7AjVhmGQeOUVGDZ7g5/z6COPJPrII5v1uTSK3wdVhb8Iybsv/rUxuKL2/hiW4J7IvxiSvdgS4JktPzB3+woAYh2xXN/vei7udTFOa+t7f1Xt9nHvJ0v4dOEWAI7slswzFw4kObr1PReRluagQ3V1dTVr164N3c7Ly2PhwoUkJiaSm5vL3XffzebNm/nPf/4DwE033cTzzz/P//3f/3HNNdcwadIk3n//fb766qumexYtRCAQYM6cObjdbnr27BkK1aWlpZSVlZGbm4vrl2+WDpBpmuD1gtUamp8UMJ1400/FOP00HIG8YO/1qm+onTcX3/T5RGTfif2Iy2HI1Xjr7FR+OwFrbAzx550XOu+2N97AvXYtCRdcEPpE2r1mDUWPPIotKanB1hVb7r6H6qlTSLvzLuJOOxUAf0UF+Zdeiqt3bzIff7zB3Kk2xTSheitsWxN8c5raW8Mkpd0xTZNAdTWGzRbqzfWVlVH51ddgmqEwClD8+ONU/TCJlFt/F1rF17t5MxsuvWyPoFy3ZDFVE78nYuDAUKi2xAWDcKCurkFQjhoxAovTibN799D9bYmJdHj7rT3Cb8pvf7urR24Hw+Eg5Zab93hu9vR0bKmpRA4dEmqrnTWbjddfT9TRR5H7z38e9Pdrb9bMms7EV1+g08AhjLs5OM0pMi6eG158g4joHb3CPk8w8ObvmBe9cTb43Wxy9yPGWkKczQcJHcnt2C+4K0SnI4PbNB4MZzR0Pjp4AQj4YeuK4OKYG2cFe7QrCoKjoYoWw+xXgsfF5ewI2DuCdlofvRYeAs/GjfgrKnF07BhazMu9fj1VP/yALSmZ+HPODh275Z4/4167lvT77iOiX3Cbqarvv2fz724lauQIcl/4W/D/0fTj21yAb+tW3Ium44wNtkXkxpD2u6twdc6GjXPA9EPAT8op/YL3q1wCFQtD5yDgh4AvOHVg97bQ137Z5gsOpd7vcftoO6jjA8Htpaq2BOvbH1f83ucw7xyeHZPRYJuoDZUbeG7+c0zcMBEAh8XBpb0v5dq+1xLnjNvHg7Rsy7dUcsu781lfWoPVYnDHCd35zdFdsGi4t0iTOOhQPXfuXMaOHRu6vXPu85VXXskbb7xBYWEhBQUFoa936tSJr776ittvv53nnnuO7OxsXn311Ta5nVYgEODI5BS2VJSTvNtwqHkTJzJj1Sp6xMRw8R/+EGrPv/dejM2bSb/nHpzdugHBoY3FjzyKq19fsp95JnRs3lln4161itzXXwv18NTMms2m3/4WV//+dHr/f9DlWKjcwtYLL6Qur5TsI8qwT30Gpj6Lxz6KrW/l4+jSpUGorv75Z2pnzCRqxIhQqA7U1VE7axb2zMwGz89fWYm/pJRATU2orX7FCjxr12HWuxsE6q1PPY138yYSLrucyMGDmuLb2zwCASjfAKWroWQVlK6CktXBf3f/tNsVF+y16TAaOoyBjAFgbeFD4KTd2/nhnLFbD61nwwa8xcU4OnTEnhYcZu3ZtJnSF1/EcNjJePDB0LGbfnsz1T/+SMYjD4deR/zbtlH86KNY4+IahGp/eTneTZvwbd0aarMmJmLvkIstIbHBfrRxZ5xJxMCBRA4eHDrWlpxM97lzsURFNpirGX/uucSfe26D52XYbEQOHXpI35uMhx8Kfn92497xAbLtFyttF1xzLdakJFJv+z32rP333O7+oUBMUjJ1lRVsWb0Cv8+L1WYHv4+IitWwcDLkTQkGW29tg3OsN05mQvn1xMYGOPc3XXFldd7bQzWexQrpfYOX4dcH2yq3BHuxd/ZmFy3Z0RO4EZZ+GDzGERMcJp47KjgvO2toMLC3MaZpYrrdGHZ76G+dt6iI+hUrsMbFNfjZLfn73/EWFZP829/iyA7+fFROnEjxY48TMXBAg7/tG6+/AU9+Ph3efiv0M+xeu5aSp54mYtCgBqHavWol9cuW45v/BZRPgG3rsM5bCoBnyTR4ctciYGmZDoxsE+f838Hi4M+1A0gEKAFmHYZvUrhY7MERFDsX+vplaI7LOuBpDKV1pby86GU+Wv0RPtOHgcEZXc7glkG3kB51kB9ctRCmafLOrAIe+nI5Hl+A9FgX/7hkEMM6ahcZkaZ00KH6mGOO2eONx+7eeOONvd5nwYIFB/tQrY7NZiPtnXdIKirCf/I4SEkJtldVEV1VRXJZWejYmpoa3rRaiY2N45rS0lCoNj0evJs2YUttuCWKsWNRjN1X17RERmCJi8MStdtCO7GZOIcdC3GrsJw4Aup/hvU/Yts2i9gOMdhj1sPkJ2HQFRCTRvzZZxM1YgTO7j1Cp3Dk5pL51N+wRjd8Y5T2f38i5ZabsWfs2v/U1acP2S+/hFlf3+DY6p9+xL1mLbGnnx5qq1+1mtLn/0HksOEN3nyHhc8TXE29ZFXDAF26Fnx1+7iTAfG5ULstGLBXfxu8ANgjg3MRO4wJBu3soWBv3nmZ0jaY3uA+rmbADIVcCM759W3dSsTAgaHfQc/GjWz/73+xxsSQfNNNoWOLn/grtfPmkXLLzUQfdRQAdUuWkH/hRdgzMuj6w/cNjq2eNIn0hx8i4fzzgzXU11Hx8cdY4+IahOqdw0v9Fbu2ObQmJxNz0knYkhoG5aTrriP+wotwdMgNHWtLSqLrhAl7POfoI8YAYxq0GRZLqOeuufxyoaWkq68i7ozTG7y++crKqJk+HYC0e+4OtVdPmYp79Wqijz4KZ9euAGxauYwZH7xLbt8BjDg7uGVQetfunHv3eHJTrFjm/CvYE71hWnDo9e4ik3fNie54FGnWbCL/Oo+ULnHYUjscjqe/p9hM6HtO8AI7hozP3TUve+Oc4L66DYaM7wjnOxc/yxkZ9iHjgZoafGVlGA5HaB9lgPKPPsZfUUH8uedg3TEyouqnn9j+zrtEDBjQYETD2qOPwbd1K50++wxXj+AoiZpp0yj8871EH300kf98OXRsxVdf4d1QQPy554RCNQETX2Ehvl98WG1LTQ2NxgDA58ERA3HHDsORYIUvbguu3l62ntSUEgJHGkSs+CvkBY+PNKDHeQaGdef7MgMsViIzDLDYwBIRHNpssQb/b3b/t6nbDMuOx9xfmxUsO9r32/Yr53XFBUNzVGrwvoeg1lvLm8ve5I1lb1DrC36gdWTWkdw25Da6J3Tfz71brqp6L3d9vISvFhcCcGzPVP52/gASoxz7uaeIHKwWufp3axZz7Fj8FZVYY3YF0jFHHUW/ujrsubveXBYWFmIaBtaMdKJ2BGqAWV4v1XfdyZC+fRucN/e1V8FiabBSbdTIkfSYNXOPGjIe2n07stth2zqcc18nK+0dqNsMkx6Bn/4CvU4nbui1cPrpDVajtMbHE3fqqXucd+fqubuzxsQQc8wxe7Sn3XMPdUuXNljkpG7hQqomfk+gprbhMNEnn8TiiiD+/POwpzfxJ8Hu6mBo3hmcd4bnsrzgELK9sTogqWtwjmJKj13/JnUNBmW/L7gC+4bpOy7TgsPQ8n4OXiD4yXnWkF092TnDwRW798eTVmHnh4k7g5e/shJPwUYMhx3XbkORK7/5Bm9xMbEnnhga7VG3dBll//439qwsUu+4PXTspt/9jrrFS8h8/LEGI1A2Xncdzp496fzpJ6FjS559jto5c8h65ulQqPaVllL22uvYc3IahGpPXh71ixfj3W2RSMPphECAQF3DD43s6ek4OnVqML/Ylp5Oym23YU1s2EObdt+9pI9/MLRCMIAtIYHs557d4/vl3O11rTWzJTWcm2yJjib39ddwr12Lbbd1Myo+/5zKL74gUFNDyq2/C7YVFVKwdBFlmzcybHR/LBumQP4UOuZNgbqyBufFFRecC71zXnRqrwavy1HAeXcOJTLWgRGu4ZrOaOh8TPACO4aMLw/2Yu/s0a7YCIWLgpfZO4bMx+XuCNg7ho030fQZ0zQx6+tDUxFMn4/Nf/gjnoICOvznTawxwd7J7e9/wNYnniD2tNPI+tuToftvfeop/GVlRB0xJhSq/aWl1EyZssdUJmPHz3ygpjrUZktLx9WvH46ODf82Jl5+BYHa2gZ/z6JGDKfjB+9jTUgIft8qNsG2tXS4eQyUpcPqJ2DGOigvwGX6ydz5edq8XeeNSgciEoJ/ixK7QFJXjKTOGEldIaFTcKG5QwyX7Y034OWj1R/x0qKXKKsP/k72TerLHUPvYFj6sDBXd2iWbKrglvfms2FbLTaLwf+d3IPrjuis4d4ih4lCdRNLv//+PdpcPXvi6tmzQVvXrl354x//SFVVFbbk5FD70tWrqaiooP9uC4SUlZVRUFBAx44dibc14r8sqQuc9Cgcey8s+xTmvhZclGbZJ8FLco/gyuIDLoKI+IM//15EjRpF1C8Wo4scNpTUO+9s0Ptm+v1sf+ddzPp6Yk89JdReM3s2NVOnETV6NFEjR+z/AWu27RiqvXLXcO2S1cHVbffFEf2L4NwzeD2+Q4O5VXuw2iBzUPAy6ubgkPHSVcFwvWE65E+D6qIdPTkzYerTwU/V0/vv6snOHbXnQkISdtWTvqfs36/i7NqVtPvGh974rz/jTNxr1tDhnXdC0xlqpk1j8+13EDl0KB3efit0jtJX/oV7xQqcXbqGQrW/bBuVX32Fq3dv2C1U+8q24ysuxl9ZFWqzRLiCYeoXI4KcvYKvIZbdFiOyZ2SQePXV2JIb/iwl3XAD8Rec3+B1x9mpE10n/7zHytbp99+3x/fBGh1N8k037rW9vbM4HMHXpd0WWgMwBvQjr2QTpCWRArA9nw5b59C1uIwO6/IxXhwBO97LBvxgcUUFXwt29kan928QNN21Xr57bTlDTu5AZrd4AKLiW9hiQhYrpPcLXnYOGa/YvGuV8YIZULw0ODd7SUFwW0gAZ+yuIeM7Vxn/lS0hvcVb8WzIJ2LAACw7dgbZ/r/3Kf7LX4g96SQy//I4EBzRVTtvHv7SUjwbCojo2ydYZnQURkQE/CJMxBx/PIG6uga/E5FDh5Lx2GM4crIbHNvxvXcxnM4GH2xHHzFmxyiLhhIvuzT4+1tVBPlTYdtarNvWEbFtXXCEVNl68Hv2/X21RwX/bid1aRCgSeoCkRqy2xRM02Tihon8fcHf2VC5AYDcmFxuHXwrJ3Y4cY9RK62JaZq8OT2fx75eiccfICs+gn9cMojBuYdh8VwRCTHMXxvL3UJUVlYSFxdHRUUFsbFtt7fPNE1WrVpFfn4+Y8eODW0rNmPGDCZMmEC3bt249NJLQ8fv/H406sW/cHEwXC/+ALw75kjbIqDfecGVwzObZx50wO2m/P0PcK9eRfqDD4Z6B4qffJKy114n/uKLyHjgASA4L7HkyUdxJtuJ6RqJpXLdrgBdu23fDxKVEvzgIKV7w39jMw/PfpGmCdvzGvZkb8/f87iUXjt6sndcYjP3PEYOj0CA4ofvpXbOXDIvHYrTtgWKl1O5YCObp8UTmeqmw/EVwf+TuBzWv1mMu6iW3LsuIGrMkRCXTfXyzRQ+8DAR/fuT/Y+/h0699amn8RYWknj1VUT0Cb6p927eTOXEidjT0ogdNy50bP3q1ZgeL47cnNDQ6l/2iEvrMO0/LzLzq6/JTHZycdfVUFFA9RYnhXPjcMX7yBlbE+yt7XQU+c9NwV/nI+ORR/e55sS0D9ew8PuNxCS6uPShkVhtrbQH0l0Fm+Y2XGXcU93wGCMYzj1R/agpjcHaoT+xZ+/a7mz16DH4y8ro9PFHwQ+mCK4Cv/mOPxAxZAgd33k7dGzl119jREQQOXRoqKd692kJTa62DLat2zFEe8e/23YE518+z91ZHZDYeUdg3i1AJ3UN7nWs3//DZm7RXJ6Z9wyLSxcDkOhK5KYBN3Fe9/OwW1r32igVdV7u/HAx3y4rAuCE3mn87bwBxEW27uclEk4HmkMVqluBRYsWMWfOHHr37s3oHb0jXq+Xv/zlL0RERHDTTTcR3dgepPpKWPy/4LZcW5fvas8cHOy97nsuOCL3ff/Dwe+j6qv3qf7+O6K7xRKT5YbSVXjz17D241gwTHqcW4hlR2dydaETX72FqC7J2Dv3+EWA7tEyPtmv2BzstdnZm12ycs9jEjrt6snuMBoSOuqN1aEyTeoXzmTbP1/E8NeSeWp68Oe8ZCX530RQV+okc8R24joFh0T76ixUb43DEVVHZPKuebTeWguGAVZnAGP3bBOR8IsFcX7xb3SahmO2QWYgwPrpP5Dg20Ri1SLIn0JN8Qa+2tyTQQlb6BqzDcNqCy7a1ekoAhkjsHQ9AuwuTI+HVUOHYXo8dJnwbWhaTfXkyVR8+hkxJ55A7Mkn4/X4+f7fyxl2akeSs9vQXtEBPxQvo+Qfz+JeuYqUvttxWjYDUJEfwZaZCUSmuOlwblRoXnb+k1/j215N5mOPEjksOCTXX1mJb9s2HFlZDRbeOyzc1bsF5vU7/t0Rouu27/t+hiU48mn3wJzYOfhvXLZWTW9ma7av4bn5z/HzpuA0rQhbBFf1uYor+1xJlL151284HBZuLOeWd+ezaXsddqvB3eN6cfWYjvqAVuQQKVS3cYWFhbz66qtERETwhz/8IfSi+dNPP1FUVMSIESPo1KnTgZ/QNIO9CHNfg+Wf7Rqa5oqDAZcEA3ZKEy/W4a2D0jU75juv3LVo2LZ1EPDueXiNlW0rY/BbYsm6ZEAoNG/6+5dUTZ1D6p13knT1VQD4q2uo/OZrXL17h3oLW5Sa0h0he0dvdtHiPbcFicncrSd7TPADAv1x3Lf6Sra/+TLVk6eQMDSZ6IQSKF5G/ZZK8iakYrEH6H5OUehbWFUYjRmVSeSAvti6DAhuDZTaO9g7HfAHh/Dv3Ou0vOAXe59uAvcB7H26+6q0e6xImxP82q8Me5UWpG57cGpH/hQmTZzLgs0R9Ikr4uTMNcGvGxbIGLhrOHfOyH2ugu0vL6du0SKijjoq9Npd9OhjbP7wW7LOOZH0++4Fgj2s2996C1ffvkT07x9asLKlMgMBfEVFWOPisEQFf65rZs2m+JGHsWVkkPvKK6Fj8y64kPrFi8n6+3PEjugNBTOpnzWRrR/MICJmOyl9dy3cZppguGKDa1PsXAAta0jT/u743MG1Nn7Z47xtXfC14NfEZu0KyzuHaSd1DQZqmxaECreimiJeWPgCn6/7nIAZwGpYOa/7edw04CaSI5L3f4IWzjRNXpuax1++WYkvYJKTGMHzFw9mQE58uEsTaRMUqtsBr9dLeXk5KTtWGQd4+eWXKSoq4txzz6Vfv35A8Pu3ZMkSOnbsSNYBbP9CTSkseAvm/ju4vdROHY8MDg3vedrBbR9Vt323ec67rbZdXgDs48fPHgnJ3fbsdU7svMdjl778T2qmTiXljttD25rUzJpNwZVXYs/MpOukH3Y9tRkzsERE4OzVKzQ3r0WorwzuRbuzJ3vzvD0/WIhM2m0br9GQ1u/X5363VT4PvrVz2fba6/i2bCTr1HgoXg4VBWyZFU9FXiTJfapI6RecpxzwG5RtyMHVrSNRo0dgpPcNhufEzof2/auv2C1kb9wRvjfuajvQ/VMjk34Rtnde33E7KkW93eHgrgp+0Jj3M1UrpuIoXYLT6gNgS20MH2/sy6AcL2OOHR4M0bmjDmlNivnvzWbmz1WMOdLFgEuD83Q9Gzey7oQTwW6nx5zZoQXivJs3Y4mLC9scd391NfWLFxOoryfm2GND7fkXXUzdwoVkP/8PYo4/HgguUJl/0cXY0tPp9tOPoWPLP/2UQFU10UcfhWO3RTyBHUPG5+xaZXzT3L0PGc/o33CV8dgMfpXfF5zfvTMs797jXL6Rff49guBq7KHA3GXXPOfEzs0/mksOSKWnkleXvMq7K97F7XcDcEKHE7h10K10jOsY3uKaSHmthz9+sIjvVwS3LjylXzp/Obc/sS4N9xZpKgrVYWAGzPCtyrrDxo0byc/PZ9CgQaEh4QsXLuTTTz8lKyuL66+/PnRsSUkJCQkJ2PbV+xEIwLofYM5rsGbCroAQnQaDLochV0F8TrBt56Isu+/rvDNAVxfv/fwQHD67Mzin9Nx1PTb7kIJE7Zw5lLz4IvaMTDIfezTUvv6MM3GvXk32iy+E3gz6SkrwbNiAs2evZt/CZ588tcFgvXNO9sbZe2715YgJvpnc2ZOdOQhsLeiDgkO1Y8/wmomfUjlpMpGpPuLSt8K2tfjdflZ/FHwD3e3sQmzO4MtYdUUGHl8akYP74Ro4CtJ6B3+mwvGm1++DqsJdgbtit8BdvmOv31+bc7mT9f/Zu+v4OMr8geOfWctuZOPubeou1IsWivvhVg4ODpfjcIpz6AEHdxwcesABP4pL0VLqLXW3NGnjno2sz/z+2GSSbVIljfX77mtfu3n2mZlns7vpfOf7iCWQCYtK30PgnSrLt3UErzPwPdvxa+BWtAJUHwvKM1hakc7k+HzGDQwNXFzMPhJv8hGYYzpuqahFn25nxXf5DJmawtEXByaYc2/fTvnzL6D5/aT/82W97q4/X0f93LkkP/YYUWedCRy6ccOO2d/hXLkS+ykn66s5NK5YQf5FF7e5aFl42+04vv+epAfuJ/q8wBJiakMDjStWYsnMaBs87y+/LzDhWfO47F1LwFHYtl5UZmB28fTxgeEz1XmBsc3NWefqvHZ7QelC7MEBc+sAuoMm8RSHntvv5oNNH/DqmldxeAI9HkYnjOa2sbcxIn5EF7eu4yzPr+LG91dSVOvCYjRw/6mDuGRCpnT3FqKDSVDdBYr/thTV6cMQZsYQasIQGrg3Nt0Hl7c8ViyGQ/pHcNu2bSxbtozU1FSObFqzVlVVnn76abxeL3/6059I2G1d7DZqdsGKt2HFOy1BsmIInGB6GgLduPfWHdae2naJqrgBEBbXaV2aNU2j4Lrrca5ZQ/asj/XlTqr/7/8ouf8BQidOIPPNN/X6znXrsaSn6UutdCmfJ7BETXMme+fitr9vk7VpreymTHbaET2na3F9GVrxWqre+wD3lm0kTQFD9WbwNlCxPpzytXbsmY2kTqwJ1A+xU74lBVNqOvbjj8aYOSqwBFF3GD+/vzStVbZ7D4F3XTF7zZ41C41rCrr3kPHuxO9Zj+HzBC5c5c0LBNG7loLfjaoFJulWFCAqk/XaaGYvrmDguHGccnvb1R06iqZq5K4up8/I+H3+f7Dj3D/gWreOrI8+1APdhsWLKZ45E/sJJ5Bw++37Pp7Ph+b36z12PLt2UfroY2heDxlvvKHXK7jlVupmzybhrjuJveIKILCUW/5ll2PJzibtHy+iNF0A9dfVYbDZOqebes2uliB752IoW79/PUNM1qaguU/bmbXD4uV70oOpmsrXuV/zj5X/oLghsC5zTlQOt4y+hSPTjuw1waaqarw6L5env9uMX9XIig3lpYtGMzS1G5yrCNEL7W8cehj2HT101AYvmlfF7/bjr9p3fZ1RwRBqxhhmCgq2gx/vFpTbTPudFc/JySEnJyeorK6uDkVRUBSF2FbrsM6fP58tW7Ywbtw4hrZeKzsqPbAk11F3wqavAtnrvHkt6zJDIMiOzt5tiar+gcchXT/RjqIopP/rn7S5juTzYUpK0meVhcBSX/mXXYbW2Eifb74hpE9gfLqvogK1vh5jXFzndr00WSD9iMBtyi36ZD96Jjt/ITRWBN6TvHmBbQxNS3/pa2WP7/psi7sOyjbhXjkXx5z5mNRKolOLAm3XoPKzRPxuI9Gx5dhivWC0EDY4DTXaTujoYXDsiYHssz2V+J5+gqQogffDFgVJQ9uv4/eCo6hVN/PdxnbX7ArM3t9YEbgVrWx/PyZr4MJWZFr7GW9bdCAgUf2Be83f6rHa6jn/bvU68zltt5/bea51+zU10Nuhzevxg7Mm0L3Y2xj0a1rr6s+SshSOmj6RfidcCNGZDPB6iTsjn8Q+Oe3/bg+Ss97DurmFjD0pC8WgoBgU+o7ax8XNJtkf/x/e0tKgNbQbf1uON38n3qLioLqlf3sSS1YmUeef3zJ+++FHqP7oIxL/egcxl10GBNYxr587FwwGNI9Hn/wr4thjMCclYWv1/4EpLo6+33zdpl3Ns213iqj0wG3YuYGfXY7Ae9ocaNeVBLLVsTnBAbQ9VYZT9DKaprGwaCF/X/53NldvBiAhNIEbRt7A6X1Px9iLJoSrrHdz+/+t5pfN5QCcPiKFx88eRniInM4L0dUkU92B/A1e1EYvaqMPtaHpvrH1feCxv9Vz+A/y16+AwbZ7EN50305w3hywK62WZdE0jdraWqKiovSyd955h9zcXE4++WTGjRsHgNPpZN68eWRlZdGvX7+Wq73lW2D7zxCRGMg6x/bt0d2PNZ9Pz7B4S8vIv+QSfBUVDPhtmb7UV9mzz1L52n+IveYaEm69BQDV5SLv/Asw2u2k/+c1PfPTsHQp7s1bsI0YrmeTNE3DV16O0W7Xx0d2TOO1QG+B5gA7f0E73SOVQPCmr5U9CcLj293d7+bzQOVWKNtI3fff4Vy/maisKizqLgAcO60ULozBGuMh+4SKQNtisinfGA1h8USeMh3L0MmBk+DDcdz4/tK0wJwFQZOo7RZ415WwX9nuw1ForN6dm+yjmPf9fJZ+/jF9Rh/BWXfOPGSH9ftVPnx0GdXFDRxxShbjTuvz+/fpcOBcuRJjZCS2kSMDZbW1bJkwEYxGBq5cgWIOjLNs/jsWfcklJN13LxD421Tz0f9hyUgndOxYva4Q3dn6yvX8/be/s6RkCQAR5gj+OOyPXDzoYqymDvw/thtYklvJTR+spNThJsRk4MHTh3DBEem9JgMvRHclmeouYAwzYwzb/xMRTdPQPGrbwLuhJRj3N+4WnDd40dx+0Ggq9x1QGxWLoU0gXh1aoWfBj+szkQFx2WSHp+KrdGIIM7Nz504WLlzIpk2b6N+/ZQbwYl8EEUMvITQ0FEPTlX+v14vH48FoNGJtFTQ6HA78fj8RERH6GG6n00ldXR0WiyUosC8pKcHn8xEfH6+v1V1fX095eTkhISGkpLSs55yXl4fL5SI9PZ2wptlmHQ4H+fn52Gy2oAz95s2bqa+vp0+fPkRHRwOBtb43btyI1WplZNOJKMCmslKq77+P/hkZekBdW1vLwpoaGDaMqa2+VEXbt7Orqgr7rl1kNGV3NE3D8d331Lz3HrF/+lNLUO12s+3IowDo/9syPdtd8/HHOH74Afv0E4k6+yx9H7WffobRHkHYkUdiaN63zwdGY/B/pIrSNC69P4ydEQi2anYGZ7KrtkPJ2sBtySuB7eL6t2SyMycFMpcHQlUDQVzpBnzbllM3fzFaVRExaTtBDXw2K3+Mw1lhIUStxpIFhCdhHdkXu8eHbdhguOSyQK8GSyiHKMTvvRQl0OU9NCYwaVN7fJ7ABZY9Bd41u4LH6yvGwFI/iiHwWDEEMnv64/18zmDYrV7zc+3t4yCea13e7nPKbvto9dhoodiXwG8LVzNx4oXEpQeWtRp5YhQRcQkMOfLY9n+XHcRoNDB6egZLv9xBzpjEjtmn3U74UUcFF2oa8TffjHPVKvy1tZjiAjMdR196KdEXXYQpseXYiqIQff55HdIWIQ4Vt99NlbOK0sZS3t/4Pt/mfQuA2WDmwoEXcvWwq4myRnVtIw+Sz69SWuemuMZJYY2T4loXxTVOimpdFNc62VDkQNWgT3wYL180mkHJ3TfJJMThSILqLqQoCkqIEUOIEaL3fzvNp6I6ffsVgOuPnV5QQfOo+D1u/DXuPe4/FXD/mksJuQA4jXUMsKQRXm+j7JXVejD+4dYvqXHXcdUxFxKXnIAhwsKKrauZ/fP3DB48mPPOazlBe+2116irq+Oaa64hOTkwwdSmTZv4/PPP6devHxdffLFe96OPPqKqqoorr7ySjKaJbfLy8vj444/JzMxkxowZet1vv/2W0tJSLr30Uvr27QsEgvJZs2aRkpISFFTPmzePgoICzj//fD2orqqqYvbs2cTHxwcF1StXriQ3N5eos88mqamsrq6OVUYjkZMmcsaVLW2Yv3o1m447luP69NED3YqKCl71+wg77w/8afCglrpz57Jp6hT67shjYNNFAJfLxdING/Du3MUR+S2zrdeWl7P18ccIcbkZsngRNAXVFf/8FxX//jcxl19O4l/vCHwmNI2iO/6KISyMhL/cHuiGGZ2Jp1bD25iJefSdWKIssHNhyzJepesCE8lVbIHlbwUOGpURnMmO7dsyxrC+PLDGc9kGnMsX4Vy3GVtoEbaIwEQw3iozJd/HY7SoRJ/lCyyBkzCIiImhhNSHYDn9ZDjqNAiNwdL0OROdwGSBmOzArT2aFuga3RyIHiaWPvMY25YtIiQ0lBOuuQmAiJg4Rp5w8iE7pupXMRgDFyAHTkim7+gEzJZD1zXVGBVF3LXXtCk372sODSE6iV/1U+uppcpZRZUrcKt0VbY8drY8rnJV0eBtCNpeQeGUPqdww6gbSA3vvv+rqKpGRYOb4ppAgFxU46KoKXAuqnVSXOOirM6Fuo9ORWePSuWRM4cSJt29heh25FvZAykmA8YIC8aI/V//UlM1NLc/OABvt4t6cNd1zasS549gqnMAOMHjaAqg8IFFBQPUzN4BWmA5B4exAMzQuLGS0hdWYIiwYAw3Y/CAyWDCtaUaV6MNY4QFk2ogNDRUz0Y3s9vtqKqK0dhysmm1WomPjw/KaAMkJiZiMpmwWFp+F6GhoWRlZREXF7z+ZGZmJmFhYfqs6ADh4eEMHTq0TXeOnJwcoqKiiImJCao7fvx4bDZbUJY4IjKS2NhY4o44Qi/zeDxogCEqCvuJJ+rlxTU1FKWmMuKyy/R91NfXs1BVsUycwLHTjtPrzpk7l9WnnsqY8nKGNQXg9fX1vFVVifH447m41WRAa5YvZ21xESmFRSTcEQi0/X4/yz/+GOdXXzPwmKNJmTkThpyFp98pbD9uGibTaLIevxZz3VrIX0j98g00rKwhNP5TItL+F9hxWAL1jlQ8O4uIyW6Zxb16cRS1eaHEDVWxjbBA3ABCBg4grCCfkAH90a6/HSW+DygKLaM+RbekKIdlF/uxp55FSGgoI6efesiPpWkaq37cxfYVZZxx6yg9kD6UAbUQXUHTNJw+Z0tg7NwtUN7t5xp3Der+TDDXitlgJsYaw6DYQVw/8noGxgw8RK9m/2iaRq3TS1FzwNyUYS6udTVlnJ2U1rrx+Pf9Os1GhaRIK8mRNlIirSRHNd1H2siKCyMnoWuW0RNC7JuMqRZ7pXn9+Pc4RtyHp84FDT60eh/+eg/+Bo8+fFNhP7JeRgVjhEUPvo0RFgxN963LDREWDD3oBNTv99PY2Ijf7w+6ELBjxw4qKyvJyMjQZ1yvqanhxx9/xGg0ctZZZ+l1v/jiC9atW8e0adP08e2VlZX84x//wGI289frrsPUlHH/bNYsVq1dy3hbKCf+9Q4URaG+vp5nnnkGgBtSU4lrWk7t2y+/ZMny5QzasIFz33gDY3g4Pp+P1554HAqLONVfTPokLxT8xnZfIqvW9SOuvIIpk9diTMqExCFs2RaOd1sNqdOPJ+q8yw9s3fIupGkamldFc/pQXb5Ajw+Xv83P+0wXAPv8eO9X0nc/Ku2jSkccRrEYMUZbMUWHYIy2YoywdPnygB1J0zSWf/UpZquNEcef1OnHb3R4eP+hxbgbfBxz6UAGT07Z90ZCdBNe1UuNqyYQCDuDM8n6rVWm2eV3HfAxokKiiLHGBN1ibbGBe2ssMbaW8nBzeKeOI270+Fpllp168Fxc25JtbvT497kfRYGEiJBAwBxlJSXS1hI0N93HhYdg6EV/e4XoDWRMtegQitmIKdIIkfs3AZnm11AbvPjrPah1Hvx1rR97UOu9+JvKNZcP/Br+mkB39L2sHhpoi8WIMcIcFGgbw5uD7+aAPPBc6wnZuoLRaCSinZlws7Ozyc4O7oYbFRXFueee26bu6aefzumnnx5UZrfbufrqq/H7/XpADTBo6FAiY2Lo06oLuqZpZGZmoqoqcX/8o17X4w/85x937rkYmjLgbrebUr8KSUnYJ50NJ5wAXhdbZr3LWqWA4Y0O1D+/iTGjLz6fj/cffRSykrnzzIv1gHrhwoUsW7aMUaNG6Uu3QaCLvtlsZsqUKfo4++rqahwOB3a7Xe+Kvz/0oLhVAKw6fa1+9qE6dw+Sfa1+3s+A+XBnUDBGhWCKCgkKtk3RIRijrBgjQ1CMPefEb8fK35j77hsoBgNpg4YQm3aQ6yUfpFC7hZP/PJzy/DoGTUru1GMLsTtN03B4HO0Gxe0FzLV7Wy5zD6xGqx4U7x4k714WFRKFydA1p6Nun5/SWnegC/bu3bKb7mud+zo7CYgJs5DcnGWOarlPibKRHGkl0W7FbJSZ54XorSSoFh1KMSoY7RaM9n13Tde8aiDg1gPtpuC73qvfN5dpXhXN48dX6YfKfV8FN4SaAgF2hLnpPvhxczbcEGbuURk5s9lMamrbcWMDBgxgwIABQWURERFB48+bnXb66Uw/8UQMhpb10S0WCxdccAFer5fIYcOaDmYlfegEPLbt9OvXD3NGYMy61+slIiICt9sd1HW/rq6O6upq3O6W8fo+n48lSwKzsk6aNCmwnJlPZdXSFcxdNI+R/YcxffQxelD80k9vY1AMXND3RML8FlSXjx21BWxvLCTZH02OJ1GfMT/PUI4RhSQ1GjOBXgwqKqBg2Fdq1qBgsBkxWE0oNhMGqykwm77VhGI17vUzsdeQfJ/x+l4q7Gvb33MtYB8dklSnD3+NG1+1C3+tG1QNf5ULf5ULaOeE2gBGewjG6BBMUdbAfXSr+8iQLr+w1Vr2qLEMOWoa8ZnZxKSmd8oxG2rduBt9xCQHLlyl5ESRkhPVKccWhx+P36OPPw4KjJ3BAXKls5IqdxU+9cAmOTUoBqJDovWMcaw1do+Bcow1hlBz6CF6pfvPr2qU1blaMss1gfHLLUGzi4r6Pc8v01p4iCkoUE6ODATKzQFzcqQNWw/qTSeE6HjS/Vt0e4FZ0v3467yo9c3Bd6tAvD44M35AmUiFQIAdvlsX9N2CcGO4GcVmOuyXrmgOilWnv00muLamhpoaBzbNTKQWiury4Wl081vVBtxeDxPU/uBSwa+xzriTjcZCcvxJjPIHMvd+VN60zgHgEteRWAlkwFcYc1lh3sEAXwpTfU2TvhngTcsc/KhcHDWNqPBIDFYjqxu282vxcgYn5HDyyGMD67nbTHyx5Ds0ReOEY48nKiEGxWygvLycgoICYmNjyczM1F9jfX09mqYRGhqqj+t3u900NjZiNpuDxuRXVVXpXfzNTUsQOZ1OampqCAkJCRqTX1JSgtfrJT4+Xs/YNzQ06LPaN0/gB7Br1y7cbjfJycn6rPb19fUUFBQQEhIS1Nthx44dNDQ0kJ6eTmRkpF5327ZthISEMGhQy0R5W7dupba2NmjOgYaGBtavX09ISAgjRoxA82v469zkb95BbXk1cdgJd1nw17hxVzVQ7CjH6FdI0CL1/Tbgxqv4sGkWQjCDAkq4GV+UAXOUlbBYe3DgHRWCYj60J6D11VWERkZiaFqjVtO0Tvv+1pQ28vkLK0GDc+8cS1hUz11qUHQ/mqZRUFfAqvJVrC5fzery1Wyp3nLAY5PDzeF7ziLbWgLnGGsMkSGRGJTuc6FM0zSqGjyBzHKtUx/D3Ho8c4nDhX8/zgcsJoM+bjklqlXQrHfRtmK39owhTkKIjifdv0WvEZgl3YQhxARxtr3W1VQtEOjVe4KCcH9dc/a7KSCv96A2eANLk9V5Ueu8UNyw1303j/9WLLufWCjtPmz7Ovawzd622718T0HB3mKFVtu0qbbH47Y8oalaS/dqp2+va6sHOrw7acChl42iOTPYMuZsqJrB8JA+KOFNGWKbCSXEwOXG0/GZVeKi4zCFWlBsJgbVR2OvTiExKYmkAf0xWE1gVkh/Zzsul4u0y8dgswU+F8a55VAM1jQ74ZNaxq1um5WL1+tl+mkn6WPzd+zYwbfffsuQIUOCgupXXnmF+vp6rrvuOn3c+9q1a/nqq68YOHAgF1xwgV73rbfewuFwcPXVV+s9CLZs2cKnn35Knz59uOyyy/S6H3/8MRUVFVxxxRVkZWUBkJ+fz0cffUR6ejp/bNVF/5tvvqG4uJiLL76Yfv36AVBcXMwHH3xAUlIS1157rV53zpw57Ny5kz/84Q96UF1ZWclnn31GbGxsUFC9ePFitm/fzplnnqkH1Q6Hg2+++YaIiAhGjBiBYlQwRVn5LW8NGzdubFqzfigQmNX+65dmExIWwm0XXY+/2oWv2s2CNXPYWJ3LhJBBDG1IA59KfV0d//MuwFCmcKW7ZYmqBaZNbDIWMcbYl3GxQzFGh+CLMPD22s8wGA3ccOk1WGLDMIQYWbBgAWvWrGHkyJFMnDgx8Cny+/nvf/+LwWDg/PPP13tLrF+/no0bN5KTk0NiuI3PnnqEgZOPwp0Y+PwdffTR+sWMnTt3smPHDpKSkoJ6eCxevBhVVRk1apT+mSopKWHnzp3ExMQErSawdu1afD4fAwYMIDQ0kJWrqqqisLAQqyUUk9kYWJfeq7J9+3Z8Ph8ZGRn6fuvq6igrK8NmswUtE1hcXIzX6yUhIUFvr9PppLq6GovFEjQBY3V1NT6fD7vdrv8evF4vDQ0NbYageL1ejEajvvyh6DmcPifrK9azunw1q8pXsaZ8DVWuqjb1TAZT0Bjk1kHx7sFytDWaEGPPvNjz08ZSHvt6I7kV+/g/GzAaFJLs1kA2WZ/0y9oUPAeyzDFhlsP+grkQ4veToFr0KopB0dcLN+9j+Vd9/HedRw/C92f8tyCQ4beZUPRu0+11pTYG/6zXN6JYjO2exMS1c6h+JNCPoW3Kr7jiijZlkydPZuzYsW32feqpp+J2u/WsLwTGp/fr1y8oQwyBLuu7b28wGDCbzUEz0gPYbDY8Hk9Q/eZsdnPw1Pp4Pp8vaB8hISHExcW1mdU+Li4OTdOCZrW32WykpqYSGxs8n3pycjKKouiBXXPdvn37trmimp6ejslkCiq3Wq0MGjSoTXvj4uLIyMgICswMBgNxcXFYLBZCMu2QGdhPWEMM1nVFxBybTeoRR6A2eKnaWQ4fLcBgNBA2MRl/daB7uVajoCkamlvFs6sOdtXhwkOjNbBWdvmLq1BQMISaKLVsp9RVSuXqAurVQozRVtQII3l5eeyutLSUdevWERoaii06gsbaGnatW03ujkIg8NloDlLz8/OZM2cOo0aNCgqqf/75ZzweDwMHDtR/H3l5ecyePZshQ4YEBdWzZ8+moaGBP//5z/rvPi8vjy+++IL+/ftz2k1nYQkxYQ038+Z7X1JTU8Mf//hH0tPT9bqzZs0iOzubyy+/XN/vp59+SllZGZdddhl9+vTR2/vBBx+QmprK1U0TDkLgQk1hYSEXXnih/jp27drFO++8Q0JCAtddd51e98MPP2Tbtm2cffbZDB8eWNO8srKS7777jqioKE4+uWUpsdzcXBobG0lLS9M/m6qq4vP5MJvNEoAcQpqmUdxQzKqyliz05qrN+LTg7tpmg5nBsYMZET+CkQkjGR43nITQhF793uyoaODhL9czZ3O5XhYXHkJqO5nl5EgbqVE24iNCMPagIV5C9HbFxcU4HI6gi8y9hQTV4rB1MOO//XUe8LV0sdvj4Ik25dpentvbdq2f09qvtp/b7P9xd6uoKC0Bs23vQXFXM5lMmExt/6yNGDGiTdmgQYOCsrjN7rrrrjZlo0ePZvTo0W3K//znP7cpGzx4MIMHD25T3jpr3axv377ccMMNbcrPOeecNmVpaWlBAVWzk05qO5t1QkICl156aZvyo48+uk1ZdHQ0559/fpvy4447rk1ZTExMu+099dRTOfXUlqWpjOEW4gen8sADD6CqatB7cqZrCK7aRoyNGqYGDV+1m9CqRi4oPwFfrRsD5kDPiEYfA5yJpCh2IvJt1OTlAoFx88cYhqBZDFT9ay3mmFBM0SGkKlEcO2IKSfEp5Azpx6k330nWyNHMX7S4zQWKpKQkxowZowe4zYYOHYrP5wuqGxMTw+DBg0lLSwuq26dPH5xOJxaLBU3T+O2bPFxmlaysLBISErDHtpwsNHfjbz0HgdVqJTExsc1EfZGRkXrw2sxkMhERERF0UQgCF2WsVmubiz3tfQ9crsBcFK33W1dXx5YtW9pcqFm4cCHbtm3jjDPOYNSoUQCUlZXxyiuvYLfbue222/S6v/76K8XFxYwdO5a+ffvqx9q4cSOhoaFBFy08Hg9Go7FNew9nbr+bjZUbA1nopkC63Fnepl6CLYERCSMYER+4DY4djMW4/8tq9mQNbh8vzdnG6/N24PGrmI0KV07O5vpjc7A1fZRa98JQVRW/34+iKEEBtc8XuDBhMBiC6qqqGqjb6nPpb5rQU1EUva6maaiqqh+vWXNZ876b6zaPrmzdO6S5rqIoQZOKNtvX/6t72q/f70dVVQwGg942TdPweDxAYN6U5n17PB58Pl/QcqSaptHQ0ICmaYSFhen7drlcuN1uLBZLUABUXV2NpmlERkbqx2tsbKShoQGr1Rp0MbasrCwwYWpcnP53qaGhgdraWqxWa9AwqeLiYnw+H4mJiXrbfD4fqqrKBb1urKioSA+Umy8y5+bm8ssvvxAfH89pp52m1/3444+prKzk8ssvbzNxb08nY6qFEEJ0K6rTF5g0rXnytGp3oKt5TeBebdz3JEuBZcJ2m0AtqtVEahYDitEARuV3T1a4cWExP7+zEVOIkUsemtAtx1B7vV5cLhchISH6yarD4WDr1q2YTKagC08//PADBQUFHH300fpJT15eHm+99RaxsbHceOONet133323TQBeUlLCK6+8QlhYGHfccYde9+OPP2bdunWcdNJJjB8/HggE9p999hnh4eFBSwrm5eVRW1tLamqq3uVdVVU8Hg8Wi6XHdmMvbSgNGgu9sXIjXjV4dmmTYmJgzEBGJIxgZPxIRsSPICksqdcFFF6vl8bGRgB96ArA8uXLqaurY/To0czJreOJbzZhqi9htKkAU0Qs119xAX3jA3NbvPzyy5SXlwedoG/atIkPPviAtLQ0rrrqKn2/r776KkVFRUE9O7Zv385///tfEhMTgy6SvvXWW+Tl5XHuuecydGigp9TOnTt54403iI6O5uabb9brvvfee2zdurXd70B4eDh/+ctf9LofffQRGzZsCPoONC+VGRISwt13363X/eyzz1i1ahXTpk1jypQpQOA7+9xzz2EwGHjggQf0ul9//TXLli3jqKOO4phjjgECw0aefPJJAO677z49oP3+++9ZuHAhEydOZPr06UAgKH/kkUcAuPPOO/UAes6cOcydO5exY8cGXTh9+OGHUVWVW2+9VX/vFixYwA8//MCIESOCvstPPPEEbrebG264Qf8uL126lG+++YZBgwYFXdR99tlnqaur45prrtF7ka1du5ZZs2a1GVL17bff4nK5mDJlCvHx8frvp7S0FLvdTmLiProrir0qLCzE4XCQmZkZFCjPmTOH+Pj4oNVpXnrpJSoqKoK+h5s3b+Z///sfycnJXHPNNXrdjz76iOrqaqZPn64PhevuZEy1EEKIHslgM2GxhUNKeLvPq24//prAeG5/tYstP83HVVJDTGQqdlssar03sFpAaSO+0sb9OCAtAbbJEFguzBi4V4wGMAXuFaMCTc8rpqb6RgOJBpiUGkpEQii+xcXUGhWU9rZp7ximPZTvdgwM+85i7Y3ZbA7KUkNgSMKYMWPa1D3++OPblGVkZHDXXXfp2b5mEydOZMCAAUFZf6PRSE5OTlBmHlqy5a17AjQ0NLB9+/Y2WfjffvuNdevWMX369KDx/88//zwmk4n77rtPr7tw4ULy8vIYOXKk3kvE4/GwcuVKzGZzUC+T6upqnE4ndrtdn3SwOfvX0YG61+9lU9UmfSz06vLVlDSUtKkXY43Ru3GPiB/BkNghWE3WDm3LoeDz+XC5XNhsNj1bWV5eTl5eHna7PaiXwocffkhtbS3nnHOO3jNizZo1fPnll/Tv35+LLrpIrzt//nyqq6t5c20jcwsDeZ8xdhMxHicZsQY9oD6c7U8+7GD/XrTed3Pme/fvhsVi0bP5zUwmE1artc3fmbCwMEwmU9A+LBYLdrs9aNgSoGe+W/cEcDoDQ4N2/3uyceNGHA4H48aN08vy8/PbHVbzxhtv4HA4OOecc/S/VWVlZaxdu5a4uLigi4rV1dUYjUbCwsJ6Xa+agoICamtryc7ODgqUf/rpJ+Lj4znzzDP1up9++ikVFRVBQ5G8Xi+7du1q8/9AUlISISEhQZ+5lJQUzj333KALZgDnnXfeIXp1XU8y1UIIIXq04q2b+fTJhzjh2pvJGTsezetvymq3ZLp9NS0Zb3+d5/ctUdYVFPQAWzEFB/2tf24dqLcJ6FuX60H8bgF9m/rNx2gO8JuD/eZ6zdvsO/D3+/243e6gbqeNjY1s2bIFgJEjR+p1f/31V3bs2MGECRP04GxfGfDp06frE9rV1NS0G4B/+eWXLF++nKOPPlofDtHY2MhTTz2F0Wjknnvu0U+kFy1axLp16xg1ahRjx47VX8O3336L2Wzm2GOP1QOIoqIidhTtoMJQwXZ1O6vLV7O+cj0WpwVVUWkwNQTmolAMDIgewPD44XognRae1iVZ6OZu0q2DoI0bN+J0OhkyZIgexGzZsoVly5aRlpbGUUcdpdd98skncTqdQZM6Ll++vN1A+YUXXqC6uporr7ySjIzAOvHr169n1qxZ5OTk6HVrGj38/e1ZbCmqYoMvEbcpjOuPzuGi0QlUV5YTERERlIH0eDxomobJZNLfN7/fr8+N0foCjtvtRtO0oPkx/H4/Xq8XRVGCgja32613OW7O8Pr9fn0Ojeb5GZrr+v1+LBZLUN3m5SVbB44ul0uv2/x7V1VVDxxbX1xyuVz6cJTm19Fcd/d5NLxeL36/P2joh6ZpevBjMrWsXqKqqr4aQk/o8aFpmv76WndBX7VqFfX19YwaNUr/vW3cuJG5c+eSlpYWlFl/7rnn2kwqumbNGj755JM2Afi//vUvSktLueSSS/R5NHbu3MmPP/5ISkoKJ554ol5306ZN+Hw+MjMz9S7vzV38O+t3u3PnThwOB9nZ2frvYfv27Xqg3LrXQHNGuXWgvGXLFt5///02k6DOmjWL6upqpk2bpmeU6+vr2blzJ5GRke0u79pbSaZaCCFEr+X3eTGaAielyf0GcNVLr2OxBk64FLMRc3wo5vj218rVVA1UDc2vovk0aLrX/IEl3zSfiuZvKvdrgcc+Va9fV+5k7ZxdpA+IJqVPZGCbpro0bav51PbLm4/Ruk7r47VqR3CjAZ+G5vOjddf5EhVaZdmbA/xWwXern1sH8ulGK4pRoTp3q37hYIQpgxEpmShFCnWlBShGhXCTgVtPugqv5qdxbbl+gWBE6iDS7UmkhibiKapHMRnQnF4G5QxAQcFb1hiYX0IDk1chIiwci8eIp6AONGisD6xWoKkavp31+LTAiXz5jmIKCwvJiknFZQ+MIXW73fz2228AZETEsatuJ/m1+ezcWYKx3oozxEFjWDWRmJiijiSqJjAWf/SQ/vSJyiYlLIWVO9axZPUqlFQvUf0N1GtFoMHHK79BQeHEgUcSarGBprGrqpj86kISw+PIic3UX8f2yp0oQGpEEhaDCTSoddVR6awhzGwjMbRlnPwP+Ytw+dwclzEemzEQDK4s28ivRcsZFJ3NCemT9Lqfr/8Ul99DxAY/sdZI0KCkahtbi7fiK25kWGmSfkHK7DPiBMq/3oLRGpiN3NzYQJ/wVGKrrFS+v1GvO8U6FC1Zw/hzJRXGetAgQYM/Z56D4oXyt9ZTVONke1k9g9VEhpDINRFWcuLDCMl3o+bvIhJAc1JOWavPXNPwDaX5MWBQAo+VlscogYlMMSg4laaLPwYFDE3bKQou/XHTdgYFX9N+MbTah6JQ33wBSVH0i0keA3hataf5GC7F1dImA5gUBdXgx6049f2Zm/btrW8AQ6A9JoOCSTEFJkl1eZpeI1gVCwqgunxNK3goGBUDpuax1H5Nr7t71hg6L9jrKLtfHGnW+iJcsz3NlXL55ZfT2NiodxOHwHwZ48aNCxrT3fqYrQP4mpoadu7c2SZzPWfOHD0Abw6qt2/fznvvvUdmZiYzZszQ686dO5f6+nrGjh2rXxhyOp2UlZURHh4eNLdFfn4+DoeDPn36BAXKP/74I/Hx8Zx99tl63S+//JLy8nIuvfRSfV4Lv99PUVFR0Hh/CGSPW/csaS47//zz2wSL7c3tEh4e3u6cMXujqYFlcVWXD83VtByryx+YO8XtxxRnw9o36oD22V1JUC2EEKJH2bJkAb++9ybnPfA49rhAhqw5oN4fzSfXiskABzH8eeuPO9lS66WysJF+lw3GaOr4k1RNaw78dwvIfS2BetvAfQ9BfOvyPV1AaFVfv9Cw+zFalzftN7jRBPbj45AH/q5Wj21AIGdSSBmFevlkAgFt6brletkwIhnGePgZyn5e1dRsjUuYig+V8n+v0etmKzZileHYfzNQsWwdAF78jDZm41NUwr9oYDBxDCaOTcZCdhjKyK4fyMDaQGt8+PkgZAE+VIYtT8GMj0Z24jCV4TF5cW6vpnZzrt6GfGug7Y6fd+Jr+mDuMObxm3k7/X3JJPpastnfhszFq/g5zz0Ruxa4eLTOmM8y8zb6+ZM4yjtEr7stJA+34mPkbylEa4Gu06qxAczQUFFHQ3FLl/RkcxQ+/HjWV9GoBX7LsYqRqYZB2CttNFa0BLRnMgYTRgybNZwEJleLQeFYBgLgLKjQ6ybSNCEW9biob/c9jQViMQJNJ/x1fqhz0F2vIfUYzRcGmgJtRX/c9Hlqvhixe9lu27Uta7540LasTd3W+zS0U7aHfSomA0qIEcViwGBpmiTVYkQJMWIwNz8XuBksBv05xWQImisjNja2zYSMaWlpbSaghMAEpK0nhAPIzMzkD3/4Q1APheZ92Gy2oIC0udfB7hcvNmzYQGlpKQMHDtSD6oKCAt577702WeKvv/6asrKyoEBZVVWKi4vbBMqpqamEhoYGTU6ZmprKBRdc0CZQbh2MNwsPD2/3QgSA5lMDwbDbj+pqDoxbBcUuP6q7+XFTuXu3em5/u/tuFjo2UYJqIYQQorOpfj+LZ31AbWkJy7/+nGMubzsj+qE24rh0jCYD/ccnHZKAGppOfI0KihGwdM9xffsM/PeU/W9d3k5vgQPK+Ov7bX7c0hOg/ZN+gk/kAQwKJsXWMhuzAl7Vg8UfQrhfo95fT6W/ElUJnGSHoqApBjZb8gkzhxJqCSPLksoQywBMxqbTKgVCFIWrlDPaBAzj/REM14ZgNpqwmZt7U2icWD8Zv+YjKjIVkzGwwkJavYavzkRSWBxhMcl6G5O2xuPxe4kalUaoORQUiK/ykVhUQ0xUIhEZ6Xo7JpcEuq7Hx2VjMweCgpFqMkP9IwkxWTC2avNZZOrZz+bAxw5kNJe1utPvW9UN+lHZe/06l5fZ60tYsbMGDQgxGThhcBIT+8ZiNCrs8XiKgkJTEryp1wlay+cRrak3iqaB2lTe9BhNa3ousK3WdB9U3nq73fenP25nO7UpEGtn37u3o/W+22zX1M7AvlseExxL7b/m16T/2HbsS08bDbM/lOYg22JsCsibgnCzEUNToB70fIih1XMtzxssRsJDbIGeL+bgv8WtZ7VuNmTIEPr06dMm+B0/fjw1NTVtgvuYmJg2y2q2FyinpKRw4YUXthmj3HocdLPQ0FD69+mH5vLhLWsMyhLr2WK3PzhAdrfNJrde7eZ3MwZWkjFYjYHlVUMC95bU3jNHgoypFkII0aM4KspZ8+NsJv3hIgydMJGMpmpsXFTMgAlJGI09q+uk2Ldady1rK9bqS1qtrVhLg7ehTb0+kX30Ja1GJowkOzIbgyKfhwPl8am8vTCPF37aSr07MOb3vLFp/PXEgcSFd7+Z87sTPfhuDpSbLybQ8rMe0Lcu05rKtPbKtN32116Z1hJ5B10kILDTpn1ru23XZvv9LaPpGD41EPx5AxlPzaOievxonsBjze1H1Z8LlB1qirlVRtxsaBWEt8qWNwXlrZ/Tnw+q3yq4b8qsa6rWkgXWs8RNwa47OEscVK/1827fwV+Eae81W4woViMGqzGwxGpTUBx43HTf9LMeNLe6N4SYUMw992+ljKkWQgjRK7gbGyjLyyV98DAA7HHxTLmg7Trch8qcdzexcWExpXkOjrl4YKcdV3Q8VVPZUbtDX9JqVdkqcmtz29QLNYUyLH6YvqTV8PjhRIZEtrNHcSB+3VLOQ1+uZ3t54KLFiLRIHjpjKCPTo7q2YT2E0ro7ddODzp/irvvS1KYeK56mQNvbFJQ3BeKqp9VzrX9uKlO9aqsA3d8UwAfqNF9U0LwqmleFBu/eG3OgTAYUAx17YcAASoipJRjeW+Db6vmWYDjw/O9ddvJwIUG1EEKIbquxtoaPHr6H2rJSzpv5OMk5A/a9UQfLHhHHlmWlpPSVoKqnqffUB7LQTUtarSlfQ52nrk29THumnoUeET+CnKgcjIbu2e2+J9pV1cgjX23g+w2lAMSGWbjzxIGcOyYNg5ywiw6iGBQUizEwZKYDexVrWmDoiepuCbL1jLl7t59bZ9PbC96bn296Tu8B4FODu+GbDK0yw03BcMgessC712t6XjEbet369t2ZBNVCCCG6LWt4BPb4BNwN9RiMnfdfVvOSMwDZI+K59NGJhEVK19SeYGXZSr7c/iWry1eztXprmzGkNpONIbFD9HWhh8cPJ8badgZg8fs5PX7+NXc7/567HbdPxWhQuGxiJrdM60+kre3M1EJ0R4qigNmI0dyxF9oCwbqmB+GoWst440M0X4c4dGRMtRBCiG7N42zE3dhIRGxcpxyvJLeWRZ9u5+Q/DyMkVE78e4oaVw3PLX+OT7d9GlSeGp4aNBa6X3Q/zAZ5Xw8lTdOYva6ER7/eSGFNYDbkiX1iefD0IQxIiuji1gkhxP6TMdVCCCF6HE3TWPbFLDRNY/yZfwDAYgvFYmt/zemOpvpVfnxzA7XlTpZ8sYMjL+jfKccVB0/TNL7K/Yqnlz1NtbsagNP7ns6x6ccyImEEcbbOuRgjAraW1vHgl+tZsK0SgJRIK/eeMpiThyVJV1QhRK8lQbUQQohuY9f6tcx7/y0AsoaPIrFPTqce32A0cOI1Q1kxO58JZ/bp1GOLA5dXm8ejix9lSckSAHKicpg5cSYjE0Z2bcMOQw6Xlxd+3MrbC/PwqRoWk4FrjuzDn4/uS6hFTjeFEL2b/JUTQgjRbWQMHc6YU8/CHhffaQG1qmo4KpxEJQSy4XFpEZxw1dBOObY4OB6/h9fXvc5/1vwHj+rBarRy7YhruWzIZdK1u5OpqsasFQU8OXszFfVuAI4fnMj9pwwmI7ZzepgIIURXk6BaCCFEl3JUlBMaGYXJHAiGjr70j512bK/bz/evr6ckt5Zz/jpGD6xF97WsZBkPL3qYPEceAJNTJnPvhHtJj0jv2oYdhtYU1DDzi/Ws3FkDQJ+4MB44bTBHD0jo2oYJIUQnk6BaCCFElynetpnPnnqErBGjOfG6W7tkzGVDjRuvy09NSaME1d1YtauaZ397ls+3fw5ArDWWu8bdxfSs6TJWt5NV1rt5+rvNfPjbLjQNwixGbjquHzMmZ2ORWYuFEIchCaqFEEJ0GXd9Pc46B+U783A3NmAN68DFRfeDOcTIKdcPp67SRVIfWYe6O9I0jS+2f8Ezvz1DjbsGgPP6n8fNY27GbpEVQTqTz6/y7uJ8nvthCw6XD4CzRqVy10kDSbRbu7h1QgjRdWRJLSGEEF0qd+Uy0gYO6bQZvgs3V9NY56Hf2MROOZ44eDtqd/DI4kdYVrIMgH7R/XhgwgMyEVkXWLS9kge/WM/m0joABifbefiMIYzNkjW+hRC9lyypJYQQotvx+3ws+vh/jD75dELtgcxwn1FHdNrxvR4/X760Gk3TsMfaSMyWC7Xdkcfv4fW1r/Pa2tfwql6sRit/HvlnLh18qUxE1smKapw89s1Gvl5TDEBUqJm/nDCAC8dlYDRIt3shhAAJqoUQQnSiH157ifW//Ejh5vWc98ATh3QsrKvey/aVZXhcfkYdnwGA2WIkITOCUHsIsalhh+zY4uDtPhHZlNQp3Dv+XtIi0rq2YYcZl9fPf+bl8vKc7Ti9fgwKXDQ+g9uPH0B0mKWrmyeEEN2KBNVCCCE6zRGnnU3+mpUccdo5HR5Qa5qG36diMhsBqC5p4Jf3NhMSamL4sWkYjYEJlI6/cgjh0SEyuVU3U+2q5pnfnuGL7V8AEGeL465xd3FC5gnyXnWynzaW8vBXG8ivbATgiKxoHjx9CENSZN4BIYRojwTVQgghDimvx43ZEgJAbFoGf3zxP/ryWR1l3a+F/PZNHkOPTGHsydkAJPaJJGNwDMk5Ufi9qh5UR8TIhErdiaZpfLbtM55d/iy17loUFM4bcB43j76ZCEtEVzfvsLKjooGHv1zPnM3lACREhHDPyYM4Y2SKXNgQQoi9kKBaCCHEIbNl8Xx+futV/nD/Y8SmBtYR/r0BdaPDQ97aCnLGJGCxBv4bMxgUGmrc7NxQpQfVBoPCaTeN/F3HEodWbm0ujyx6hN9KfwOgf3R/Zk6cyfD44V3cssNLg9vHS3O28fq8HXj8KmajwpVTsrnx2H6Eh8ipohBC7Iv8pRRCCHFIaKrK8m++oKG6ipWzv2LaH//cIfv99NkV1JQ2EhJqou+oBACyR8YRFjWC1AFRHXIMcWi5/W7+s/Y//Gftf/CpPmwmG9eNuI6LB18sE5F1Ik3T+GJ1EU98s4kShwuAo/rH88Bpg+kb37nL2wkhRE8mQbUQQohDQjEYOOMv97L6+28Yf/Z5B7x9o8PDb1/voKqkgTNuGaV3P80cFovFasTQauZhW7iFzKGxHdZ2cegsKV7CI4sfId+RD8DU1KncO+FeUsNTu7hlh5cNRQ4e/GI9S/OqAMiICeX+UwczbVCCdPUWQogDJEG1EEKIDuNqqKdo80b6jA4skxVqj2TiuRfu17bOOg/uRh9RiYH1qk0WA+sXFKH6NGpKG4lOCszWPfnsHBRZyqfHqXJV8cyyZ/gy90sA4m3x3DXuLo7PPF6CuE5U0+jh2e+38N6SfFQNrGYDNxyTw1VT+2BtmuRPCCHEgZGgWgghRIdw1jn4YOadVBcXcvbdD5E1fNR+b7thQRFz3t1E1rA4TrkuMJ7WYjUx6awc7PG2oMnFJKDuWdqbiOz8Aedz0+ibZCKyTuRXNT5YtpNnvttMdaMXgFOGJ3PvyYNIibJ1ceuEEKJnk6BaCCFEh7CGR5CY3RePs5FQ+56X3inaWsP2FWUMmJBEQqYdgITMCNACa0trmqZnLkccl94pbReHRm5NLg8vfpjlpcsBGBA9gJkTZzIsflgXt+zwsjy/iplfrGddoQOAAYkRzDx9MJP6xnVxy4QQoneQoFoIIUSHUBSFE669GVd9HeHRMXq5x+nDYmv572bdr4VsXVaKyWLQg+rY1HAuf2IS4dGy3FVv4PK5eG3ta7yx7g19IrLrR17PxYMuxmSQU4/OUuZw8bdvN/HJykIAIqwmbju+P5dOyMTUtMScEEKI30/+ZxNCCHFQNE1j6Wf/h7uxgSMvngEElstqDqh9Xj9fvLCK0h0OrnhyMrZwCwD9jkjEZDaQPrhlYjFFUSSg7iUWFS3i0cWPsrNuJwBHpR3FPePvISU8pYtbdvjw+FTeWriDF3/aRr3bh6LAeWPSuePEAcSFh3R184QQoteRoFoIIcRBKd66mfkfvANA9uhxKIYUXPVecsYElrkymY14XH5Uv0bx1lr6jIoP1B0eR/Zw6Xba21Q6K3n6t6f5OvdrABJsCdw9/m6OyzhOJiLrRL9uKefBL9eTW94AwIj0KB4+fQgj0qO6tmFCCNGLSVAthBDioKT0H8jk8y8lJCwMTUvii7+vJDTSQt9R8fpkYkdfPIBQuwV7rEyE1FupmsqnWz/lueXP4fA4UFC4cOCF3DjqRsItstZxZ9lV1cgjX23g+w2lAMSFW/jriQM5d3Ra0PJzQgghOp4E1UIIIfZbbVkpJbluNi6soM+oBCacfT4Afq+KPc5KSk4UXrdfH0OdlL3nCctEz7e9ZjsPL3qYFWUrABgUM4gHJj7A0LihXdyyw0dlvZu3F+bx719zcftUjAaFyydmccvx/bBbzV3dPCGEOCxIUC2EEGKPVL9KSW4tiVmRlO7YwmdPP0JYVAZ1juNAURh+TBoARrOBSx6ZKN18DxMun4tX17zKm+vf1Cciu2HkDVw06CKZiKwTeHwqczaX8fHyAuZsKsOnagBM6hvLg6cPoX+iLFUmhBCdSf7nE0IIsUcfPraMqqIGTrtxBCazhsfZSFhkPRNOT6PfuMyguhJQHx4WFi3k0cWPsqtuFwBHpx3NPePvITk8uYtb1rtpmsb6IgcfLy/gi9VFVDV49OeGp0Vy7VF9OWloknwPhRCiC0hQLYQQAnejl02LSqguaeDoiwfq5Ul9ImmoddPo8DBw4iDOvecREvvkYLbKTN2HmwpnBU8ve5pvdnwDQEJoAveMu4djM46VQO4QKq9z8/mqQj5eXsCmkjq9PCEihLNGpXLOmDTJTAshRBeToFoIIQ5DmqrhcfsJaRr7rKoaCz7eiqbBmJOyiIix4vd5Ub3z+MOdZxCZEJjRO22wjJU93Kiayqyts/j78r9T56nDoBi4aOBF3DDqBsLMYV3dvF7J7fPz08YyZi0v4Jct5fibundbTAZOGJzIOWPSmJoTJ2tNCyFENyFBtRBCHGZyV5bz64dbSO0fxfFXDgHAFm5h6NFp2GOtmC1GAOa89Sqrf/iW4i3rueixZzEYjF3ZbNEFtlZv5eFFD7OqfBUQmIhs5sSZDIkb0rUN64U0TWNNQS2zVhTw+aoiap1e/blRGVGcMzqN04anEBkqk48JIUR3I0G1EEL0Yh6Xj53rq0jIjMAeF1jWyhZhpqHGTdHWGjRV05e/OvL8/kHbHnH6ueSvXcWk8y6WgPow4/Q5eXXNq7y17i18mo9QUyg3jrqRCwZeIBORdbBSh4tPVxYya3kBW8vq9fIku5WzR6dy9ug0chJkaTIhhOjOFE3TtK5uxL44HA4iIyOpra3Fbrd3dXOEEKLH+OZfa9ixuoLxp2cz9uRsINDVe9fGKtL6R2M0B3cf9TgbsdhC9Z/9Ph9GkwRRh5MFhQt4dPGjFNQXAHBs+rHcPf5uksKSurhlvYfL6+eHDaXMWlHAr1vKaerdTYjJwIlDkzhndBqTc+IwyvrSQgjRpfY3DpUzJSGE6AVUv8rqnwvIX1fByX8ejsUa+POeNTyOyqIGLLaWLqMGg0LmkNg2+9i08Fd+ev1fnHPPwyT17QcgAfVhpMJZwVNLn+LbvG8BSAxN5O7xd3NcxnFd3LLeQdM0Vu6q4ePlBXy1ugiHy6c/NzYzmnPHpHHy8GRZW1oIIXogOVsSQogeSNM0Gh0ewiJDAFAMCut+LcRR7mTXhir6jg5MLDZwYjKDJiXvc3ZmTdPYMPcnXPV1rPnxWz2oFr2fqql8vOVjnl/+PHVemYisoxXXOvlkRSGzVhSQW96gl6dEWjlnTBpnj04jO05+z0II0ZNJUN2BNE2TZUWEEIdc+a46vvnXGoxGAxc/PAFFUVAUhdEnZOD3aST1jdTrGvaz+6iiKJxy852s+fFbxpx65iFquehutlRv4eFFD7O6fDUAg2MHM3PiTAbHDu7ilvVsTo+f7zeU8PHyAuZvq6B5oJ3NbOSkoUmcOyaNCX1i9/v7KYQQonuToLoD/WPlP6hx1/CXsX8h1By67w2EEGIfvB4/uzZUYQ0zk9IvCoDIeBtOhxfFAA01HsKjA9nqIVNTAVD9/qB9FG3ZRHl+LonZOSTlBCYja6ip5pO/PYjX5eLK5/8NQEhoKEecfk4nvTLRlZw+J6+sfoV31r+jT0R20+ibuGDABRhlUrqDomkay/Or+Xh5AV+vKabO3dK9e1x2TKB797BkwkPk1EsIIXob+cveQYrri3lz3Zv4NB+Lixfz+JTHGZkwsqubJYTo4VZ+n8/SLzaROiCUs24/GgCL1cTYE1XqKndQuSuK8OiRANRVVvD2Hdfj9/q46Z2P9Z4zG379mdU/fMPEcy/Ug2qz1UrZju0AeN0uzCHWTn9tomvMK5jHY0seo7C+EIDjMo7jrnF3yURkB6mgupFPm7p351U26uXpMTbOHpXGOaPTyIiVC+1CCNGbHVRQ/fLLL/P0009TUlLCiBEj+Mc//sG4ceParev1enniiSd4++23KSwsZMCAATz55JOceOKJv6vh3U1yeDL/Pv7f3LvgXnbV7eLy2ZczY8gMrh95PWajTDoihGjh83qp3JWPu7GRjKHD9fLv/v0hO1YtZ/RJxzPu9OMBSMhQcNf+kx2/mdC0o/RAuaZ0M6u//xqLzUrm8JEAhISF4W4IjNn0ud2YrYFAObFPDn3HTiA6KUU/ljnEyll3zsQWYcdokr9Rh4PyxnKeXPYk3+V9B0BSWBL3jLuHYzKO6eKW9TyNHh+z1wW6dy/cXqmXh1qMnDIsmXPGpDEuK0a6dwshxGHigIPqDz/8kNtuu41XXnmF8ePH8/zzzzN9+nQ2b95MQkJCm/r33Xcf7777Lq+99hoDBw7ku+++46yzzmLhwoWMGjWqQ15EdzEueRyfnP4Jf1v6N77Y/gWvr3ud+YXzeXzq4/SP7r/vHQghepTW8yh4nI3sWLUCr9vF0KOn6XUWf/IhW5csZNSJpzL0mECg3FhTzbt334LBZOKWdz/V91G0dRsNVRvIXZmmB9Up/QLZQ3OIGZ/HrWeUM4eOwGQ2kdp/oH4sc4iVK579Fza7HVNIiF4+7NgTGHbsCUFtVxSFPqOP6OhfieiGVE3l/zb/H8+veJ56bz0GxcAlgy7h+pHXy1ClA6CqGkvzqpi1vIBv1hbT4GkZZjGpbyznjknjxKFJhFqkE6AQQhxuDnid6vHjx3PEEUfw0ksvAaCqKunp6dx4443cddddbeqnpKRw7733cv311+tl55xzDjabjXfffXe/jtkT16n+Kf8nHlr0ENXuaswGMzeOupHLBl8mY9VEj+P1+DGZDYflJHzOOgdrf/4en8fDpD9cpJf//Na/2TD3Zyb94SJGn3wGAI6KMl67/koMRhO3vNcSKP/0xr9Y9d3XTDj7fCaffykADbUNvHrdVWialatf/gcR0eEArJi9kMLN2xk8ZRR9xwwFAoG73+vFZLF05ksXvcSK0hU8+9uzrKlYA8DQ2KE8MPEBBsUO6uKW9Rw7KxuZtaKAT1YWsKvKqZdnxoZy7ug0zhqdSlq0XJwQQoje6JCsU+3xeFi+fDl33323XmYwGJg2bRqLFi1qdxu3243VGjxWz2azMX/+/D0ex+1243a79Z8dDseBNLNbOC7zOEYkjOChhQ/xS8EvPLf8OX7Z9QuPTnmU9Ij0rm6eEPvlu/+sY9tvZQyenMwxl7achK+ZU0Co3ULmsFjMlt5xocjrcTP/f+8w5KjjSMjqEyhzu5n3/lsYjCYmnnuhHihrqoa7sYHGVn+bbBF2UgcOwRZhx+/zYTIHulQPPvJ4wqIHEBIWr9cNiwwjZfCtVBc3UlPsISI6UD76xEmMPnFSULsURZGAWhywzVWbeXHli/xa8CsAYeYwbhp1E+cPOF8u7u6HerePb9YWM2t5AUt2VOnl4SEmTh2ezLlj0hiTGX1YXmwUQgjR1gEF1RUVFfj9fhITE4PKExMT2bRpU7vbTJ8+neeee44jjzySvn378tNPP/HJJ5/g32122taeeOIJHnrooQNpWrfx3WvfkZARx/DjhhNni+PFY1/ks22f8belf2NF2QrO/eJc/nrEXzm739nyn7Ho9vqMjGfbb2WERrZ0JfZ5/Mz7cAsAf3x2qh5Ub1hQxLblZfQbm8CgSS1jd+ur3YRGWrr92MJ577/Fym+/ZOfaVVz61IsYDEZsdjtDjjoOa4Qd1e/HaAr8yTzi9HMYdeKphEXF6NubQ6xc8NCTbfbrcUez/LudhEbWMnKahtL0ezj+yiGER4dgsUpXUdFxCuoKeHnVy3yd+zUaGkbFyNn9zubaEdeSENp2iJZooaoai3Mr+Xh5Ad+uK8HpDZynKApMyYnj3DFpnDA4CVsvuZAohBCi4xzys7kXXniBq6++moEDB6IoCn379mXGjBm88cYbe9zm7rvv5rbbbtN/djgcpKd3/+xu3toK1v/8DuvUWizW+xhyVGD92GkxU8kcGMVLhW+yrHolDy56kDm75vDgpAeJs8V1dbOFAKCmtJHFn+cycEISWcMDn8uc0QkkPR5JiK3lT4XPo9LviEQaHR5CQlvKy/Lr2LWhisQse6u6ft6+ewEGg8KVz0whJDSQvS3aVkNtWSMJWXZiU8I76RXu3YSzzqdg43qOvPByDE2ZPLMlhBOvu7VNXXtcfJsygB2ry1n7SwF9RsYz9Kg0ANL6RxOdHEZa/yi8Hr8eRMckhx2iVyIOR5XOSl5d8yofbfkInxpYyml61nRuHHUjmfbMLm5d95ZX0RDo3r2ikMKalu7dfeLDOGd0GmePTiU50taFLRRCCNHdHVBQHRcXh9FopLS0NKi8tLSUpKT2l+KIj4/ns88+w+VyUVlZSUpKCnfddRd9+vTZ43FCQkIIaTXJTk+ROiCS8Jg4Gms9ZI9q6Sq7delC5rz1KqeNGcdRpxzHCyteYG7BXK7+z7lcNfVGTh4hWWvR9TYuLGL7ijJqShvJHBaLoigoBoWImODhG9ZwMyf8cUib7YcemUJiVgRxaRF6WUOtB4NBwWg2YGkVmG9ZWsr6XwsZe3IWsacHgmqf18/nf19FRKyVYy8biMkcCGxdDV6MJgPmkI7NDtWUFLNz/RqGHzcdgNDIKC792wsH9F2sLmnAHmfDaDIAUFvuZNfGalS/pgfVRrOBi2aO79C2C9Gs3lPP2xve5u31b+P0BQLCickTuXnMzQyJbfs9FQEOl5dv1hTz8fICfsuv1ssjrCZOG5HCuWPSGJUeJf83CyGE2C8HFFRbLBbGjBnDTz/9xJlnngkEJir76aefuOGGG/a6rdVqJTU1Fa/Xy6xZszjvvPMOutHdldli5k8vv4CqqhgMgZNsj8uH1+UnIi6ehKy+nDXkYialTOLeuXcz9lsnm+a/yaLLFvHX4x/AbrFTXVKEz+MhJiVN72oqxKHgbvTi92mE2gPjdUedkImj0sWYE7MO6kQyLi04oAaIjLdxzUtH46r3Bu0zJjmU9MExxKW1ZKnrq9yU5NZSUVivB6kAiz/PZf2vhYw/vQ9jT84CwO9VWTevkIgYK9nD4/Qu1fvLUVHGO3+9EZ/HQ2xqOqkDBwMc0Ov+9NkVFG2t4fSbRpI+ONANvO/oBFRVo8+I9jPZQnQUj9/Dh5s/5LU1r1HtDgSFQ2OHcvOYm5mQPKGLW9c9+VWNhdsr+Hh5Ad+tL8HlVQEwKHBk/3jOGZ3G8YMTsZqle7cQQogDc8BR22233cbll1/O2LFjGTduHM8//zwNDQ3MmDEDgMsuu4zU1FSeeOIJAJYsWUJhYSEjR46ksLCQBx98EFVV+etf/9qxr6QbaQ6oNVXjxzc3UFGQyDn3/IOY5MDsoP2i+/HKpH/w5o+34qyt5fPy71jyxWoemfwIjbNXs3L2l4w59SyOvvSPTftRKdyykfiMbEJCZYZR8fttX1HGnHc3kTk0luOvDGSzrGFmpl81tMOPZTAoeuDebPgx6Qw/JnhIh81uYfrVQ/G4fEHBrbPOAxC0j7oqF/M/2oopxMifnj9SL18+O4+S7bUMmZqqd2FXVQ1XvRdbhFnfrz0ugX7jJ+EoLyMidu9DMPx+lcJN1ZTl1+lBPUBUUiglubVUlzboQXVEjJXRJ0hXW3Ho+FU/X+V+xcurXqa4oRiALHsWN466keMzj5fMaju2l9cza3mge3eJw6WX90sI59wxaZw5KpVEu3UvexBCCCH27oCD6vPPP5/y8nIeeOABSkpKGDlyJLNnz9YnL9u5c6ceVAK4XC7uu+8+cnNzCQ8P5+STT+a///0vUVFRHfYiuqvGOg+VRQ001nrwuv0orX4vMQnJ3P7SB6woXs6iRQ+ws24nV39/NZeWTMBqtZGQma3XrSkt5sOZd2KyhHDj2x/p4z2rigowW62ER8fKiZQ4IPY4G+5GH5WF9fg8fkzdYOKdEJuJnDFtJ1I66ZphuBu9bbLRfUfFg6IEffaLt9WSv66S7FaZYke5k/dmLsZk2sWMZ87DYg2MjRww+QI8TkAJzq7DbutPN/r46qXVaBoMmJCkd4cfd2o2k8/OCerWLsShomkav+z6hRdXvsi2mm0AJNgS+PPIP3NmzpmYDPI5bK3W6eWrNUV8vLyAlTtr9PJIm5kzRqZwzug0hqdFyv+dQgghOsQBr1PdFXriOtXNXPVeyvIdZAyJ3WOdRm8jzy1/jg83fwhAtj2LRyc9yvDEEQAUbFrP1y8+TVhkNJc88Xd9u1lPzCRv1XKO/9MNDD/uRAA8LieO8jJiUtIwGLs+UBJdT9M0dqyqwOv2MWBCsl5euLma5H5R3X5W7gNRtK2GqqIG0gZEE5UY6NVRuKWa/3vsH/hdSxk5/RSOu/LPAHz10mry11Vy9MUDGDI1FQBHhZPvXltHbFo4x7ZaQmz2q+uwhpkYPT0Te5xMWCQ614rSFTy/4nlWlq0EIMISwVXDruKigRdhNe05w1rqcPHe4nz+t2wX5XWBZSqbY8jmb72iKK0eNz+n6BXae25v+2i73d7rK7ttqCjtH7Ptcy1/t/R6reqXOFx4fIHu3UaDwtH94zl3TBrHDkogxCT/NwohhNg/h2SdanHgrOHmoIC6vtrFmjkFjD+9jz5uNNQcyn0T7uOY9GO4f8H97HDkcdl3l/On4X/i6uFXkzZwCNf88y28HnfQvlWfD8VgIC69pbtp4cb1fPK3B4nPzOayp/6hl1cVFRIeE6Nn6cThY8eqCr7991pCwkxkDY/TZ+BOHRDdxS3reCk5UaTkRAWVpfaP5rSbT+bzp37DZAnRs9DxGRH4PH6iW83CXVvmpCy/jpoyJ0ddNACjMfAdPfFPHd8tXoh92X2taavRysWDLmbG0BlEhkS2u42maazcVcNbC/L4Zm0xPlXb7fmm+90Lgmt1zAvoYgOTIjh3TBpnjEwlPqLnTX4qhBCi55BMdQd6avYm/JrGdUflENkUuLSmqRofP/kbZfl1DJmawtEXD2xTp9Zdy2OLH+PbvG8BGBI7hMenPk6fyPZnS/d5PBiMRj0rvX7uT/z0xiv0GX0Ep97cMm799Zuvpqa0hPNnPkHaoECA4Kyvw+/xEBYdI13gehm/V8VoDgSEql9l1lPLyRgSy6gTMg6LdZH9Ph+OijKik1rWy64uKQr6uT0NtW6KttYQmxJOdHKofC9ElziYtabdPj/frC3mrQV5rC6o1cvHZcVwxeQsxma1uoi2W2CtaaA1/bR70K1pWpu4u6WO1mr7lvq775s97rud47az74M5rt1qpm98mHyHhRBC/C77G4dKUN1BSh0upj45B49fxW41ce3RfZkxKRvbbmNV89ZWMO+jrZxx88i9diP9dse3PLr4URweByHGEG4ZfQsXDboIg2LY4zbNNFXF43Lpk5r5vF5ev/lq6isr+PNr7xFqD2Q4Vnz7JXPe+jcDJx/FKTfdoW9fW1ZCRFy8PnZb9BwNtW4WfLyN2nIn5945Rj+h1FTtgGfI7qkcFWV88ezjNNbWcvkzLxESKutBi57hYNaaLnO4eG/JTt5bspOK+kBvJovJwBkjUrh8UhZDU9vPaAshhBBi36T7dydLiAjhX5eM5qnZm9lcWsdTszfz1oI8bp7Wj/PGpmNu6kaaNSyO9MExerdSCKzDaw0LzmyflH0SoxNGM3PhTBYULeDJZU/yy65feGTyIySHJ7M3isEQNEu4yWzmmn++RaOjVg+oARprq1EUA1FJLfvz+7y8ccu1GIxG/vjia4RHB2Y1bnTUYraEYLbKDKndmaIo5K2pwOv2U7rDQVKfwPt9uATUANbwCFwN9XjdLioLdpHSv22PECG6k4NZa3rVrhreWrCDr9cW4/UHro0n2kO4dEImF47LIDZcujsLIYQQnUUy1R3Mr2p8vqqQZ7/fQmFN4OQoOy6Mv5wwgJOHJbXpila+s45Pn1vBxDP7MuzotDb70zSNjzZ/xLPLn8XpcxJuDuee8fdwap9TO6Rbm9ftwu/zYQ0LrBdcXVzIO3fehNFo4vo3PtCP8fNb/2bl7K+YfN4lTDj7/EDbVJVGRy1hUb1vbG5P4XH6KNhcTZ+RLbNdb1laQnRSGPEZbWe17q3cjQ1BGemyvFxC7ZGEx+x5gkAhutqBrjXt8al8u66YNxfksWpXjV4+JjOaKyZlceLQJP0CrhBCCCF+P+n+3cXcPj/vLd7JS3O2UdUQWGd3WGokd544kCn9WtbFXThrGyt/2EnG4BhOvWHEHjOK+Y587pl/D2vK1wAwLWMaD0x8gGhrxwe0quqnvqoKe1xLoPbZ04+y/bfFnHjdrQw56jgAakqKef3mq8kcPorTb79HJkHrZM46D+8/tAR3g5cLZ44nOunw7Oa8edE8fnztZaZfdys5Y8d3dXOE2KcDXWu6vM7N+0t28u6SfH0Wb4vRwKkjkrliUhbD06I6+yUIIYQQhwXp/t3FQkxGrpySzXlHpPPar7n8Z14uawtrueT1JUzJieOvJw5geFoUE8/uS2SCjZyxiXvtoptpz+TtE9/mzXVv8s9V/+THnT+ysmwlD016iKPSj+rQthsMxqCAGuDMO+6joaYak6WlS2F1cSEoCl63WwLqLmCLsJDUJ5Ka0kZcDb6ubk6XKdm+FVdDPWt+/FaC6gNU6azkvgX3UVRfxMSUiUxNncrYpLGEGKXr8KFwoGtNry2o5c2FO/hqdTEef2B5qISIEC5p6uItM1oLIYQQ3YNkqjtJRb2bl37exntL8vXxb6cMS+b2E/rTJz48qO6GBUWk5ETp6+zubmPlRu6Zf49+UnZOv3O444g7CDN3fqaytqwEd2MjCVmB2cn9Pi8fPngX/cZPZuT0UzBb5KSvI2iaRv7aSlb/vIuTrh2mz+DtrPcQYjNhOMy6fGqqimIIvGaf18uaH79lxPEnYzTJdcL9tbFyIzfPuVnPlDazGq2MSx7HlNQpTE2dSlpE22Ep4sDt71rTXr/K7HUlvLUwj+X51Xr5qIworpiUxUlDk7GYDq/vuxBCCNFVpPt3N7WrqpG//7CFT1cVomlgNCicNzadW6b1I9FuJW9tBV//cw0hNhMX3D+e8Oj2g1K3381LK1/i7fVvo6GRGp7KY1MeY0zimE5+RcE2zv+Fb/7xDGHRMVz1j9cxmdsuLSYOnN+v8v7MxTgqXEw4sw9jTszq6iZ1CZ/Hw/wP3qG+qpJTbv6rLJdzkGbvmM39C+7H5XeRac/k6mFXs7JsJfMK51HWWBZUN8uexdS0qUxJncLYxLFYjJYuanXPtL9rTVfWu/nf0p38d3E+pY5AF2+zUeHU4YFZvEemR3VF84UQQojDmgTV3dzGYgdPf7eZnzcFTmCtZgNXTMrmilHpzH97I/EZERx14YB97mdZyTLum38fRQ1FKChcMeQKbhh1Q5ed+Pq8XjbN/wUUhaFHT9PLF8/6gD5jxukZbbFvteWN2ONseuC4bXkZZfkORk/PbDNb/OGiLC+X9+65FdXv56JHnyW5376/I6KFqqm8tPIlXlv7GgCTUybz5JFP6sGdpmlsqd7C/ML5zC+cz8qylfg1v769zWRjfNJ4pqROYUraFFLDU7vkdfQE+7vW9LrCWt5amMcXq4vw+AJdvOPCQ7hkQgYXjc8gIUJWXBBCCCG6igTVPcTSHVU8OXuT3s0v0mbm2ql9uHxSJqHWQODk96koBgXDHsZc13vqefq3p/lk6ycA5ETl8MTUJxgY0z2WEireupn377sdg9HENa+8HbSsl2jf3Pc3s25eIadcN5ysYXH73uAwsuLbL4lMSKTvmHFd3ZQepd5Tz13z7mJuwVwAZgyZwc2jb8a4l/Xo6zx1LC5ezLyCecwvnE+5szzo+T6RfZiaOpUpaVMYkzAGs/HwvNjT2v6sNe3zq3y3vpS3Fu5gWV5LF+8RaZHMmJzNScOSCDHt+X0RQgghROeQoLoH0TSNnzaW8dR3m9hSWg8E1hu9ZVp/zh2dyrz3t9BQ62H6VUOw2PY8ZnTOzjk8uOhBqlxVmAwmrh95PTOGzNjrSXNnqCoqYOH/vY85xMr0a2/Sy4u2bCSxTw5Gk5yI7655VvjR0zOZeFbfrm5Ol6ktK2HO2/9h2h//LMtj/Q75jnxu+vkmcmtzsRgsPDjpQU7re9oB7UPTNDZXb2Z+4XzmFcxjdfnqoCx2qCmU8cnjmZo2lampU0kKS+rol9Gt7c9a01UNHv63dCfvLs6nuNYFgMmgcPKwZK6YnMXoDFmeUAghhOhOJKjugfyqxqcrC/n7Dy1rXA+3h3JiEeDXOP3mkaQNjNnrPqpcVTy86GF+2vkTACPjR/LYlMfIsGcc6ubvk6ZpelfmhppqXrvhSmwRdi554vnDeq1rj8vHmp930X9cEva4wCzqrgYvtWVOErN77+d9f3z00N3s2rCWfuMncfpt93R1c3qkBYULuOPXO6jz1JFgS+CFY19gaNzQ373fWncti4oXMb8g0FW80lUZ9HxOVA5TU6cyNW0qIxNGYjb0zotn+7PW9IYiB28vzOOzVYW4m7p4x4ZZuHh8BhdPyCTRLl28hRBCiO5IguoezO3z8+7inbz081aqG70k+hTG2MO45KIhTMrZd1dgTdP4MvdLnljyBPXeemwmG38Z+xf+0P8P3WZip4IN6/jqxaewx8Zz4aPP6O3yeTyYLIfXREjf/Wcd234rY8D4JKbNGNzVzelWKgt2MeftVzn+6uuJTDi8Mp+/l6ZpvLPhHZ5b/hyqpjIifgR/P/rvxIfG73vjA6RqKpuqNulZ7DUVa1A1VX8+zBzGhOQJga7iqVNIDEvs8DZ0tn2tNe1XNX7cWMqbC/JYsqNK325oqp0Zk7I5dUSydPEWQgghujkJqnuBOpeX1+bt4D/zcmn0BLpZTu0Xx61H5hBR66Pf2L2fmBbVF3H/gvtZWrIUgCmpU3h40sOH5KT6YPi8Xhqqq4hMCLwOv8/Hm7deQ3K/gRxz+dWERkZ1bQMPEU3VUDUNY9MyWGX5Dr77z3omnNFnn+9pb5e7chnuxkYGTe7YtdcPNy6fi4cXPcyXuV8CcFbOWdw34b5Om8Cw1l3LwqKF+oRnVa6qoOf7R/fXl+wakTCiR2Wx97XWdL1L5YNlu/jvony9x5HRoHDS0CRmNHXx7i4XN4UQQgixdxJU9yLldW5enhNY49rv0zi3wUKmz8iA6elMO6vfXrdVNZX3Nr7H88ufx6N6iAyJ5L4J93Fi1omd1Pr9l7dqObOemEloZBRXv/RGr8xYF26uZv7HWxkwPomR01q65KuqtseJ6A4XeatXMOvxBzCHWLns6ZeISpTM9MEobSjlljm3sK5yHUbFyB1H3MFFAy/qskBO1VQ2Vm5kXuE85hXOY235WjRa/tsJN4czMWUiU1OnMjl1ctDM2N3N3taazqvw8PbCPD5dWYjLG8jSx4RZuGhcBhdPyCA50taVTRdCCCHEQZCguhfaWdnIc99vomJJOaNdJj60e5g2MY2bj+tHwj7G5G2v2c498+9hQ+UGAE7KPol7x98btE5qd1Cauw1HZTn9jpiol/3w2kuk9B/EwMlH9vhJzTYsKGLOfzdhj7Ny8cMTD/tAujVV9fN/j9xLQlZfplx4GWZL+2u0iz1bVbaKW3+5lQpnBZEhkTxz1DP6uN7uotpVrWexFxQu0MchNxsYM1DPYg+PH47JsOfJGTvLntaavnzwDJbmOnlrQR6LclvGlA9OtjNjchanjUjBapYu3kIIIURPJUF1L7ahyMHfv9zADzsCJ3FWs4ErJ2VxzdE5RNr2HHR6VS+vrnmV19a8hl/zkxCawCOTHmFS6qTOavoBK9m+lffuuRWD0cgfX/wP9rju0XV9f1WXNODzqsSnRwCg+lVWfLeTIUemYAvvfZn4A+H3+dgw72eGHjUNxWDQy4ymrg+ieqJPt37KI4sfwat6yYnK4cVjXyQ9Ir2rm7VXftXPhsoNgbHYhfNYV7EuKIsdYYlgUsqkwLrYqVOIs3Xu8nJ7Wmv64v5X8dN6J+8syqeguqWL94lDkrh8UhZHZEkXbyGEEKI3kKD6MLAkt5InZ28id0ctZzdYmBet8ofpfblsYtZesyNry9dyz/x7yHPkAXDBgAu4dcythJpDO6nl+8/VUM+aH2fT6Kjl6Ev/qJdvWTyfpL79scd3366iW5aV8OObG4nPiODcO8fISXYrmqbxf4/cy671azj6sqsYc8qZXd2kHsun+nj2t2d5d+O7AByXcRyPT3m8W36f96XKVcWCwgWBLHbRAmrdtUHPD4oZxJTUKRyZdiTD4oYdsuUC97TW9GnpV/Ldaj+frijE6Q3McxEVaubCcRlcMiGT1Cjp4i2EEEL0JhJUHyY0TeOdZ5dTv83BTqOfD8M9JEdZuWVaP84ZnYapaTKs3Tl9Tp5f/jzvb3ofgEx7Jo9NeYwR8SM6s/kHpdFRy2vXzcDv93H5My8Tm9o9s3GNDg//vX8RaQOiOX7G4L2uMX44Wv3Dt8z731scf/UNDJg4taub0yPVuGr4y69/YUnxEgD+POLPXDviWgxK+9/7nsSv+llXuY55BfOYXzif9ZXrg563W+xMTpnMlLQpTE6ZTKzt969j3t5a0xOSJzIp+lJ+Wm1h/rYKve7ApAhmTM7ijJGp0sVbCCGE6KUkqD6MeFw+FszaRml6CC8uyKWo1gVA3/gw7pg+gOlDkvaYJV1YtJD7F9xPWWMZBsXAVcOu4trh12I2dt+xy1VFhfz0+j/xOBu56LHn9NfmqCgnIjauSzLCXo+ftXMKcDV4mXR2jl7eUOMmLErGBgPUV1fh93r0pbE0TcPpqO21s7wfalurt3LTzzdRUF+AzWTj8SmPMy1zWlc365CpcFYExmIXBLLYDo8j6PkhsUOYmhZYsmto7NADymK3t9b0oJghDAw5n19WRbGzqhEAgwInDE7iislZjM+Okd4nQgghRC8nQfVhyuX18+7ifD6dvZ0tPg9eBUakR3HniQOY1Lf98Yi17lr+tvRvfJX7FRDoYvnE1CfoG9W3M5t+wLwuF2ZrYII21e/nPzddRYgtlNNuu5uYlLRObUvxtho+eWYFikHhogfHE5XQ87reHkq7Nqzli+eeICohkQseflrGTf9OP+X/xN3z78bpc5IansqLx75I/+j+Xd2sTuNTfayrWMevBb8yv3A+G6s2Bj0fFRLFpJRJTE2byuSUyURbo9vdT3trTaeEZZCqnc3itSk0egKzeEfazFwwLp1LJ2SSFi3fbSGEEOJwIUH1YaxwczWfv7AKv93Ef4wN1PoDY/+O7B/PX6cPYGhq+zN+f5/3PQ8vfphady0Wg4WbR9/MJYMv6RFdScvycvlg5p2YLBaufvkNfeZoTdMOSTZJ0zTqKl3Y41rGUM59fzOJfez0H5cks3rvpq6ygrfvuJ7I+CTOumsm4dExXd2kHknVVP695t/8c9U/ARiXNI5njnpmj0Hj4aLCWRGY7KxgHouKFlHnrdOfU1AYGjeUqamBLPaQuCEoKG3Wmo40x2J3ncKGLQOAQJZ7QGIEV0zO4syRqdgs0sVbCCGEONxIUH0YK95WwzevrCVtYDQjz+vLy3O28/6SnfjUwFt92ogUbj++P1lxYW22LW8sZ+bCmcwrnAfA2MSxPDblMVLCUzr1NRwMV0M9lQW7SB0wSC/7/JlHscclMO7MPxAW1TGBR12Vi9n/Xkt9tZtLHp2IWU6221VfVUl4TMs417K8XGLT0nv8smhdpdHbyH0L7uOH/B8AuGjgRfzliL9gNsjvszWf6mNN+RrmFQbGYm+q2hT0fIw1hnhbPJurNwMQYgjD6DiO0oIjQDOjKHD8oESumJzFxD6x0sVbCCGEOIxJUH2Yc1Q6CY2wYGoK+PIq6nnuh618sboIAJNB4YJx6dx0XD8SIoLXuNY0jVlbZ/HUsqdw+pyEmcO4a9xdnNH3jB51glmxM4+377gBxWDgqhf/87tmCvd5/ZiaJiPy+1Tef3AxjXVeTrthBCn9ojqoxb2DqvqZ/793WPHtF1z4yDMkZnfvYQQ9QUFdATfNuYmt1VsxGUzcP+F+zu53dlc3q0coayxjQeEC5hUGstj13noAjFjwVU+mvmwqqKHYrSYuGJfBpRMySY+RLt5CCCGEkKBa7GbRp9sxmg2Ejorm6e+2MHdLOQA2s5E/TsnmT0f1wW4Nznjtcuzi3gX3srJsJQDHpB/DzIkzO2SW3c6gaRr5a1ZSlpfLuDPO1ctXfvcVkQmJZI8Yo6+PvCc1ZY38/PZGGh0eLn54gn5RoXSHg4hYK6H2w3ut6fZomsaXzz3B1qULmXTexUw858KublKPtrR4KbfPvZ0adw2x1lieP+Z5RiaM7Opm9Qh+VSO/soGNxXVsKnGwvriK9ZVrqfIU4KsfhOaz0y8hnCsmZ3HWqFRCLTLWXwghhBAtJKgWurJ8B//3xG8AnHX7aFL6RbFoe2CN61W7aoDAWqvXH53DpRMzg5aH8at+3lr/Fi+tegmf6iPGGsPMiTM5NuPYrngpv5uzzsGr183A53Fz/kNPkjZwiP6cpmlUFTWg+jXiMyIA8Lr9vP6Xefi9Khc9OJ7opLZd5gVoqoqqqvoEZI2OWoq2bCJn7PgublnPpWka/9v0P55a9hR+zc/g2MG8cMwLJIUldXXTuqVap5fNJXVsLHawqcTBhuI6tpTU6etJt6YocNzABK6YlM3kHOniLYQQQoj2SVAtgmxYUER9tZtxp2brZZqm8d36Up7+bhPbyxsASI60cuu0/pw9OjVojevNVZu5e/7dbK3eCsAZfc/grnF3EW4J79wX8js56xws/fxjSnO38Yf7H2vJPOduY9dmH0s+LyJzaCyn3tCyXnfemgri0sMJj7buabeHNUdFGbP/+TzJOf2ZetEVXd2cXsHj9/DYksf4ZOsnAJzS5xQenPggVpN8Bpuzz5uaAuiNxYH7whpnu/WtZgMDEiMYlGxnYFLTfbKdSJuMRRdCCCHE3klQLfbK6/bjqHASmxqOz6/yyYpC/v7jFoqb1rjOSQjnLycMYPqQRD3w9Pg9vLzqZd5c9yYaGilhKTw65VGOSDqiK1/KQSncUsW238oZOCmZ+PQw3rjlTzTU1GAKPYu+Y0Yw/eqhkr3aT1uXLuSLZx/HYrPxxxf/Q6i9/dnlxf6pcFZw65xbWVW+CoNi4NbRt3L5kMsPy8+jw+VlU1PX7Y3Fe88+A6RG2fTAORA8R5AVG4ZRZuMXQgghxEGQoFrskaZqfPfaOvLXV3LCVUPJHh5Yv9rl9fPfRfm8/Ms2ahq9AIzKiOLOEwcyoU/LOOoVpSu4d/69FNQXAJAekU7fqL7kROXo99mR2YQYQzr/xe2B36diNLVk3n94Yz1blpYyYlo6w4+O5LMnH6auupIZz71OqD0wSZHX7cIcIpnB/bH084/pN34S0Undf5b47mx9xXpunnMzpY2lRJgjeOqop5iSOqWrm3XIqapGflVjU+Z5/7PPA5PsDEpuzkLbiQyV7LMQQgghOo4E1WKPvG4/3/57LYVbqjnjllGk5EQFPe9weXl1bi6vz9+hZ4SO6h/PX08cwJCUQBaywdvAM789w6wts9Bo+xEyKAYyIjLoG9VXD7RzonLIsmdhNnbeia/fr/L9f9aza0MVlzwyUZ9YLH99JbkryxkwPpGUftFomoajvJTIhJbxqh8/dj9+n5djr7iG+MzsPR3isJO3egVLP/+Ys/76AGarXHToKF/nfs3MhTNx+91k2bP4x7H/ICsyq6ub1eEcrpaxz80B9Oa9ZJ9TIq1BmedByXbJPgshhBCiU0hQLfZK9auU5deR1GfPXXXLHC5e/HkrHyzdpa9xffqIFG4/oT+ZsYEJu6pcVWyr3sa2msBte812ttVsw+FxtLtPk2Iiw54RFGjnROWQbk/vkPV2G2rd1JQ2ktq/ZU3qjx5fRvnOOqZdMYgBE5L3az91VRX854ar0FSVP774GpEJib+7bb2Bz+Ph9Vv+RH1lBRPOuZDJ513c1U3q8fyqnxdWvsCb694E4Mi0I/nb1L8RYYno4pb9Pq2zz5uaum5vKnFQUN1+9jnEZGBgUkv2eWCynUGSfRZCCCFEF5KgWhyQhho38z7awpEXDGizTFReRQPP/rCFL1utcX3R+AxuPLYf8RFtu3hrmkaFs6JNoL29Zru+RuzuTAYTWfasoEC7b1Rf0iPSMRqM7W6zu5LcWmY9vRxbuJkrnpyCoSmTVbilGovVRFx6+AGNS62rrGDXhrUMnnqMXrbo4/9RVVTAlAsu1bPaZXm5bF26kJiUNAZNOVqvu3XpQnxuN5nDRxEaGQUEZsWuLi7CGhZObFq6XtdZF7gIYbGF6jNod1f5a1exbdkijrx4hnSP/50cHgd3/non8wvnA3DVsKu4YeQN+/2Z7y6Cs8+B+/3JPg9s1XU7O06yz0IIIYToXiSoFgfkixdXsWtDFRlDYjntxhHt1llXWMtT323m16Y1rkMtRmZMzmJinziSo6wkR1r3us6rpmmUNpa2CbS312yn0dfY7jYWg4XsyGxyopsC7chAhjusMZpty8qISgql/xGB4NbvV3nzjvlEJYZy0jXDCIvq2DHdzvo6Xr3uCnxuN5c++SIJWX0Cv5c5P/DdKy+QPXIMZ9/9kF7/9ZuvpqakmAseeorUgYMB2LxoPl89/zfSBg3l/Af/ptd9586bKM/L5Zy7HyJr5Bgg0M36y78/QVLffvzh/sf1urP/+Xcqdu3kqEtmkD5kOAAVu/JZ9PH/sMcncNQlV+p11/78PY7yUvpPmKJ3YW901LJ9+RKsYeH0GzdJr1uxMw93YyPRySn6RQCfx8PSz/+PtEFDyRja/udCHJzc2lxu/vlm8hx5WI1WHp78MCdln9TVzdqr5uzzpuKWicP2lX0ekBTBoKTWAXQEUaGyvrsQQgghur/9jUO7d0pMdJqp5/Xj53c2cuQF/fdYZ2hqJO9cOY6F2yt4cvZmVu+q4eU523l5zna9TqTNTHJkIMBOirTpj5MjbU2BdxxTUpOCJl9SNZXihuKgQHtbzTZya3Jx+V1srt7M5qotKICmBK4BjSo9lvG5Z+BOqCbKVqtnti95dALWQ3TCbrHaOOm6W3GUlxEe0zJxW3RKGiOnn0JsakZQ/dQBQ7DHJ2KNaOnGa7KYiUpMDtoeQPX5ADCaW7q6ej1uPE4nXo8nqG5lwU5Kc7ficbn0sobqarYsnk9cRlZQUL1x/i/sWr+G2PRMPaiuKSni+1deJDIxKSionve/t8ldsYwTrrmJYceeAEBVUQGLPv4f4bFxzHjuX1istgP7pYl2/VrwK3f+eif13nqSwpJ44ZgXGBw7uKubFaTO5W2zbNW+ss8Dk5u6bicFxkBL9lkIIYQQhwPJVAudpmlB3aNryhqJjLe122U6sMZ1CR8s20VBtZPiGicNnvZPtndnt5pIibKR1BxsR1pJirSSEhkoS4kKZLxVTaWwrpCFn2+lYqWP8nFrWB+2hNyaXMzOUKbsOJfc2FVsjf9N37fNZAuahbz5PjE0sdsvSaSqfhQUFENglnKvy0V9TRUGgzFoTHfRlk246utI7JNDWFRg7Lijooztvy0hJCw8qLv6ytlfUlVUyNBjjicxuy8QyGrPe/8tQiOjmH7tzXrd71/9B7vWr+HIi2fowXZp7jY+e+phpl50BYOmHtPtf4fdnaZpvLHuDV5Y8QIaGqMTRvPs0c8SZ4s7pMfUNNCaHqsaaDSVNT0uc7hbJg5rCqT3lX0OWrpKss9CCCGE6IWk+7f4Xcp31fHJ08vpNzaRoy4aELQc1Z44XF5Kal0U1TgD97UuSmqdFNe6Arf9CLzNGqT6DFRFGPTs9qBCH+EFLqyDIul/cgYJEWZUYyVFjXl6ZntrzVbyHHn4VF+7+w03hwcF2n2j+tIvqh9xtjgJFA8jLq+fmkYvVQ0eaho9VDV6qG70Ut3gobrRQ3WDhxqnF59fQ0NDVdsGoKrWFKgCaiBSbXrcUk9t+rPa/FgD/JqbuvD3cVuXA2BpnISt7hw01dRUNzjgDcwNuNvx1Ka59lvtVwt63BJAN7fn90huHvvcKoDOig3FZNz33wMhhBBCiJ5Oun+L36V8Zx1+r0p9tYv9jTntVjN2q5n+iXuetbg58G4OsgMBd+C+vNrJ9O1+LJrCqwYXm111bC6tY61fISJMYWdRCerrJa2OZyI5chBJkaMYGGXlyCQzltAqfIZi6tUCytz55DlyyXfkU++tZ3X5alaXrw5us8UeFGg3T5IWa4vdvemim3F6/IGguDkg3i04rm70NpV7qG4IPG7cz94UHU0x1WBL+y9GayGaZsBdcjp1NRMAb9Ota4WYDPRPjAha83lQsmSfhRBCCCH2h2SqxR7t2lRFQkYEIYdgSRtnvYdtv5XhcfkYc2KWXv7Z31dQW+FiwOmZuKMtlNQ6KapxBQJxRyAQL6l1UeduPyO9uwirieRIM1GRNVhDy9EsJTgpotq7kwpXESpqu9tFh0S3CbRzonKIskZ1wKsXrWmaRoPHv9fguKqxKbPc4G269+D2tf/e7YvRoBAdaiY61BK4hTU9DrMQHWomKtSCxWhAUUBRFBTAoCiBn2kqa3rcXG5oKgiuq2BQYJtjLa9smkmdt5pwcyTXD36YQdEjoen53Y9B63007XtPx1VoqacoTftrXdbOMWizXwgxGWXssxBCCCHEbqT7t+hwa+YUEJlgI3PIwWVxVVXTl7kqya1l1lPLMVuN/PHpqRjNge6k7kYvFptpn12y65q7mrfuYl5zgIG34sVgKScsvIKwiApM1jK8hmIatTICHWjbirXGkh6RTnxoPLHWWOJD44m3xRNniyM+NHAfY43BoBye3WM1TcPh8lGzW3Ac6G7dOjj2tOqG7cXjP7gA2WxU9hgcN5fHhFmICjU33VuwW/f9+eooH2/5mMeWPIZP9TEgegAvHPsCqeGpnXJsIYQQQgjx+0j3b9GhCrdUM++jwAzc5917BHFpe+7ivbuty0pZPjuPPqMSGHdqYAbqxCw7WcPjSM6JxO9X9aB6f7PiEVYzEVYz/fbS1byudVfzpsA7KBCvcVHnTgncKlttqHgwhJRhCCnFYCnDGFKKyVoK5moqXZVUuir3eEwABQMhSiRWQxQ2Q3TTLYpQY7R+CzNFE2aMxmywYFDAYFAwNGUajQZFzzoaFKXpucBjY6vMqMHQdN90Mxqas5VK230qrfbZqjywXat97rYfo6LgU9Xg7HGbbHJLN+uaRi8+9eCu01lMBmJaB8VN9zGhgWC4dXDcHDyHWYzdcky8V/Xy1NKn+GDzBwCckHkCj0x+hFBzaBe3TAghhBBCdDQJqsV+ScqOZODEZIwmA7Gp4Xusp6oaJbm1xKWFY7EGPl4+r0plYQMGY4UeVCsGhVOuG35I27y/gXepw9XSxVwPwFP1QLyuvCnjrbgDwba5BsVUh2Kqw2By6I8VUx2KsQEUFZdWjctfTY1/x17bqPpC0XwRaD47mi8C1RfR9HNE08+BcrSOXXP7ULOZjW0yxDFNXavbC46jQ83YzN0zQD5QVa4qbv/ldn4rDcxKf+OoG7l62NW94rUJIYQQQoi2pPu32G/NMws3d+H2+1S8bj/WsJbs8ifPLKd4Wy0nXDWEfmMDy0C5Grzkr60gc1hcUN2eot7tC+pi7vT6UTUNv6rpMy/7m343Pr+PBn8N9b4qGv3VNPiraWy6OdVqnP4anGo1Lq0Gjf2fNMuAFQuRmLVITERi0gI3o2bHqEZi0CIxqXY0NRRNU9Ca2qSqgfapTTNLN88I7Ve1No+bn1c1LTDLtAZ+TcOgKHsMjlsyysFdsK1m4yF8R7qvzVWbuennmyhqKCLUFMrfpv6NYzKO2feGQgghhBCi25Hu36LDNU+UBIEA+9cPtrBteRl/fHaqHmgn9YmksrABV33LjMbWMDMDJiR3RZM7RHiIiZyECHIS9r/L+76omkqtu5ZyZzkVzgoqnBWUNwYelzvLgx47fU5UXIF/SmlgB3tIepoN5sD47lbjvGNtscTbmsZ+N40Bj7HGYDLI178jfZ/3PfctuA+nz0l6RDovHvMiOdE5Xd0sIYQQQghxiMlZtTgornovOzdU4nH6qCpqIC4t0CV8zElZjD+9z36ta304MygGoq3RRFuj6R/df691G7wNlDeWU+4sp9JZGQi6neVUNFboQXm5s5xady1e1UtxQzHFDcV73aeCQow1Rp9crTkIbw7EWwflIcae1fW8s6mayj9X/ZN/r/k3ABOTJ/L0UU8TGRLZxS0TQgghhBCdQbp/i4PW6PCwdVkp2SPisMfZuro5hz2P36MH2BXOijZBd3P2u9JViart/2zbUSFR9Inso6/n3Xwva3lDvaeeu+ffzS+7fgHgssGXceuYW6UXgBBCCCFELyBLagkh2uVX/VS7q9tmv1t1OW/uju5RPXvcT3RINDnROfSN7BsUcB8ua3nvcuzixp9vZHvtdiwGCzMnzeT0vqd3dbOEEEIIIUQHkaBaCPG7aJqGw+OguKGYbTXb2F6znW0129hWvY3C+kK0vazlnROVEwi4W2W27Zbe891dWLSQO+begcPjIN4Wz/PHPM/w+EM7m70QQgghhOhcElQLIQ4Zp89Jbm1uUKC9vWY7RQ1Fe9wmwZYQCLKjc/RAu29kX8Ite16irbvRNI3/bvgvzy5/FlVTGR43nL8f83cSQhO6umlCCCGEEKKDSVAthOh0Dd4GcmtyWzLbtYGAu7SxdI/bJIUl0TeqL/2i+umZ7T6RfQg1h3Ziy/fN7Xfz8KKH+WL7FwCc3vd0Hpj4gEzkJoQQQgjRS0lQLYToNuo8dWyv2a5ntpvvy53le9wmNTw1kM1uFXD3ieyD1WTtxJYHlDWWceucW1lTsQaDYuAvY//CJYMuQVH2sLaZEEIIIYTo8SSoFkJ0e7Xu2qBAe3vNdrbWbKXKVdVufQWFtIi0NpntrMisQ5YxXlO+hlvm3EK5sxy7xc4zRz3DxJSJh+RYQgghhBCi+5CgWgjRY1W7qoMy2s33Ne6adusbFAMZERltMttZ9izMRvNBt+OL7V/w0MKH8Kge+kb25cVjXyTDnnHQ+xNCCCGEED2HBNVCiF5F0zQqXZXtZrbrPHXtbmNSTGTYM/SMdvMt3Z6O2bDnYNun+nhu+XP8d8N/ATgm/RiemPoEYeawQ/LahBBCCCFE9yNBtRDisKBpGuXO8qBAe1vNNrbVbKPB29DuNiaDiSx7lj4LeXNmOz0inXpvPXfMvYNFxYsAuGb4NVw38joMiqEzX5YQQgghhOhiElQLIQ5rmqZR2ljaphv59prtNPoa293GYrBgNVlxeBzYTDYenfwoJ2Sd0MktF0IIIYQQ3cH+xqGmTmyTEEJ0GkVRSApLIiksiSmpU/RyVVMpbihuM147tyYXl9+Fx+MhJSyFF499kQExA7rwFQghhBBCiJ5AgmohxGHFoBhIDU8lNTyVI9OO1MtVTaWwrpDihmKGxA2R8dNCCCGEEGK/SFAthBAEgu10ezrp9vSubooQQgghhOhBZOYdIYQQQgghhBDiIElQLYQQQgghhBBCHCQJqoUQQgghhBBCiIMkQbUQQgghhBBCCHGQJKgWQgghhBBCCCEOkgTVQgghhBBCCCHEQZKgWgghhBBCCCGEOEgHFVS//PLLZGVlYbVaGT9+PEuXLt1r/eeff54BAwZgs9lIT0/n1ltvxeVyHVSDhRBCCCGEEEKI7uKAg+oPP/yQ2267jZkzZ7JixQpGjBjB9OnTKSsra7f++++/z1133cXMmTPZuHEjr7/+Oh9++CH33HPP7268EEIIIYQQQgjRlQ44qH7uuee4+uqrmTFjBoMHD+aVV14hNDSUN954o936CxcuZPLkyVx00UVkZWVxwgkncOGFF+4zuy2EEEIIIYQQQnR3BxRUezweli9fzrRp01p2YDAwbdo0Fi1a1O42kyZNYvny5XoQnZubyzfffMPJJ5+8x+O43W4cDkfQTQghhBBCCCGE6G5MB1K5oqICv99PYmJiUHliYiKbNm1qd5uLLrqIiooKpkyZgqZp+Hw+rr322r12/37iiSd46KGHDqRpQgghhBBCCCFEpzvks3//8ssvPP744/zzn/9kxYoVfPLJJ3z99dc88sgje9zm7rvvpra2Vr/t2rXrUDdTCCGEEEIIIYQ4YAeUqY6Li8NoNFJaWhpUXlpaSlJSUrvb3H///Vx66aVcddVVAAwbNoyGhgb+9Kc/ce+992IwtI3rQ0JCCAkJOZCmCSGEEEIIIYQQne6AMtUWi4UxY8bw008/6WWqqvLTTz8xceLEdrdpbGxsEzgbjUYANE070PYKIYQQQgghhBDdxgFlqgFuu+02Lr/8csaOHcu4ceN4/vnnaWhoYMaMGQBcdtllpKam8sQTTwBw2mmn8dxzzzFq1CjGjx/Ptm3buP/++znttNP04FoIIYQQQgghhOiJDjioPv/88ykvL+eBBx6gpKSEkSNHMnv2bH3ysp07dwZlpu+77z4UReG+++6jsLCQ+Ph4TjvtNB577LGOexVCCCGEEEIIIUQXULQe0Ae7traWqKgodu3ahd1u7+rmCCGEEEIIIYTo5RwOB+np6dTU1BAZGbnHegecqe4KdXV1AKSnp3dxS4QQQgghhBBCHE7q6ur2GlT3iEy1qqoUFRURERGBoihd3Zw9ar6SIRn13kve495P3uPeTd7f3k/e495P3uPeT97j3q+nvMeaplFXV0dKSkq7q1Y16xGZaoPBQFpaWlc3Y7/Z7fZu/eEQv5+8x72fvMe9m7y/vZ+8x72fvMe9n7zHvV9PeI/3lqFudkBLagkhhBBCCCGEEKKFBNVCCCGEEEIIIcRBkqC6A4WEhDBz5kxCQkK6uiniEJH3uPeT97h3k/e395P3uPeT97j3k/e49+tt73GPmKhMCCGEEEIIIYTojiRTLYQQQgghhBBCHCQJqoUQQgghhBBCiIMkQbUQQgghhBBCCHGQJKgWQgghhBBCCCEOkgTVQgghhBBCCCHEQZKgugO9/PLLZGVlYbVaGT9+PEuXLu3qJokO8sQTT3DEEUcQERFBQkICZ555Jps3b+7qZolD5G9/+xuKonDLLbd0dVNEByosLOSSSy4hNjYWm83GsGHD+O2337q6WaKD+P1+7r//frKzs7HZbPTt25dHHnkEWeSk5/r111857bTTSElJQVEUPvvss6DnNU3jgQceIDk5GZvNxrRp09i6dWvXNFYclL29x16vlzvvvJNhw4YRFhZGSkoKl112GUVFRV3XYHFA9vUdbu3aa69FURSef/75TmtfR5KguoN8+OGH3HbbbcycOZMVK1YwYsQIpk+fTllZWVc3TXSAuXPncv3117N48WJ++OEHvF4vJ5xwAg0NDV3dNNHBli1bxr///W+GDx/e1U0RHai6uprJkydjNpv59ttv2bBhA88++yzR0dFd3TTRQZ588kn+9a9/8dJLL7Fx40aefPJJnnrqKf7xj390ddPEQWpoaGDEiBG8/PLL7T7/1FNP8eKLL/LKK6+wZMkSwsLCmD59Oi6Xq5NbKg7W3t7jxsZGVqxYwf3338+KFSv45JNP2Lx5M6effnoXtFQcjH19h5t9+umnLF68mJSUlE5qWceTdao7yPjx4zniiCN46aWXAFBVlfT0dG688UbuuuuuLm6d6Gjl5eUkJCQwd+5cjjzyyK5ujugg9fX1jB49mn/+8588+uijjBw5ssdeMRXB7rrrLhYsWMC8efO6uiniEDn11FNJTEzk9ddf18vOOeccbDYb7777bhe2THQERVH49NNPOfPMM4FAljolJYXbb7+dv/zlLwDU1taSmJjIW2+9xQUXXNCFrRUHY/f3uD3Lli1j3Lhx5Ofnk5GR0XmNE7/bnt7fwsJCxo8fz3fffccpp5zCLbfc0iN7CkqmugN4PB6WL1/OtGnT9DKDwcC0adNYtGhRF7ZMHCq1tbUAxMTEdHFLREe6/vrrOeWUU4K+y6J3+OKLLxg7dix/+MMfSEhIYNSoUbz22mtd3SzRgSZNmsRPP/3Eli1bAFi9ejXz58/npJNO6uKWiUNhx44dlJSUBP29joyMZPz48XLu1YvV1taiKApRUVFd3RTRAVRV5dJLL+WOO+5gyJAhXd2c38XU1Q3oDSoqKvD7/SQmJgaVJyYmsmnTpi5qlThUVFXllltuYfLkyQwdOrSrmyM6yAcffMCKFStYtmxZVzdFHAK5ubn861//4rbbbuOee+5h2bJl3HTTTVgsFi6//PKubp7oAHfddRcOh4OBAwdiNBrx+/089thjXHzxxV3dNHEIlJSUALR77tX8nOhdXC4Xd955JxdeeCF2u72rmyM6wJNPPonJZOKmm27q6qb8bhJUC3GArr/+etatW8f8+fO7uimig+zatYubb76ZH374AavV2tXNEYeAqqqMHTuWxx9/HIBRo0axbt06XnnlFQmqe4mPPvqI9957j/fff58hQ4awatUqbrnlFlJSUuQ9FqKH83q9nHfeeWiaxr/+9a+ubo7oAMuXL+eFF15gxYoVKIrS1c353aT7dweIi4vDaDRSWloaVF5aWkpSUlIXtUocCjfccANfffUVc+bMIS0traubIzrI8uXLKSsrY/To0ZhMJkwmEwyzSq0AAANOSURBVHPnzuXFF1/EZDLh9/u7uonid0pOTmbw4MFBZYMGDWLnzp1d1CLR0e644w7uuusuLrjgAoYNG8all17KrbfeyhNPPNHVTROHQPP5lZx79X7NAXV+fj4//PCDZKl7iXnz5lFWVkZGRoZ+7pWfn8/tt99OVlZWVzfvgElQ3QEsFgtjxozhp59+0stUVeWnn35i4sSJXdgy0VE0TeOGG27g008/5eeffyY7O7urmyQ60HHHHcfatWtZtWqVfhs7diwXX3wxq1atwmg0dnUTxe80efLkNsvgbdmyhczMzC5qkehojY2NGAzBpzVGoxFVVbuoReJQys7OJikpKejcy+FwsGTJEjn36kWaA+qtW7fy448/Ehsb29VNEh3k0ksvZc2aNUHnXikpKdxxxx189913Xd28AybdvzvIbbfdxuWXX87YsWMZN24czz//PA0NDcyYMaOrmyY6wPXXX8/777/P559/TkREhD5eKzIyEpvN1sWtE79XREREm/HxYWFhxMbGyrj5XuLWW29l0qRJPP7445x33nksXbqUV199lVdffbWrmyY6yGmnncZjjz1GRkYGQ4YMYeXKlTz33HNceeWVXd00cZDq6+vZtm2b/vOOHTtYtWoVMTExZGRkcMstt/Doo4/Sr18/srOzuf/++0lJSdnr7NGie9nbe5ycnMy5/9/e/eIoE8NhAB7DDBoDakgwINBYBFdYzxHmAIQzIHAYDgF+LAqP4g44BPzWfWKz3/7pzu6Y50nq36Rp2rdJ05eX7Hw+Z8fjMXs8Hv/OX71eL8vzvK3YfNFna/jtJUmn08kGg0E2Ho//OurPBY3ZbrdRlmXkeR6z2SxOp1PbkWhIlmXvjv1+33Y0fsl8Po+qqtqOQYMOh0NMp9MoiiImk0nsdru2I9Gg2+0WVVVFWZbR7XZjNBrFarWK+/3edjQS1XX97t67XC4jIuL5fMZ6vY5+vx9FUcRisYjL5dJuaL7lozm+Xq//PX/Vdd12dL7gszX81nA4jM1m86cZm+KfagAAAEjkTTUAAAAkUqoBAAAgkVINAAAAiZRqAAAASKRUAwAAQCKlGgAAABIp1QAAAJBIqQYAAIBESjUAAAAkUqoBAAAgkVINAAAAiV4B6FJEOmDmRMUAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "for name, values in losses.items():\n",
        "  if '2D 3D' not in name:\n",
        "    linestyle = ':' if '3D' in name else '-'  # Dotted if \"3D\" in name, solid otherwise\n",
        "    plt.plot(values, label=name, linestyle=linestyle)\n",
        "\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss Over Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qaKT5NtRBx_I",
      "metadata": {
        "id": "qaKT5NtRBx_I"
      },
      "source": [
        "- Next step is to define some alternate ways of combining the attentions\n",
        "- Write the testing set\n",
        "- Write the Validation set"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AsCJvylNvk1o",
      "metadata": {
        "id": "AsCJvylNvk1o"
      },
      "source": [
        "# Fluorescence Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5BbZJPVawX62",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BbZJPVawX62",
        "outputId": "fd28390e-2ddb-4996-c0dc-009a5a75b7ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tape-proteins\n",
            "  Downloading tape_proteins-0.5-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from tape-proteins) (4.67.1)\n",
            "Collecting tensorboardX (from tape-proteins)\n",
            "  Downloading tensorboardx-2.6.4-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from tape-proteins) (1.16.2)\n",
            "Collecting lmdb (from tape-proteins)\n",
            "  Downloading lmdb-1.7.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Collecting boto3 (from tape-proteins)\n",
            "  Downloading boto3-1.40.39-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from tape-proteins) (2.32.4)\n",
            "Collecting biopython (from tape-proteins)\n",
            "  Downloading biopython-1.85-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from tape-proteins) (3.19.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from biopython->tape-proteins) (2.0.2)\n",
            "Collecting botocore<1.41.0,>=1.40.39 (from boto3->tape-proteins)\n",
            "  Downloading botocore-1.40.39-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3->tape-proteins)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.15.0,>=0.14.0 (from boto3->tape-proteins)\n",
            "  Downloading s3transfer-0.14.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->tape-proteins) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->tape-proteins) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->tape-proteins) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->tape-proteins) (2025.8.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorboardX->tape-proteins) (25.0)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.12/dist-packages (from tensorboardX->tape-proteins) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.12/dist-packages (from botocore<1.41.0,>=1.40.39->boto3->tape-proteins) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.41.0,>=1.40.39->boto3->tape-proteins) (1.17.0)\n",
            "Downloading tape_proteins-0.5-py3-none-any.whl (68 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m68.9/68.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading biopython-1.85-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m110.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading boto3-1.40.39-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lmdb-1.7.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (302 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m303.0/303.0 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboardx-2.6.4-py3-none-any.whl (87 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading botocore-1.40.39-py3-none-any.whl (14.0 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m147.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading s3transfer-0.14.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lmdb, tensorboardX, jmespath, biopython, botocore, s3transfer, boto3, tape-proteins\n",
            "Successfully installed biopython-1.85 boto3-1.40.39 botocore-1.40.39 jmespath-1.0.1 lmdb-1.7.3 s3transfer-0.14.0 tape-proteins-0.5 tensorboardX-2.6.4\n"
          ]
        }
      ],
      "source": [
        "!pip install tape-proteins"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "C2gzo4k9vzeP",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2gzo4k9vzeP",
        "outputId": "ee0e01c0-7a03-4d16-e1eb-e7ed1d3b272d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sequences for training set: 21446\n",
            "Number of sequences for validation set: 5362\n",
            "Number of sequences for test set: 27217\n"
          ]
        }
      ],
      "source": [
        "from tape.datasets import LMDBDataset\n",
        "\n",
        "# Change the path below to the dataset you want to inspect\n",
        "dataset_train = LMDBDataset(\"/content/drive/My Drive/BioTransformer/TAPE /fluorescence/fluorescence_train.lmdb\")\n",
        "dataset_test = LMDBDataset(\"/content/drive/My Drive/BioTransformer/TAPE /fluorescence/fluorescence_test.lmdb\")\n",
        "dataset_valid = LMDBDataset(\"/content/drive/My Drive/BioTransformer/TAPE /fluorescence/fluorescence_valid.lmdb\")\n",
        "\n",
        "\n",
        "\n",
        "# Show size\n",
        "print(\"Number of sequences for training set:\", len(dataset_train))\n",
        "print(\"Number of sequences for validation set:\", len(dataset_valid))\n",
        "print(\"Number of sequences for test set:\", len(dataset_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bsUxbxEmv0gY",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsUxbxEmv0gY",
        "outputId": "f295ed14-31c9-4c87-b106-fae956eee958"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 512])\n",
            "torch.Size([16, 1])\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import Dataset\n",
        "from tape.tokenizers import TAPETokenizer\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Setup tokenizer\n",
        "tokenizer = TAPETokenizer(vocab='iupac')\n",
        "\n",
        "# Dataset Class\n",
        "class FluorescenceDataset(Dataset):\n",
        "    def __init__(self, lmdb_dataset):\n",
        "        self.data = lmdb_dataset\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        entry = self.data[idx]\n",
        "        sequence = entry['primary']\n",
        "        target = entry['log_fluorescence']\n",
        "        token_ids = tokenizer.encode(sequence)\n",
        "        return torch.tensor(token_ids, dtype=torch.long), torch.tensor(target, dtype=torch.float32)\n",
        "\n",
        "# Collate function with padding to 512\n",
        "FIXED_LEN_SEQ = 512\n",
        "\n",
        "def collate_fn(batch):\n",
        "    def pad_tensor(t, pad_value):\n",
        "        return torch.nn.functional.pad(t, (0, FIXED_LEN_SEQ - t.size(0)), value=pad_value)\n",
        "\n",
        "    sequences, targets = zip(*batch)\n",
        "    input_ids = torch.stack([pad_tensor(seq, pad_value=0) for seq in sequences])\n",
        "    targets = torch.stack(targets)\n",
        "\n",
        "    return {\n",
        "        'input_ids': input_ids,\n",
        "        'targets': targets\n",
        "    }\n",
        "\n",
        "\n",
        "# Dataloaders\n",
        "train_loader = DataLoader(FluorescenceDataset(dataset_train), batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
        "valid_loader = DataLoader(FluorescenceDataset(dataset_valid), batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(FluorescenceDataset(dataset_test), batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# Print a batch\n",
        "batch = next(iter(train_loader))\n",
        "print(batch['input_ids'].shape)       # [32, 512]\n",
        "print(batch['targets'].shape)         # [32]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fNssp40108r6",
      "metadata": {
        "id": "fNssp40108r6"
      },
      "source": [
        "## Defining the Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OtF_Hb9-08r7",
      "metadata": {
        "id": "OtF_Hb9-08r7"
      },
      "outputs": [],
      "source": [
        "def softmax_5d(X, axis):\n",
        "    \"\"\"\n",
        "    Compute softmax for a 5D tensor along the specified axis.\n",
        "\n",
        "    Parameters:\n",
        "        X (numpy.ndarray or torch.Tensor): Input tensor of shape (sample_size, heads, n, n, n)\n",
        "        axis (int or tuple): Axis along which to apply softmax\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray or torch.Tensor: Softmax-applied tensor of the same shape as X.\n",
        "    \"\"\"\n",
        "    X_exp = torch.exp(X - torch.amax(X, dim=axis, keepdim=True))  # Stability trick\n",
        "    return X_exp / torch.sum(X_exp, dim=axis, keepdim=True)\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, p):\n",
        "        super().__init__()\n",
        "        self.layer1 = nn.Linear(d_model, d_ff)\n",
        "        self.layer2 = nn.Linear(d_ff, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layer2(self.dropout(self.relu(self.layer1(x))))\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, p, dim_feedforward, attn_mechanism, **kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "        # Extract required arguments from kwargs\n",
        "        d_model = kwargs[\"d_model\"]\n",
        "        num_heads = kwargs[\"num_heads\"]\n",
        "        len_seq = kwargs[\"len_seq\"]\n",
        "        self.block_size = kwargs.get('block_size', None)\n",
        "        self.stride = kwargs.get('stride',None)\n",
        "\n",
        "        # Modify len-seq if padding is required\n",
        "        if attn_mechanism in {\"Attn2DMultiPed3Dselected\", \"Multiped2D\", \"MultiLin2D\", \"Attn3DLinformerMultiPed\"}:\n",
        "              remainder = (len_seq - self.block_size) % self.stride\n",
        "              pad_len = (self.stride - remainder) % self.stride\n",
        "              if pad_len > 0:\n",
        "                len_seq = len_seq + pad_len\n",
        "                #print(f'len of seq in enc is changed to {len_seq}')\n",
        "                kwargs[\"len_seq\"] = len_seq  # Update the value in kwargs  # Update the value in kwargs\n",
        "\n",
        "        # Initialize attention mechanism\n",
        "        self.mha = get_attention(attn_mechanism, **kwargs)\n",
        "\n",
        "        # Feedforward and normalization layers\n",
        "        self.ffn = FeedForward(d_model, dim_feedforward, p)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(p)\n",
        "\n",
        "    def forward(self, x,mask = None):\n",
        "        attn_output = self.dropout(self.mha(x,mask))\n",
        "        x = self.norm1(x + attn_output)\n",
        "        ffn_output = self.dropout(self.ffn(x))\n",
        "        x = self.norm2(x + ffn_output)\n",
        "        return x\n",
        "\n",
        "\n",
        "class FluorescencePredict(nn.Module):\n",
        "    def __init__(self, vocab_size, num_layers=2, p=0.2, dim_feedforward=32, attn_mechanism=\"Plain3D\", **kwargs):\n",
        "        super().__init__()\n",
        "        self.kwargs = kwargs\n",
        "        d_model = kwargs[\"d_model\"]\n",
        "        self.attn_mechanism = attn_mechanism\n",
        "        len_seq = kwargs[\"len_seq\"]\n",
        "        self.block_size = kwargs.get('block_size', None)\n",
        "        self.stride = kwargs.get('stride',None)\n",
        "\n",
        "        # Encoder and Attention\n",
        "        self.encoder_layers = nn.ModuleList([\n",
        "            Encoder(p=p, dim_feedforward=dim_feedforward, attn_mechanism=attn_mechanism, **kwargs)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Modify the len-seq based on padding required\n",
        "        if attn_mechanism in {\"Attn2DMultiPed3Dselected\", \"Multiped2D\", \"MultiLin2D\", \"Multiped3D\", \"Attn3DLinformerMultiPed\", \"Attn2DMultiPed3Dselected_Dual\"}:\n",
        "          # how much we need to pad\n",
        "          remainder = (len_seq - self.block_size) % self.stride\n",
        "          pad_len = (self.stride - remainder) % self.stride\n",
        "          if pad_len > 0:\n",
        "            len_seq = len_seq + pad_len\n",
        "            #print(f'len of seq var in transformer is changed to {len_seq}')\n",
        "            kwargs[\"len_seq\"] = len_seq  # Update the value in kwargs\n",
        "\n",
        "        # Feed Forward Layer\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(), # shape: (batch_size, len_seq * len_emb)\n",
        "            nn.Linear(len_seq * d_model, dim_feedforward),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(dim_feedforward, 1)\n",
        "            )\n",
        "\n",
        "        # AA embedding layer\n",
        "        self.token_embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
        "\n",
        "        # Positional Embeddings\n",
        "        self.position_embedding = nn.Embedding(len_seq, d_model)\n",
        "\n",
        "    def sliding_blocks(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x (Tensor): Input tensor of shape (batch, seq_len, d_model).\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Reshaped tensor of shape (batch, num_blocks, l, d_model).\n",
        "        \"\"\"\n",
        "        l = self.block_size\n",
        "        d = self.stride\n",
        "        batch, seq_len = x.shape\n",
        "\n",
        "        # unfold along the sequence dimension\n",
        "        blocks = x.unfold(dimension=1, size=l, step=d)   # (batch, num_blocks, l, d_model)\n",
        "\n",
        "        return blocks\n",
        "\n",
        "    def generate_padding_mask(self, input_seq):\n",
        "      seq_len = input_seq.shape[1]\n",
        "      batch_size = input_seq.shape[0]\n",
        "      mask = input_seq != 0 # shape = [bs, len_seq]\n",
        "      # if self.num_peds:\n",
        "      #   len_ped = seq_len // self.num_peds\n",
        "      if self.attn_mechanism == \"Multiped3D\":\n",
        "        return self.sliding_blocks(input_seq !=0).unsqueeze(2).unsqueeze(2).unsqueeze(2)\n",
        "      elif self.attn_mechanism == \"Multiped2D\":\n",
        "        return self.sliding_blocks(input_seq !=0).unsqueeze(2).unsqueeze(2)\n",
        "      elif self.attn_mechanism == 'Linformer2D':\n",
        "        return mask.unsqueeze(1).unsqueeze(3)\n",
        "      elif self.attn_mechanism == 'Plain2D':\n",
        "        #print(f'shape of the mask: {mask.shape}')\n",
        "        return mask.unsqueeze(1).unsqueeze(1)\n",
        "      elif self.attn_mechanism == 'Plain3D':\n",
        "        return mask.unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
        "      elif self.attn_mechanism == 'Linformer3D':\n",
        "        return mask.unsqueeze(1).unsqueeze(3).unsqueeze(3)\n",
        "      elif self.attn_mechanism == 'MultiLin2D':\n",
        "        return self.sliding_blocks(input_seq !=0).unsqueeze(2).unsqueeze(4)\n",
        "      elif self.attn_mechanism == \"Attn3DLinformerMultiPed\":\n",
        "        return self.sliding_blocks(input_seq !=0).unsqueeze(2).unsqueeze(4).unsqueeze(4)\n",
        "      elif self.attn_mechanism == \"Combined2D3D\":\n",
        "        mask2d = mask.unsqueeze(1).unsqueeze(1)\n",
        "        mask3d = mask.unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
        "        return tuple([mask2d,mask3d])\n",
        "      elif self.attn_mechanism == \"Attn2D3DLin\":\n",
        "        mask2d = mask.unsqueeze(1).unsqueeze(1)\n",
        "        mask3d =  mask.unsqueeze(1).unsqueeze(3).unsqueeze(3)\n",
        "        return tuple([mask2d,mask3d])\n",
        "      elif self.attn_mechanism == \"Attn2D3D_MultiPed\":\n",
        "        mask2d = mask.unsqueeze(1).unsqueeze(1)\n",
        "        mask3d = mask.reshape(batch_size, self.num_peds, len_ped).unsqueeze(2).unsqueeze(2).unsqueeze(2)\n",
        "        return tuple([mask2d,mask3d])\n",
        "      elif self.attn_mechanism == \"Attn2D3D_MultiPedLin\":\n",
        "        mask2d = mask.unsqueeze(1).unsqueeze(1)\n",
        "        mask3d = mask.reshape(batch_size, self.num_peds, len_ped).unsqueeze(2).unsqueeze(4).unsqueeze(4)\n",
        "        return tuple([mask2d,mask3d])\n",
        "      elif self.attn_mechanism == \"Attn2D3DLinAlter\":\n",
        "        mask2d = mask.unsqueeze(1).unsqueeze(1)\n",
        "        mask3d = mask.unsqueeze(1).unsqueeze(3).unsqueeze(3)\n",
        "        return tuple([mask2d,mask3d])\n",
        "      elif self.attn_mechanism == \"Attn2D3D_MultiPedLinAlter\":\n",
        "        mask2d = mask.unsqueeze(1).unsqueeze(1)\n",
        "        mask3d = mask.reshape(batch_size, self.num_peds, len_ped).unsqueeze(2).unsqueeze(4).unsqueeze(4)\n",
        "        return tuple([mask2d,mask3d])\n",
        "      elif self.attn_mechanism == \"Attn2D3D_MultiPedAlter\":\n",
        "        mask2d = mask.unsqueeze(1).unsqueeze(1)\n",
        "        mask3d = mask.reshape(batch_size, self.num_peds, len_ped).unsqueeze(2).unsqueeze(2).unsqueeze(2)\n",
        "        return tuple([mask2d,mask3d])\n",
        "      elif self.attn_mechanism == \"Attn2DMultiPed3Dselected\":\n",
        "        return self.sliding_blocks(input_seq !=0)\n",
        "\n",
        "    def pad_to_sliding_blocks(self, x, pad_value=0):\n",
        "        \"\"\"\n",
        "        Pad sequence length so it divides evenly into sliding-window blocks.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): (B, L, D) input sequence\n",
        "            block_size (int): size of each block (l)\n",
        "            stride (int): sliding window step (d)\n",
        "            pad_value (float, optional): value to use for padding. Defaults to 0.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: padded sequence of shape (B, L', D), where\n",
        "                    (L' - block_size) % stride == 0\n",
        "            int: number of padding tokens added\n",
        "        \"\"\"\n",
        "        B, L = x.shape\n",
        "\n",
        "        # how much we need to pad\n",
        "        remainder = (L - self.block_size) % self.stride\n",
        "        pad_len = (self.stride - remainder) % self.stride\n",
        "        #print(f'pad_len : {pad_len}')\n",
        "        if pad_len > 0:\n",
        "            pad_tensor = x.new_full((B, pad_len), pad_value)\n",
        "            x = torch.cat([x, pad_tensor], dim=1)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "      #print(f'entering the transforer florecence prediction')\n",
        "      B, L = input_ids.shape\n",
        "      is_multiped = self.attn_mechanism in {\"Attn3DLinformerMultiPed\", \"MultiLin2D\", \"Attn2D3D_MultiPed\", \"Attn2D3D_MultiPedLin\", \"Attn2DMultiPed3Dselected\", \"Multiped2D\", \"Multiped3D\"}\n",
        "      #print(f'is_multiped: {is_multiped}')\n",
        "      # Add Padding if multiped\n",
        "      if is_multiped:\n",
        "        if self.attn_mechanism in {\"Attn2DMultiPed3Dselected\", \"Multiped2D\", \"MultiLin2D\", \"Multiped3D\", \"Attn3DLinformerMultiPed\"}:\n",
        "          # how much we need to pad\n",
        "          remainder = (L - self.block_size) % self.stride\n",
        "          pad_len = (self.stride - remainder) % self.stride\n",
        "          #print(f'number of pads needed: {pad_len}')\n",
        "          if pad_len > 0:\n",
        "            input_ids = F.pad(input_ids, (0, pad_len), value=0)\n",
        "            L = L + pad_len\n",
        "\n",
        "      # Create the mask internally\n",
        "      masks = self.generate_padding_mask(input_ids)\n",
        "      # Create placeholder for positions in the sequence\n",
        "      pos_ids = torch.arange(L, device=input_ids.device).unsqueeze(0).expand(B, L)\n",
        "      # Embedding of AAs + Positional Encoding\n",
        "      x = self.token_embedding(input_ids) + self.position_embedding(pos_ids)\n",
        "      for layer in self.encoder_layers:\n",
        "        x = layer(x, mask=masks)\n",
        "      #print(f'shape of output of encoder layers : {x.shape}')\n",
        "      # Apply the classifier and squeeze the output\n",
        "      return self.classifier(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0I4VTTUj9bEE",
      "metadata": {
        "id": "0I4VTTUj9bEE"
      },
      "source": [
        "### 2D Attention Mechanisms"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nzMF2b7M9bEE",
      "metadata": {
        "id": "nzMF2b7M9bEE"
      },
      "source": [
        "#### Attention 2D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xKBeTN609bEE",
      "metadata": {
        "id": "xKBeTN609bEE"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttn2D(nn.Module):\n",
        "\n",
        "    def __init__(self,num_heads, d_model):\n",
        "        super(MultiHeadAttn2D,self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_params = d_model//num_heads\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, d_model) # mapping to the same number of features\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # Check if the number of len_emb (d_model) is divisable by num_heads\n",
        "        assert d_model % num_heads == 0, \"d_model is not divisable by the number of heads\"\n",
        "\n",
        "\n",
        "    def self_attention(self, q, k, v, mask):\n",
        "\n",
        "        # Calculate the dot produce of Q and K\n",
        "        dotqk = torch.matmul(q, k.transpose(-2,-1))/(self.head_params**0.5) # shape = (#samples, num_heads, len_seq, len_seq)\n",
        "        # Apply the mask if givne\n",
        "        if mask is not None:\n",
        "            dotqk = dotqk.masked_fill(mask == 0, -1e9)\n",
        "            #print(f'mask is applied, shape: {mask.shape}')\n",
        "            #print(f'shape of dot product: {dotqk.shape}')\n",
        "        # Apply the Softmax to the attention weights\n",
        "        attention_scores = torch.softmax(dotqk, dim = -1)\n",
        "        # Multiply to the value matrix\n",
        "        result = torch.matmul(attention_scores, v)\n",
        "        return(attention_scores,result)\n",
        "\n",
        "    def split_heads(self, q):\n",
        "\n",
        "        \"\"\"\n",
        "        Reshaping\n",
        "        Input of shape : (#samples, seq_len, d_model)\n",
        "\n",
        "        to\n",
        "        output of shape: (#samples, num_heads, seq_len, head_parameters)\n",
        "        \"\"\"\n",
        "        samples, seq_len, _ = q.shape\n",
        "        return q.view(samples, seq_len, self.num_heads, self.head_params).transpose(1,2)\n",
        "\n",
        "    def forward(self, q, mask=None):\n",
        "        k = q.clone()\n",
        "        v = q.clone()\n",
        "\n",
        "        # Define Query, Key, Value\n",
        "        Query = self.split_heads(self.W_q(q))\n",
        "        Key  = self.split_heads(self.W_k(k))\n",
        "        Value = self.split_heads(self.W_v(v))\n",
        "\n",
        "        # Run the self-attention\n",
        "        attention_scores, attn_output = self.self_attention(Query, Key,Value, mask)\n",
        "\n",
        "        # Concatenate the heads\n",
        "        batch_size, _,seq_len, _= attn_output.shape\n",
        "        attn_output = attn_output.transpose(1,2).contiguous().view(batch_size, seq_len, self.d_model) #shape = (#samples, len_seq, d_model)\n",
        "\n",
        "        # Apply through the last linear layer\n",
        "        result_final = self.W_o(attn_output) #shape = (#samples, len_seq, d_model)\n",
        "        #print(f'shape of the result final: {result_final.shape}')\n",
        "\n",
        "        return result_final\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BSDkIuMZ9bEE",
      "metadata": {
        "id": "BSDkIuMZ9bEE"
      },
      "source": [
        "#### Attention 2D Multi-Ped"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LqZXwUuB9bEE",
      "metadata": {
        "id": "LqZXwUuB9bEE"
      },
      "outputs": [],
      "source": [
        "class Attn2D_MultiPed(nn.Module):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,num_heads, d_model, stride = 10, block_size = 40):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_params = d_model//num_heads\n",
        "        self.stride = stride\n",
        "        self.block_size = block_size\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, d_model) # mapping to the same number of features\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def self_attention(self, q, k, v, mask):\n",
        "        # Calculate the dot produce of Q and K\n",
        "        dotqk = torch.matmul(q, k.transpose(-2,-1))/(self.head_params**0.5) # shape = (#samples, num_heads, len_seq, len_seq)\n",
        "        #print(f'shape of dot-product: {dotqk.shape}')\n",
        "        # Apply the mask if givne\n",
        "        if mask is not None:\n",
        "            dotqk = dotqk.masked_fill(mask == 0, -1e9)\n",
        "            #print(f'mask applied in the new sliding block multiped2d')\n",
        "        # Apply the Softmax to the attention weights\n",
        "        attention_scores = torch.softmax(dotqk, dim = -1)\n",
        "        # Multiply to the value matrix\n",
        "        result = torch.matmul(attention_scores, v)\n",
        "        return(result)\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        # Shape: (batch, seq_len, d_model) --> (batch, num_heads, seq_len, head_dim)\n",
        "        bsz, num_peds, len_peds, d_model = x.size()\n",
        "        return x.reshape(bsz, num_peds, len_peds, self.num_heads, self.head_params).transpose(2,3)\n",
        "\n",
        "    def _sliding_blocks(self, x):\n",
        "        \"\"\"\n",
        "        Create sliding-window blocks from the sequence.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): (B, L, D).\n",
        "\n",
        "        Returns:\n",
        "            Tensor: (B, num_blocks, block_size, D).\n",
        "        \"\"\"\n",
        "        l = self.block_size\n",
        "        d = self.stride\n",
        "        B, L, D = x.shape\n",
        "\n",
        "        num_blocks = (L - l) // d + 1\n",
        "\n",
        "        # Compute new shape and strides\n",
        "        shape = (B, num_blocks, l, D)\n",
        "        strides = (\n",
        "            x.stride(0),        # batch stride\n",
        "            d * x.stride(1),    # jump d steps for each block\n",
        "            x.stride(1),        # step 1 inside a block\n",
        "            x.stride(2),        # feature stride\n",
        "        )\n",
        "\n",
        "        blocks = x.as_strided(shape, strides)\n",
        "        return blocks.contiguous()\n",
        "\n",
        "    def _reconstruct_from_blocks(self, blocks, seq_len):\n",
        "        \"\"\"\n",
        "        Reconstruct sequence from overlapping sliding blocks.\n",
        "\n",
        "        Args:\n",
        "            blocks (Tensor): (B, num_blocks, block_size, D).\n",
        "            seq_len (int): Original sequence length.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: (B, seq_len, D).\n",
        "        \"\"\"\n",
        "        l = self.block_size\n",
        "        d = self.stride\n",
        "        B, num_blocks, _, D = blocks.shape\n",
        "\n",
        "        device = blocks.device\n",
        "\n",
        "        # Build index map: (num_blocks, block_size) -> positions in seq_len\n",
        "        start_idx = torch.arange(num_blocks, device=device) * d\n",
        "        block_offsets = torch.arange(l, device=device)\n",
        "        positions = start_idx[:, None] + block_offsets[None, :]  # (num_blocks, block_size)\n",
        "\n",
        "        # Expand to batch and feature dims\n",
        "        positions = positions.unsqueeze(0).expand(B, -1, -1)  # (B, num_blocks, block_size)\n",
        "\n",
        "        # Flatten everything for scatter\n",
        "        flat_pos = positions.reshape(B, -1)  # (B, num_blocks * block_size)\n",
        "        flat_blocks = blocks.reshape(B, -1, D)  # (B, num_blocks * block_size, D)\n",
        "\n",
        "        # Allocate output\n",
        "        out = torch.zeros(B, seq_len, D, device=device)\n",
        "        counts = torch.zeros(B, seq_len, 1, device=device)\n",
        "\n",
        "        # Scatter add the block values into the right positions\n",
        "        out.scatter_add_(1, flat_pos.unsqueeze(-1).expand(-1, -1, D), flat_blocks)\n",
        "        counts.scatter_add_(1, flat_pos.unsqueeze(-1), torch.ones_like(flat_pos, device=device).unsqueeze(-1).float())\n",
        "\n",
        "        return out / counts\n",
        "\n",
        "    def forward(self, q,mask = None):\n",
        "        k = q.clone()\n",
        "        v = q.clone()\n",
        "        # Define Query, Key, Value\n",
        "        Query = self.split_heads(self._sliding_blocks(self.W_q(q)))\n",
        "        Key  = self.split_heads(self._sliding_blocks(self.W_k(k)))\n",
        "        Value = self.split_heads(self._sliding_blocks(self.W_v(v)))\n",
        "\n",
        "        # Run the self-attention\n",
        "        attn_output = self.self_attention(Query, Key, Value, mask = mask)\n",
        "        #print(f'shape of the attn outputs:{attn_output.shape}')\n",
        "\n",
        "        batch_size, num_peds, num_heads, len_ped ,len_head = attn_output.shape\n",
        "        attn_output = attn_output.contiguous().view(batch_size, num_peds, len_ped, self.d_model)\n",
        "        #print(f'shape of attention output after combining heads {attn_output.shape}')\n",
        "\n",
        "        # Combine peds -> (batch_size, len_peds * num_peds, d_model)\n",
        "        attn_output = self._reconstruct_from_blocks(attn_output, q.shape[1])\n",
        "\n",
        "        # Apply through the last linear layer\n",
        "        result_final = self.W_o(attn_output) #shape = (#samples, len_seq, d_model)\n",
        "\n",
        "        return result_final"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wa7-qpLi9bEE",
      "metadata": {
        "id": "wa7-qpLi9bEE"
      },
      "source": [
        "#### 2D Attention Linformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "G3aBd8Ug9bEE",
      "metadata": {
        "id": "G3aBd8Ug9bEE"
      },
      "outputs": [],
      "source": [
        "class Attn2DLinformer(nn.Module):\n",
        "    \"\"\"\n",
        "    Modified Multi-Head Attention with 3D product using E, F, G projections.\n",
        "\n",
        "    Args:\n",
        "        num_heads (int): Number of attention heads\n",
        "        d_model (int): Input dimension\n",
        "        k (int): Latent projection size for E, F, G\n",
        "        len_seq (int): Sequence length\n",
        "\n",
        "        Note that the length of the sequence must be inputted correctly here.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_heads, d_model, k, len_seq):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        self.head_dim = d_model // num_heads\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.E = nn.Linear(len_seq, k)\n",
        "        self.F = nn.Linear(len_seq, k)\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        B, L, D = x.size()\n",
        "        x = x.view(B, L, self.num_heads, self.head_dim)\n",
        "        return x.transpose(1, 2)  # [B, H, L, D]\n",
        "\n",
        "    def self_attention(self, q, k, v, mask):\n",
        "\n",
        "        # Calculate the dot produce of Q and K\n",
        "        dotqk = torch.matmul(q, k.transpose(-2,-1))/(self.head_dim**0.5) # shape = (#samples, num_heads, len_seq, len_seq)\n",
        "        # Apply the mask if givne\n",
        "        if mask is not None:\n",
        "            dotqk = dotqk.masked_fill(mask == 0, -1e9)\n",
        "            #print(f'mask applied in the linformer')\n",
        "        # Apply the Softmax to the attention weights\n",
        "        attention_scores = torch.softmax(dotqk, dim = -1)\n",
        "        # Multiply to the value matrix\n",
        "        result = torch.matmul(attention_scores, v)\n",
        "        return(result)\n",
        "\n",
        "\n",
        "    def forward(self, q,mask = None):\n",
        "        k = q.clone()\n",
        "        v = q.clone()\n",
        "        Q = self.split_heads(self.W_q(q))\n",
        "\n",
        "        def project(x, linear_proj, W):\n",
        "            x = W(x).permute(0, 2, 1)\n",
        "            x = linear_proj(x).permute(0, 2, 1)\n",
        "            return self.split_heads(x)\n",
        "\n",
        "        K = project(k, self.E, self.W_k)\n",
        "        V = project(v, self.F, self.W_v)\n",
        "\n",
        "        attn_out = self.self_attention(Q, K, V, mask)\n",
        "\n",
        "        B, H, L, D = attn_out.shape\n",
        "        concat = attn_out.transpose(1, 2).contiguous().view(B, L, self.d_model)\n",
        "        return self.W_o(concat)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZH_dV5ab9bEE",
      "metadata": {
        "id": "ZH_dV5ab9bEE"
      },
      "source": [
        "#### 2D Linformer + Multi-Ped"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3nxfdoyO9bEE",
      "metadata": {
        "id": "3nxfdoyO9bEE"
      },
      "outputs": [],
      "source": [
        "class Attn2DLinformerMultiPed(nn.Module):\n",
        "    \"\"\"\n",
        "    Modified Multi-Head Attention with 3D product using E, F, G projections.\n",
        "\n",
        "    Args:\n",
        "        num_heads (int): Number of attention heads\n",
        "        d_model (int): Input dimension\n",
        "        k (int): Latent projection size for E, F, G\n",
        "        len_seq (int): Sequence length\n",
        "\n",
        "        Note that the length of the sequence must be inputted correctly here.\n",
        "        first multi-ped then linformer attention\n",
        "    \"\"\"\n",
        "    def __init__(self, num_heads, d_model, k, len_seq, num_peds):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        self.head_dim = d_model // num_heads\n",
        "        self.num_peds = num_peds\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.E = nn.Linear(len_seq//num_peds, k)\n",
        "        self.F = nn.Linear(len_seq//num_peds, k)\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        # Shape: (batch, seq_len, d_model) --> (batch, num_heads, seq_len, head_dim)\n",
        "        bsz, num_peds, len_peds, d_model = x.size()\n",
        "        return x.reshape(bsz, num_peds, len_peds, self.num_heads, self.head_dim).transpose(2,3)\n",
        "\n",
        "    def self_attention(self, q, k, v, mask= None):\n",
        "\n",
        "        # Calculate the dot produce of Q and K\n",
        "        dotqk = torch.matmul(q, k.transpose(-2,-1))/(self.head_dim**0.5) # shape = (#samples, num_heads, len_seq, len_seq)\n",
        "        # Apply the mask if givne\n",
        "        if mask is not None:\n",
        "            dotqk = dotqk.masked_fill(mask == 0, -1e9)\n",
        "            print(f'mask in implemented')\n",
        "            print(f'shape of the mask implemented: {mask.shape}')\n",
        "            print(f'shape of the attn: {dotqk.shape}')\n",
        "\n",
        "        # Apply the Softmax to the attention weights\n",
        "        attention_scores = torch.softmax(dotqk, dim = -1)\n",
        "        # Multiply to the value matrix\n",
        "        result = torch.matmul(attention_scores, v)\n",
        "        return(result)\n",
        "\n",
        "    def split_sequence(self, q):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            q (Tensor): Input tensor of shape (samples, num_heads, seq_len, d_model).\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Reshaped tensor of shape (samples, num_heads, num_peds, seq_len // num_peds, d_model).\n",
        "        \"\"\"\n",
        "\n",
        "        samples, seq_len, d_model = q.shape\n",
        "        assert seq_len % self.num_peds == 0, \"Sequence length is not divisible by number of peds\"\n",
        "        return q.reshape(samples, self.num_peds, seq_len // self.num_peds, d_model)\n",
        "\n",
        "\n",
        "    def forward(self, q,mask = None):\n",
        "        k = q.clone()\n",
        "        v = q.clone()\n",
        "        Q = self.split_heads(self.split_sequence(self.W_q(q)))\n",
        "\n",
        "        def project(x, linear_proj, W):\n",
        "            '''\n",
        "            Expected input shape: ([batch_size, len_seq, d_model])\n",
        "\n",
        "            '''\n",
        "            x = self.split_sequence(W(x)) #shape: ([batch_size, num_peds, len_ped, d_model])\n",
        "            x = x.permute(0, 1, 3, 2) #shape: ([batch_size, num_peds, d_model, len_ped])\n",
        "            #print(f'linear proj: {linear_proj}, shape of x: {x.shape}')\n",
        "            x = linear_proj(x) # shape: torch.Size([batch_size, num_peds, d_model, k])\n",
        "            x = x.permute(0, 1, 3, 2) # shape: torch.Size([batch_size, num_peds, k, d_model])\n",
        "            return self.split_heads((x)) # shape: torch.Size([batch_size, num_peds, num_heads, k , head_dim])\n",
        "\n",
        "        K = project(k, self.E, self.W_k)\n",
        "        V = project(v, self.F, self.W_v)\n",
        "\n",
        "        attn_output = self.self_attention(Q, K, V,mask = mask)\n",
        "\n",
        "        batch_size, num_peds, num_heads, len_ped ,len_head = attn_output.shape\n",
        "        attn_output = attn_output.contiguous().view(batch_size, num_peds, len_ped, self.d_model)\n",
        "        #print(f'shape of attention output after combining heads {attn_output.shape}')\n",
        "\n",
        "        # Combine peds -> (batch_size, len_peds * num_peds, d_model)\n",
        "        attn_output = attn_output.contiguous().view(batch_size, len_ped * num_peds, self.d_model)\n",
        "        #print(f'shape of attention output after combining the peds {attn_output.shape}')\n",
        "\n",
        "        # Apply through the last linear layer\n",
        "        result_final = self.W_o(attn_output) #shape = (#samples, len_seq, d_model)\n",
        "        #print(f'shape of final output: {result_final.shape}')\n",
        "\n",
        "        return result_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "T6m7NH1O9bEE",
      "metadata": {
        "id": "T6m7NH1O9bEE"
      },
      "outputs": [],
      "source": [
        "class Attn2DLinformerMultiPed(nn.Module):\n",
        "    \"\"\"\n",
        "    Modified Multi-Head Attention with 3D product using E, F, G projections.\n",
        "\n",
        "    Args:\n",
        "        num_heads (int): Number of attention heads\n",
        "        d_model (int): Input dimension\n",
        "        k (int): Latent projection size for E, F, G\n",
        "        len_seq (int): Sequence length\n",
        "\n",
        "        Note that the length of the sequence must be inputted correctly here.\n",
        "        first multi-ped then linformer attention\n",
        "    \"\"\"\n",
        "    def __init__(self, num_heads, d_model, k, len_seq, stride, block_size):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        self.head_dim = d_model // num_heads\n",
        "        self.stride = stride\n",
        "        self.block_size = block_size\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.E = nn.Linear(block_size, k)\n",
        "        self.F = nn.Linear(block_size, k)\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        # Shape: (batch, seq_len, d_model) --> (batch, num_heads, seq_len, head_dim)\n",
        "        bsz, num_peds, len_peds, d_model = x.size()\n",
        "        return x.reshape(bsz, num_peds, len_peds, self.num_heads, self.head_dim).transpose(2,3)\n",
        "\n",
        "    def self_attention(self, q, k, v, mask):\n",
        "\n",
        "        # Calculate the dot produce of Q and K\n",
        "        dotqk = torch.matmul(q, k.transpose(-2,-1))/(self.head_dim**0.5) # shape = (#samples, num_heads, len_seq, len_seq)\n",
        "        #print(f'shape of dotqk: {dotqk.shape}')\n",
        "        # Apply the mask if givne\n",
        "        if mask is not None:\n",
        "            dotqk = dotqk.masked_fill(mask == 0, -1e9)\n",
        "            #print(f'shape of the mask: {mask.shape}')\n",
        "        # Apply the Softmax to the attention weights\n",
        "        attention_scores = torch.softmax(dotqk, dim = -1)\n",
        "        # Multiply to the value matrix\n",
        "        result = torch.matmul(attention_scores, v)\n",
        "        return(result)\n",
        "\n",
        "    def _sliding_blocks(self, x):\n",
        "        \"\"\"\n",
        "        Create sliding-window blocks from the sequence.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): (B, L, D).\n",
        "\n",
        "        Returns:\n",
        "            Tensor: (B, num_blocks, block_size, D).\n",
        "        \"\"\"\n",
        "        l = self.block_size\n",
        "        d = self.stride\n",
        "        B, L, D = x.shape\n",
        "\n",
        "        num_blocks = (L - l) // d + 1\n",
        "\n",
        "        # Compute new shape and strides\n",
        "        shape = (B, num_blocks, l, D)\n",
        "        strides = (\n",
        "            x.stride(0),        # batch stride\n",
        "            d * x.stride(1),    # jump d steps for each block\n",
        "            x.stride(1),        # step 1 inside a block\n",
        "            x.stride(2),        # feature stride\n",
        "        )\n",
        "\n",
        "        blocks = x.as_strided(shape, strides)\n",
        "        return blocks.contiguous()\n",
        "\n",
        "    def _reconstruct_from_blocks(self, blocks, seq_len):\n",
        "        \"\"\"\n",
        "        Reconstruct sequence from overlapping sliding blocks.\n",
        "\n",
        "        Args:\n",
        "            blocks (Tensor): (B, num_blocks, block_size, D).\n",
        "            seq_len (int): Original sequence length.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: (B, seq_len, D).\n",
        "        \"\"\"\n",
        "        l = self.block_size\n",
        "        d = self.stride\n",
        "        B, num_blocks, _, D = blocks.shape\n",
        "\n",
        "        device = blocks.device\n",
        "\n",
        "        # Build index map: (num_blocks, block_size) -> positions in seq_len\n",
        "        start_idx = torch.arange(num_blocks, device=device) * d\n",
        "        block_offsets = torch.arange(l, device=device)\n",
        "        positions = start_idx[:, None] + block_offsets[None, :]  # (num_blocks, block_size)\n",
        "\n",
        "        # Expand to batch and feature dims\n",
        "        positions = positions.unsqueeze(0).expand(B, -1, -1)  # (B, num_blocks, block_size)\n",
        "\n",
        "        # Flatten everything for scatter\n",
        "        flat_pos = positions.reshape(B, -1)  # (B, num_blocks * block_size)\n",
        "        flat_blocks = blocks.reshape(B, -1, D)  # (B, num_blocks * block_size, D)\n",
        "\n",
        "        # Allocate output\n",
        "        out = torch.zeros(B, seq_len, D, device=device)\n",
        "        counts = torch.zeros(B, seq_len, 1, device=device)\n",
        "\n",
        "        # Scatter add the block values into the right positions\n",
        "        out.scatter_add_(1, flat_pos.unsqueeze(-1).expand(-1, -1, D), flat_blocks)\n",
        "        counts.scatter_add_(1, flat_pos.unsqueeze(-1), torch.ones_like(flat_pos, device=device).unsqueeze(-1).float())\n",
        "\n",
        "        return out / counts\n",
        "\n",
        "    def forward(self, q,mask = None):\n",
        "        k = q.clone()\n",
        "        v = q.clone()\n",
        "        Q = self.split_heads(self._sliding_blocks(self.W_q(q)))\n",
        "        #print(f'shape of query: {Q.shape}')\n",
        "\n",
        "        def project(x, linear_proj, W):\n",
        "            '''\n",
        "            Expected input shape: ([batch_size, len_seq, d_model])\n",
        "\n",
        "            '''\n",
        "            x = self._sliding_blocks(W(x)) #shape: ([batch_size, num_peds, len_ped, d_model])\n",
        "            x = x.permute(0, 1, 3, 2) #shape: ([batch_size, num_peds, d_model, len_ped])\n",
        "            #print(f'linear proj: {linear_proj}, shape of x permuted: {x.shape}')\n",
        "            x = linear_proj(x) # shape: torch.Size([batch_size, num_peds, d_model, k])\n",
        "            x = x.permute(0, 1, 3, 2) # shape: torch.Size([batch_size, num_peds, k, d_model])\n",
        "            return self.split_heads((x)) # shape: torch.Size([batch_size, num_peds, num_heads, k , head_dim])\n",
        "\n",
        "        K = project(k, self.E, self.W_k)\n",
        "        #print(f'shape of key lin: {K.shape}')\n",
        "        V = project(v, self.F, self.W_v)\n",
        "\n",
        "        attn_output = self.self_attention(Q, K, V,mask)\n",
        "\n",
        "        batch_size, num_peds, num_heads, len_ped ,len_head = attn_output.shape\n",
        "        attn_output = attn_output.contiguous().view(batch_size, num_peds, len_ped, self.d_model)\n",
        "        #print(f'shape of attention output after combining heads {attn_output.shape}')\n",
        "\n",
        "        # Combine peds -> (batch_size, len_peds * num_peds, d_model)\n",
        "        attn_output = self._reconstruct_from_blocks(attn_output, q.shape[1])\n",
        "        #print(f'shape of attention output after combining the peds {attn_output.shape}')\n",
        "\n",
        "        # Apply through the last linear layer\n",
        "        result_final = self.W_o(attn_output) #shape = (#samples, len_seq, d_model)\n",
        "        #print(f'shape of final output: {result_final.shape}')\n",
        "\n",
        "        return result_final"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ljIqH6n9tzO",
      "metadata": {
        "id": "4ljIqH6n9tzO"
      },
      "source": [
        "### 3D Attention Mechanisms"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pBKtmPNx9tzP",
      "metadata": {
        "id": "pBKtmPNx9tzP"
      },
      "source": [
        "#### Combined 2D+ 3D Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Bm8NtWmN9tzQ",
      "metadata": {
        "id": "Bm8NtWmN9tzQ"
      },
      "outputs": [],
      "source": [
        "class Attn_2D3DC(nn.Module):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,num_heads, d_model):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_params = d_model//num_heads\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, d_model) # mapping to the same number of features\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_l = nn.Linear(d_model,d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(self.head_params)\n",
        "        self.norm2 = nn.LayerNorm(self.head_params)\n",
        "\n",
        "    def self_attention1(self, q, k, v, mask= None):\n",
        "        # Calculate the dot produce of Q and K\n",
        "        dotqk = torch.matmul(q, k.transpose(-2,-1))/(self.head_params**0.5) # shape = (#samples, num_heads, len_seq, len_seq)\n",
        "        #print(f'shape of dotqk: {dotqk.shape}')\n",
        "        if mask is not None:\n",
        "            #print(f'shape of the mask applied: {mask.shape}')\n",
        "            dotqk = dotqk.masked_fill(mask == 0, -1e9)\n",
        "            #print(f'mask applied')\n",
        "        # Apply the Softmax to the attention weights\n",
        "        attention_scores = torch.softmax(dotqk, dim = -1)\n",
        "        # Multiply to the value matrix\n",
        "        result = torch.matmul(attention_scores, v)\n",
        "        return(result)\n",
        "\n",
        "    def self_attention2(self, q, k,l, v,mask= None):\n",
        "        # Save the length of the sequence\n",
        "        #seq_len = k.shape[2]\n",
        "        # Calculate the dot produce of Q and K\n",
        "        mul_qkl = torch.einsum('abid,abjd,abkd -> abijk', q, k, l) /(self.head_params**0.5) # Shape: (#samples,num_heads,len_seq,len_seq, len_seq)\n",
        "        #print(f'shape of mul_qkl: {mul_qkl.shape}')\n",
        "        if mask is not None:\n",
        "            #print(f'shape of the mask received: {mask.shape}')\n",
        "            mul_qkl = mul_qkl.masked_fill(mask == 0, -1e9)\n",
        "            #print(f'mask applied')\n",
        "        # Apply the Softmax to the attention weights\n",
        "        attention_scores = softmax_5d(mul_qkl, axis = (-1,-2))\n",
        "        # Calculate the V^2 matrix\n",
        "        vtilde = torch.einsum('abcd,abed->abced', v, v)\n",
        "        # Multiply 3D attention with V^2\n",
        "        result = torch.einsum('abijk,abjkl->abil', attention_scores, vtilde)\n",
        "        return(result)\n",
        "\n",
        "    def combined_attention(self, q, k,l, v, masks):\n",
        "      \"\"\"\n",
        "      This is combining the original attention with the modified attention.\n",
        "      mask1 is the mask with the original dimensions\n",
        "      mask2 is the mask with the modified dimensions\n",
        "      \"\"\"\n",
        "      mask2d, mask3d = masks\n",
        "      result1_attention = self.self_attention1(q, k, v, mask2d)\n",
        "      result2_attention = self.self_attention2(q, k,l, v, mask3d)\n",
        "\n",
        "      return(self.norm1(result1_attention)+self.norm2(result2_attention)) #combined normalized results\n",
        "\n",
        "\n",
        "    def split_heads(self, q):\n",
        "\n",
        "        \"\"\"\n",
        "        Reshaping\n",
        "        Input of shape : (#samples, seq_len, d_model)\n",
        "\n",
        "        to\n",
        "        output of shape: (#samples, num_heads, seq_len, head_parameters)\n",
        "        \"\"\"\n",
        "        samples, seq_len, d_model = q.shape\n",
        "        # Check if dimensions are valid\n",
        "        assert d_model % self.num_heads == 0, \"d_model is not divisable by the number of heads\"\n",
        "        return q.view(samples, seq_len, self.num_heads, self.head_params).transpose(1,2)\n",
        "\n",
        "    def forward(self, q, masks = None):\n",
        "        k = q.clone()\n",
        "        l = q.clone()\n",
        "        v = q.clone()\n",
        "\n",
        "        # Define Query, Key, Value\n",
        "        Query = self.split_heads(self.W_q(q))\n",
        "        Key  = self.split_heads(self.W_k(k))\n",
        "        L_matrix = self.split_heads(self.W_l(l))\n",
        "        Value = self.split_heads(self.W_v(v))\n",
        "\n",
        "        # Run the self-attention\n",
        "        attn_outputs_combined = self.combined_attention(Query, Key,L_matrix, Value, masks)\n",
        "\n",
        "        # Concatenate the heads\n",
        "        batch_size, _,seq_len, _= attn_outputs_combined.shape\n",
        "        attn_outputs_combined = attn_outputs_combined.transpose(1,2).contiguous().view(batch_size, seq_len, self.d_model) #shape = (#samples, len_seq, d_model)\n",
        "\n",
        "        # Apply through the last linear layer\n",
        "        result_final = self.W_o(attn_outputs_combined) #shape = (#samples, len_seq, d_model)\n",
        "\n",
        "        return result_final"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TS2CIa9U9tzQ",
      "metadata": {
        "id": "TS2CIa9U9tzQ"
      },
      "source": [
        "#### Combined 2D + Linformer 3D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Y7JZsOlt9tzQ",
      "metadata": {
        "id": "Y7JZsOlt9tzQ"
      },
      "outputs": [],
      "source": [
        "class Attn2D3DLin(nn.Module):\n",
        "\n",
        "    \"\"\"\n",
        "    Combined 2D and 3D Linformer Attention\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,num_heads, d_model, k, len_seq):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_params = d_model//num_heads\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, d_model) # mapping to the same number of features\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_l = nn.Linear(d_model,d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # Linformer Projections\n",
        "        self.E = nn.Linear(len_seq, k)\n",
        "        self.F = nn.Linear(len_seq, k)\n",
        "        self.G = nn.Linear(len_seq, k)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(self.head_params)\n",
        "        self.norm2 = nn.LayerNorm(self.head_params)\n",
        "\n",
        "    def self_attention2D(self, q, k, v, mask= None):\n",
        "        # Calculate the dot produce of Q and K\n",
        "        dotqk = torch.matmul(q, k.transpose(-2,-1))/(self.head_params**0.5) # shape = (#samples, num_heads, len_seq, len_seq)\n",
        "        #print(f'shape of dotqk: {dotqk.shape}')\n",
        "        if mask is not None:\n",
        "            #print(f'shape of the mask applied: {mask.shape}')\n",
        "            dotqk = dotqk.masked_fill(mask == 0, -1e9)\n",
        "            #print(f'mask applied in 2D attn')\n",
        "        # Apply the Softmax to the attention weights\n",
        "        attention_scores = torch.softmax(dotqk, dim = -1)\n",
        "        # Multiply to the value matrix\n",
        "        result = torch.matmul(attention_scores, v)\n",
        "        return(result)\n",
        "\n",
        "    def self_attention3DLin(self, q, k, l, v, mask = None):\n",
        "        # Compute trilinear attention scores\n",
        "        scores = torch.einsum('bhid,bhjd,bhkd->bhijk', q, k, l) / (self.head_params ** 0.5)\n",
        "        #print(f'shape of scores: {scores.shape}')\n",
        "        if mask is not None:\n",
        "            #print(f'shape of the mask applied: {mask.shape}')\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "            #print(f'mask applied in 3D attn')\n",
        "        attn_weights = softmax_5d(scores, axis=(-1, -2))\n",
        "\n",
        "        v_3d = torch.einsum('bhld,bhLd->bhlLd', v, v)  # Could be simplified\n",
        "        output = torch.einsum('bhijk,bhjkl->bhil', attn_weights, v_3d)\n",
        "        return output\n",
        "\n",
        "\n",
        "    def split_heads(self, q):\n",
        "\n",
        "        \"\"\"\n",
        "        Reshaping\n",
        "        Input of shape : (#samples, seq_len, d_model)\n",
        "\n",
        "        output of shape: (#samples, num_heads, seq_len, head_parameters)\n",
        "        \"\"\"\n",
        "        samples, seq_len, d_model = q.shape\n",
        "        # Check if dimensions are valid\n",
        "        assert d_model % self.num_heads == 0, \"d_model is not divisable by the number of heads\"\n",
        "        return q.view(samples, seq_len, self.num_heads, self.head_params).transpose(1,2)\n",
        "\n",
        "    def forward(self, q, mask = None):\n",
        "        # Clone Query to define key, utinity, and value matrices\n",
        "        k = q.clone()\n",
        "        l = q.clone()\n",
        "        v = q.clone()\n",
        "\n",
        "        # Unwarp the masks\n",
        "        mask2D = mask[0]\n",
        "        mask3D = mask[1]\n",
        "\n",
        "        # Query will be the same for both attentions\n",
        "        Q = self.split_heads(self.W_q(q))\n",
        "\n",
        "        # Define Key and Value for 2D Attention\n",
        "        K_2D  = self.split_heads(self.W_k(k))\n",
        "        V_2D = self.split_heads(self.W_v(v))\n",
        "\n",
        "        # Run the 2D self-attention\n",
        "        attn_outputs_2D = self.self_attention2D(Q, K_2D, V_2D, mask2D)\n",
        "\n",
        "        def project(x, linear_proj, W):\n",
        "          x = W(x).permute(0, 2, 1)\n",
        "          x = linear_proj(x).permute(0, 2, 1)\n",
        "          return self.split_heads(x)\n",
        "\n",
        "        # Define the Query, Key, Utinity and Value matrices for Linformer 3D\n",
        "        K_Lin = project(k, self.E, self.W_k)\n",
        "        L_Lin = project(l, self.F, self.W_l)\n",
        "        V_Lin = project(v, self.G, self.W_v)\n",
        "\n",
        "        # Run Linformer 3D attention\n",
        "        attn_outputs_3DLin= self.self_attention3DLin(Q, K_Lin, L_Lin, V_Lin, mask3D)\n",
        "\n",
        "        # Combine the attentions\n",
        "        attn_outputs_combined = self.norm1(attn_outputs_2D) + self.norm2(attn_outputs_3DLin)\n",
        "\n",
        "        # Concatenate the heads\n",
        "        batch_size, _,seq_len, _= attn_outputs_combined.shape\n",
        "        attn_outputs_combined = attn_outputs_combined.transpose(1,2).contiguous().view(batch_size, seq_len, self.d_model) #shape = (#samples, len_seq, d_model)\n",
        "\n",
        "        # Apply through the last linear layer\n",
        "        result_final = self.W_o(attn_outputs_combined) #shape = (#samples, len_seq, d_model)\n",
        "\n",
        "        return result_final"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FjNHDWXi9tzQ",
      "metadata": {
        "id": "FjNHDWXi9tzQ"
      },
      "source": [
        "#### Alternate Combined 2D + 3D Linformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Al9yUypF9tzQ",
      "metadata": {
        "id": "Al9yUypF9tzQ"
      },
      "outputs": [],
      "source": [
        "class Attn2D3DLinAlter(nn.Module):\n",
        "\n",
        "    \"\"\"\n",
        "    Combined 2D and 3D Linformer Attention\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,num_heads, d_model, k, len_seq):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_params = d_model//num_heads\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, d_model) # mapping to the same number of features\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_l = nn.Linear(d_model,d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # Linformer Projections\n",
        "        self.E = nn.Linear(len_seq, k)\n",
        "        self.F = nn.Linear(len_seq, k)\n",
        "        self.G = nn.Linear(len_seq, k)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(self.head_params)\n",
        "        self.norm2 = nn.LayerNorm(self.head_params)\n",
        "\n",
        "        self.linear = nn.Linear(2*self.head_params,self.head_params)\n",
        "\n",
        "    def self_attention2D(self, q, k, v, mask= None):\n",
        "        # Calculate the dot produce of Q and K\n",
        "        dotqk = torch.matmul(q, k.transpose(-2,-1))/(self.head_params**0.5) # shape = (#samples, num_heads, len_seq, len_seq)\n",
        "        #print(f'shape of dotqk 2D: {dotqk.shape}')\n",
        "        if mask is not None:\n",
        "            #print(f'shape of the mask applied: {mask.shape}')\n",
        "            dotqk = dotqk.masked_fill(mask == 0, -1e9)\n",
        "            #print(f'mask applied in 2D attn')\n",
        "        # Apply the Softmax to the attention weights\n",
        "        attention_scores = torch.softmax(dotqk, dim = -1)\n",
        "        # Multiply to the value matrix\n",
        "        result = torch.matmul(attention_scores, v)\n",
        "        return(result)\n",
        "\n",
        "    def self_attention3DLin(self, q, k, l, v, mask = None):\n",
        "        # Compute trilinear attention scores\n",
        "        scores = torch.einsum('bhid,bhjd,bhkd->bhijk', q, k, l) / (self.head_params ** 0.5)\n",
        "        #print(f'shape of scores Linformer: {scores.shape}')\n",
        "        if mask is not None:\n",
        "            #print(f'shape of the mask applied: {mask.shape}')\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "            #print(f'mask applied in 3D attn')\n",
        "        attn_weights = softmax_5d(scores, axis=(-1, -2))\n",
        "\n",
        "        v_3d = torch.einsum('bhld,bhLd->bhlLd', v, v)  # Could be simplified\n",
        "        output = torch.einsum('bhijk,bhjkl->bhil', attn_weights, v_3d)\n",
        "        return output\n",
        "\n",
        "\n",
        "    def split_heads(self, q):\n",
        "\n",
        "        \"\"\"\n",
        "        Reshaping\n",
        "        Input of shape : (#samples, seq_len, d_model)\n",
        "\n",
        "        output of shape: (#samples, num_heads, seq_len, head_parameters)\n",
        "        \"\"\"\n",
        "        samples, seq_len, d_model = q.shape\n",
        "        # Check if dimensions are valid\n",
        "        assert d_model % self.num_heads == 0, \"d_model is not divisable by the number of heads\"\n",
        "        return q.view(samples, seq_len, self.num_heads, self.head_params).transpose(1,2)\n",
        "\n",
        "    def forward(self, q, mask = None):\n",
        "        # Clone Query to define key, utinity, and value matrices\n",
        "        k = q.clone()\n",
        "        l = q.clone()\n",
        "        v = q.clone()\n",
        "\n",
        "        # Unwarp the masks\n",
        "        mask2D = mask[0]\n",
        "        mask3D = mask[1]\n",
        "\n",
        "        # Query will be the same for both attentions\n",
        "        Q = self.split_heads(self.W_q(q))\n",
        "\n",
        "        # Define Key and Value for 2D Attention\n",
        "        K_2D  = self.split_heads(self.W_k(k))\n",
        "        V_2D = self.split_heads(self.W_v(v))\n",
        "\n",
        "        # Run the 2D self-attention\n",
        "        attn_outputs_2D = self.self_attention2D(Q, K_2D, V_2D, mask2D)\n",
        "\n",
        "        def project(x, linear_proj, W):\n",
        "          x = W(x).permute(0, 2, 1)\n",
        "          x = linear_proj(x).permute(0, 2, 1)\n",
        "          return self.split_heads(x)\n",
        "\n",
        "        # Define the Query, Key, Utinity and Value matrices for Linformer 3D\n",
        "        K_Lin = project(k, self.E, self.W_k)\n",
        "        L_Lin = project(l, self.F, self.W_l)\n",
        "        V_Lin = project(v, self.G, self.W_v)\n",
        "\n",
        "        # Run Linformer 3D attention\n",
        "        attn_outputs_3DLin= self.self_attention3DLin(Q, K_Lin, L_Lin, V_Lin, mask3D)\n",
        "\n",
        "        # Concatenate the two attentions\n",
        "        combined_attention = torch.cat((attn_outputs_2D, attn_outputs_3DLin), dim=-1)\n",
        "        #print(f'shape of combined attention before linear: {combined_attention.shape}')\n",
        "        combined_attention = self.linear(combined_attention)\n",
        "        #print(f'shape of combined attention after linear: {combined_attention.shape}')\n",
        "\n",
        "        # Concatenate the heads\n",
        "        batch_size, _,seq_len, _= combined_attention.shape\n",
        "        combined_attention = combined_attention.transpose(1,2).contiguous().view(batch_size, seq_len, self.d_model) #shape = (#samples, len_seq, d_model)\n",
        "\n",
        "        # Apply through the last linear layer\n",
        "        result_final = self.W_o(combined_attention) #shape = (#samples, len_seq, d_model)\n",
        "\n",
        "        return result_final"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1x1V_9ju9tzR",
      "metadata": {
        "id": "1x1V_9ju9tzR"
      },
      "source": [
        "#### Combined 2D and 3D Multiped Alternative"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jJiVI2VE9tzR",
      "metadata": {
        "id": "jJiVI2VE9tzR"
      },
      "outputs": [],
      "source": [
        "class Attn2D3D_MultiPedAlter(nn.Module):\n",
        "\n",
        "    \"\"\"\n",
        "    Combined 2D and 3D Multi-Ped Attention\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,num_heads, d_model, num_peds):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_params = d_model//num_heads\n",
        "        self.num_peds = num_peds\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, d_model) # mapping to the same number of features\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_l = nn.Linear(d_model,d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(self.head_params)\n",
        "        self.norm2 = nn.LayerNorm(self.head_params)\n",
        "\n",
        "        self.linear = nn.Linear(2*self.head_params,self.head_params)\n",
        "\n",
        "    def self_attention2D(self, q, k, v, mask = None):\n",
        "        # Calculate the dot produce of Q and K\n",
        "        dotqk = torch.matmul(q, k.transpose(-2,-1))/(self.head_params**0.5) # shape = (#samples, num_heads, len_seq, len_seq)\n",
        "        #print(f'shape of dotqk in 2D : {dotqk.shape}')\n",
        "        if mask is not None:\n",
        "            #print(f'shape of the mask applied: {mask.shape}')\n",
        "            dotqk = dotqk.masked_fill(mask == 0, -1e9)\n",
        "            #print(f'mask applied in 2D attn')\n",
        "        # Apply the Softmax to the attention weights\n",
        "        attention_scores = torch.softmax(dotqk, dim = -1)\n",
        "        # Multiply to the value matrix\n",
        "        result = torch.matmul(attention_scores, v)\n",
        "        return(result)\n",
        "\n",
        "    def self_attention3DMultiPed(self, q, k,l, v,mask = None):\n",
        "        # Save the length of the sequence\n",
        "        seq_len = k.shape[2]\n",
        "        # Calculate the dot produce of Q and K\n",
        "        mul_qkl = torch.einsum('abcid,abcjd,abckd -> abcijk', q, k, l) /(self.head_params**0.5)# Shape: (#samples,num_heads,len_seq,len_seq, len_seq)\n",
        "        #print(f'shape of mul_qkl in 3D MultiPed: {mul_qkl.shape}')\n",
        "        #print(f'shape of mul_qkl: {mul_qkl.shape}')\n",
        "        if mask is not None:\n",
        "            #print(f'shape of the mask applied: {mask.shape}')\n",
        "            mul_qkl = mul_qkl.masked_fill(mask == 0, -1e9)\n",
        "            #print(f'mask applied in 3D attn')\n",
        "            #print(f'mask applied to 3D multiped; shape of the mask: {mask.shape}')\n",
        "        # Apply the Softmax to the attention weights\n",
        "        attention_scores = softmax_5d(mul_qkl, axis = (-1,-2))\n",
        "        # Calculate V^2\n",
        "        vtilde = torch.einsum('abcil,abcjl->abcijl', v, v)\n",
        "        # Multiply to the value matrix\n",
        "        result = torch.einsum('abcijk,abcjkl->abcil', attention_scores, vtilde)\n",
        "        #print(f'shape of attention results: {result.shape}')\n",
        "        return(result)\n",
        "\n",
        "    def split_heads2D(self, q):\n",
        "\n",
        "        \"\"\"\n",
        "        Reshaping\n",
        "        Input of shape : (#samples, seq_len, d_model)\n",
        "\n",
        "        output of shape: (#samples, num_heads, seq_len, head_parameters)\n",
        "        \"\"\"\n",
        "        samples, seq_len, d_model = q.shape\n",
        "        # Check if dimensions are valid\n",
        "        assert d_model % self.num_heads == 0, \"d_model is not divisable by the number of heads\"\n",
        "        return q.view(samples, seq_len, self.num_heads, self.head_params).transpose(1,2)\n",
        "\n",
        "\n",
        "    def split_sequence(self, q):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            q (Tensor): Input tensor of shape (samples, num_heads, seq_len, d_model).\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Reshaped tensor of shape (samples, num_heads, num_peds, seq_len // num_peds, d_model).\n",
        "        \"\"\"\n",
        "\n",
        "        samples, seq_len, d_model = q.shape\n",
        "        assert seq_len % self.num_peds == 0, \"Sequence length is not divisible by number of peds\"\n",
        "        #print(f'shape of q: {q.shape}')\n",
        "\n",
        "        return q.reshape(samples, self.num_peds, seq_len // self.num_peds, d_model)\n",
        "\n",
        "    def split_heads3D(self, x):\n",
        "        # Shape: (batch, seq_len, d_model) --> (batch, num_heads, seq_len, head_dim)\n",
        "        bsz, num_peds, len_peds, d_model = x.size()\n",
        "        return x.reshape(bsz, num_peds, len_peds, self.num_heads, self.head_params).transpose(2,3)\n",
        "\n",
        "    def forward(self, q, mask = None):\n",
        "        #print(f'combined 2D and 3D multiped is activated')\n",
        "\n",
        "        # Clone Query to define key, utinity, and value matrices\n",
        "        k = q.clone()\n",
        "        l = q.clone()\n",
        "        v = q.clone()\n",
        "\n",
        "        mask_2D = None\n",
        "        mask_3DMP = None\n",
        "\n",
        "        # Unpack the masks\n",
        "        if mask is not None:\n",
        "          mask_2D = mask[0]\n",
        "          #print(f'mask_2D{mask_2D}, shape = {mask_2D.shape}')\n",
        "          mask_3DMP = mask[1]\n",
        "          #print(f'mask_3DMP{mask_3DMP}, shape = {mask_3DMP.shape}')\n",
        "\n",
        "        Query_2D = self.split_heads2D(self.W_q(q))\n",
        "        Key_2D  = self.split_heads2D(self.W_k(k))\n",
        "        Value_2D = self.split_heads2D(self.W_v(v))\n",
        "\n",
        "        # Run the 2D self-attention\n",
        "        attn_outputs_2D = self.self_attention2D(Query_2D, Key_2D, Value_2D, mask = mask_2D) # (batch_size, num_heads, len_seq, head_params)\n",
        "\n",
        "        # Define the Query, Key, Utinity and Value matrices for Multi-Ped 3D\n",
        "        Query_3D = self.split_heads3D(self.split_sequence(self.W_q(q)))\n",
        "        Key_3D  = self.split_heads3D(self.split_sequence(self.W_k(k)))\n",
        "        Value_3D = self.split_heads3D(self.split_sequence(self.W_v(v)))\n",
        "        L_matrix = self.split_heads3D(self.split_sequence(self.W_l(l)))\n",
        "\n",
        "        # Run MultiPed 3D attention\n",
        "        attn_outputs_3DMultiPed= self.self_attention3DMultiPed(Query_3D, Key_3D, L_matrix, Value_3D, mask = mask_3DMP) # (batch_size, num_peds, num_heads, len_ped, head_params)\n",
        "        #print(f'shape of attention 2D output: {attn_outputs_3DMultiPed.shape}')\n",
        "\n",
        "        # Combine the peds\n",
        "        batch_size, num_peds, num_heads, len_ped ,len_head = attn_outputs_3DMultiPed.shape\n",
        "        attn_outputs_3DMultiPed = attn_outputs_3DMultiPed.contiguous().view(batch_size, num_heads, num_peds*len_ped, len_head)\n",
        "        #print(f'shape of attention 3D multi-ped output after combining heads {attn_outputs_3DMultiPed.shape}')\n",
        "        # Concatenate the output of the attentions\n",
        "        attn_outputs_concat = torch.cat((attn_outputs_2D, attn_outputs_3DMultiPed), dim=-1)\n",
        "        #print(f'shape of attns concatednate : {attn_outputs_concat.shape}')\n",
        "\n",
        "        # Combine the attentions\n",
        "        attn_outputs_combined = self.linear(attn_outputs_concat)\n",
        "        #print(f'shape of attention 3D multi-ped output after combining heads {attn_outputs_combined.shape}')\n",
        "\n",
        "        # Concatenate the heads\n",
        "        batch_size, _,seq_len, _= attn_outputs_combined.shape\n",
        "        attn_outputs_combined = attn_outputs_combined.transpose(1,2).contiguous().view(batch_size, seq_len, self.d_model) #shape = (#samples, len_seq, d_model)\n",
        "\n",
        "        # Apply through the last linear layer\n",
        "        result_final = self.W_o(attn_outputs_combined) #shape = (#samples, len_seq, d_model)\n",
        "\n",
        "        return result_final"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6rlzr68s9tzR",
      "metadata": {
        "id": "6rlzr68s9tzR"
      },
      "source": [
        "#### Combined 2D + Multi-Ped 3D (global 2D + Local 3D)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "N2BG3yYR9tzR",
      "metadata": {
        "id": "N2BG3yYR9tzR"
      },
      "outputs": [],
      "source": [
        "class Attn2D3D_MultiPed(nn.Module):\n",
        "\n",
        "    \"\"\"\n",
        "    Combined 2D and 3D Multi-Ped Attention\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,num_heads, d_model, num_peds):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_params = d_model//num_heads\n",
        "        self.num_peds = num_peds\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, d_model) # mapping to the same number of features\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_l = nn.Linear(d_model,d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(self.head_params)\n",
        "        self.norm2 = nn.LayerNorm(self.head_params)\n",
        "\n",
        "    def self_attention2D(self, q, k, v, mask = None):\n",
        "        # Calculate the dot produce of Q and K\n",
        "        dotqk = torch.matmul(q, k.transpose(-2,-1))/(self.head_params**0.5) # shape = (#samples, num_heads, len_seq, len_seq)\n",
        "        #print(f'shape of dotqk in 2D : {dotqk.shape}')\n",
        "        if mask is not None:\n",
        "            #print(f'shape of the mask applied: {mask.shape}')\n",
        "            dotqk = dotqk.masked_fill(mask == 0, -1e9)\n",
        "            #print(f'mask applied in 2D attn')\n",
        "        # Apply the Softmax to the attention weights\n",
        "        attention_scores = torch.softmax(dotqk, dim = -1)\n",
        "        # Multiply to the value matrix\n",
        "        result = torch.matmul(attention_scores, v)\n",
        "        return(result)\n",
        "\n",
        "    def self_attention3DMultiPed(self, q, k,l, v,mask = None):\n",
        "        # Save the length of the sequence\n",
        "        seq_len = k.shape[2]\n",
        "        # Calculate the dot produce of Q and K\n",
        "        mul_qkl = torch.einsum('abcid,abcjd,abckd -> abcijk', q, k, l) /(self.head_params**0.5)# Shape: (#samples,num_heads,len_seq,len_seq, len_seq)\n",
        "        #print(f'shape of mul_qkl in 3D MultiPed: {mul_qkl.shape}')\n",
        "        #print(f'shape of mul_qkl: {mul_qkl.shape}')\n",
        "        if mask is not None:\n",
        "            #print(f'shape of the mask applied: {mask.shape}')\n",
        "            mul_qkl = mul_qkl.masked_fill(mask == 0, -1e9)\n",
        "            #print(f'mask applied in 3D attn')\n",
        "            #print(f'mask applied to 3D multiped; shape of the mask: {mask.shape}')\n",
        "        # Apply the Softmax to the attention weights\n",
        "        attention_scores = softmax_5d(mul_qkl, axis = (-1,-2))\n",
        "        # Calculate V^2\n",
        "        vtilde = torch.einsum('abcil,abcjl->abcijl', v, v)\n",
        "        # Multiply to the value matrix\n",
        "        result = torch.einsum('abcijk,abcjkl->abcil', attention_scores, vtilde)\n",
        "        #print(f'shape of attention results: {result.shape}')\n",
        "        return(result)\n",
        "\n",
        "    def split_heads2D(self, q):\n",
        "\n",
        "        \"\"\"\n",
        "        Reshaping\n",
        "        Input of shape : (#samples, seq_len, d_model)\n",
        "\n",
        "        output of shape: (#samples, num_heads, seq_len, head_parameters)\n",
        "        \"\"\"\n",
        "        samples, seq_len, d_model = q.shape\n",
        "        # Check if dimensions are valid\n",
        "        assert d_model % self.num_heads == 0, \"d_model is not divisable by the number of heads\"\n",
        "        return q.view(samples, seq_len, self.num_heads, self.head_params).transpose(1,2)\n",
        "\n",
        "\n",
        "    def split_sequence(self, q):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            q (Tensor): Input tensor of shape (samples, num_heads, seq_len, d_model).\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Reshaped tensor of shape (samples, num_heads, num_peds, seq_len // num_peds, d_model).\n",
        "        \"\"\"\n",
        "\n",
        "        samples, seq_len, d_model = q.shape\n",
        "        assert seq_len % self.num_peds == 0, \"Sequence length is not divisible by number of peds\"\n",
        "        #print(f'shape of q: {q.shape}')\n",
        "\n",
        "        return q.reshape(samples, self.num_peds, seq_len // self.num_peds, d_model)\n",
        "\n",
        "    def split_heads3D(self, x):\n",
        "        # Shape: (batch, seq_len, d_model) --> (batch, num_heads, seq_len, head_dim)\n",
        "        bsz, num_peds, len_peds, d_model = x.size()\n",
        "        return x.reshape(bsz, num_peds, len_peds, self.num_heads, self.head_params).transpose(2,3)\n",
        "\n",
        "    def forward(self, q, mask = None):\n",
        "        #print(f'combined 2D and 3D multiped is activated')\n",
        "\n",
        "        # Clone Query to define key, utinity, and value matrices\n",
        "        k = q.clone()\n",
        "        l = q.clone()\n",
        "        v = q.clone()\n",
        "\n",
        "        mask_2D = None\n",
        "        mask_3DMP = None\n",
        "\n",
        "        # Unpack the masks\n",
        "        if mask is not None:\n",
        "          mask_2D = mask[0]\n",
        "          #print(f'mask_2D{mask_2D}, shape = {mask_2D.shape}')\n",
        "          mask_3DMP = mask[1]\n",
        "          #print(f'mask_3DMP{mask_3DMP}, shape = {mask_3DMP.shape}')\n",
        "\n",
        "        Query_2D = self.split_heads2D(self.W_q(q))\n",
        "        Key_2D  = self.split_heads2D(self.W_k(k))\n",
        "        Value_2D = self.split_heads2D(self.W_v(v))\n",
        "\n",
        "        # Run the 2D self-attention\n",
        "        attn_outputs_2D = self.self_attention2D(Query_2D, Key_2D, Value_2D, mask = mask_2D) # (batch_size, num_heads, len_seq, head_params)\n",
        "\n",
        "        # Define the Query, Key, Utinity and Value matrices for Multi-Ped 3D\n",
        "        Query_3D = self.split_heads3D(self.split_sequence(self.W_q(q)))\n",
        "        Key_3D  = self.split_heads3D(self.split_sequence(self.W_k(k)))\n",
        "        Value_3D = self.split_heads3D(self.split_sequence(self.W_v(v)))\n",
        "        L_matrix = self.split_heads3D(self.split_sequence(self.W_l(l)))\n",
        "\n",
        "        # Run MultiPed 3D attention\n",
        "        attn_outputs_3DMultiPed= self.self_attention3DMultiPed(Query_3D, Key_3D, L_matrix, Value_3D, mask = mask_3DMP) # (batch_size, num_peds, num_heads, len_ped, head_params)\n",
        "        #print(f'shape of attention 2D output: {attn_outputs_3DMultiPed.shape}')\n",
        "\n",
        "        # Combine the peds\n",
        "        batch_size, num_peds, num_heads, len_ped ,len_head = attn_outputs_3DMultiPed.shape\n",
        "        attn_outputs_3DMultiPed = attn_outputs_3DMultiPed.contiguous().view(batch_size, num_heads, num_peds*len_ped, len_head)\n",
        "        #print(f'shape of attention 3D multi-ped output after combining heads {attn_outputs_3DMultiPed.shape}')\n",
        "\n",
        "        # Combine the attentions\n",
        "        attn_outputs_combined = self.norm1(attn_outputs_2D) + self.norm2(attn_outputs_3DMultiPed)\n",
        "\n",
        "        # Concatenate the heads\n",
        "        batch_size, _,seq_len, _= attn_outputs_combined.shape\n",
        "        attn_outputs_combined = attn_outputs_combined.transpose(1,2).contiguous().view(batch_size, seq_len, self.d_model) #shape = (#samples, len_seq, d_model)\n",
        "\n",
        "        # Apply through the last linear layer\n",
        "        result_final = self.W_o(attn_outputs_combined) #shape = (#samples, len_seq, d_model)\n",
        "\n",
        "        return result_final"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KQVgSfCB9tzR",
      "metadata": {
        "id": "KQVgSfCB9tzR"
      },
      "source": [
        "#### Combined 2D with 3D MultiPed Linformer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Attn2D3D_MultiPedLin(nn.Module):\n",
        "\n",
        "    \"\"\"\n",
        "    Combined 2D and 3D Multi-Ped Attention\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,num_heads, d_model, num_peds, len_seq, k):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_params = d_model//num_heads\n",
        "        self.num_peds = num_peds\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, d_model) # mapping to the same number of features\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_l = nn.Linear(d_model,d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # Linformer Projections\n",
        "        self.E = nn.Linear(len_seq, k)\n",
        "        self.F = nn.Linear(len_seq, k)\n",
        "        self.G = nn.Linear(len_seq, k)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(self.head_params)\n",
        "        self.norm2 = nn.LayerNorm(self.head_params)\n",
        "\n",
        "    def self_attention2D(self, q, k, v, mask = None):\n",
        "        # Calculate the dot produce of Q and K\n",
        "        dotqk = torch.matmul(q, k.transpose(-2,-1))/(self.head_params**0.5) # shape = (#samples, num_heads, len_seq, len_seq)\n",
        "        #print(f'shape of dotqk 2D Attn: {dotqk.shape}')\n",
        "        if mask is not None:\n",
        "            #print(f'shape of the mask applied: {mask.shape}')\n",
        "            dotqk = dotqk.masked_fill(mask == 0, -1e9)\n",
        "            #print(f'mask applied in 2D attn')\n",
        "            #print(f'mask applied to 2D attention. Shape of the mask: {mask.shape}')\n",
        "        # Apply the Softmax to the attention weights\n",
        "        attention_scores = torch.softmax(dotqk, dim = -1)\n",
        "        # Multiply to the value matrix\n",
        "        result = torch.matmul(attention_scores, v)\n",
        "        return(result)\n",
        "\n",
        "    def self_attention3D(self, q, k,l, v,mask = None):\n",
        "        # Save the length of the sequence\n",
        "        seq_len = k.shape[2]\n",
        "        # Calculate the dot produce of Q and K\n",
        "        mul_qkl = torch.einsum('abcid,abcjd,abckd -> abcijk', q, k, l) /(self.head_params**0.5)# Shape: (#samples,num_heads,len_seq,len_seq, len_seq)\n",
        "        #print(f'shape of mul_qkl: {mul_qkl.shape}')\n",
        "        if mask is not None:\n",
        "            #print(f'shape of the mask applied: {mask.shape}')\n",
        "            mul_qkl = mul_qkl.masked_fill(mask == 0, -1e9)\n",
        "            #print(f'mask applied to 3D multiped Linformer; shape of the mask: {mask.shape}')\n",
        "        # Apply the Softmax to the attention weights\n",
        "        attention_scores = softmax_5d(mul_qkl, axis = (-1,-2))\n",
        "        # Calculate V^2\n",
        "        vtilde = torch.einsum('abcil,abcjl->abcijl', v, v)\n",
        "        # Multiply to the value matrix\n",
        "        result = torch.einsum('abcijk,abcjkl->abcil', attention_scores, vtilde)\n",
        "        #print(f'shape of attention results: {result.shape}')\n",
        "        return(result)\n",
        "\n",
        "    def split_heads(self, q):\n",
        "\n",
        "        \"\"\"\n",
        "        Reshaping\n",
        "        Input of shape : (#samples, seq_len, d_model)\n",
        "\n",
        "        output of shape: (#samples, num_heads, seq_len, head_parameters)\n",
        "        \"\"\"\n",
        "        samples, seq_len, d_model = q.shape\n",
        "        # Check if dimensions are valid\n",
        "        assert d_model % self.num_heads == 0, \"d_model is not divisable by the number of heads\"\n",
        "        return q.view(samples, seq_len, self.num_heads, self.head_params).transpose(1,2)\n",
        "\n",
        "\n",
        "    def split_sequence(self,q):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            q (Tensor): Input tensor of shape (samples, num_heads, seq_len, d_model).\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Reshaped tensor of shape (samples, num_heads, num_peds, seq_len // num_peds, d_model).\n",
        "        \"\"\"\n",
        "\n",
        "        samples, num_heads, seq_len, head_params = q.shape\n",
        "        assert seq_len % self.num_peds == 0, \"Sequence length is not divisible by number of peds\"\n",
        "        #print(f'shape of q: {q.shape}')\n",
        "\n",
        "        return q.reshape(samples, self.num_peds, num_heads, seq_len // self.num_peds, head_params)\n",
        "\n",
        "\n",
        "    def forward(self, q, mask = None):\n",
        "        #print(f'combined 2D and 3D multiped is activated')\n",
        "\n",
        "        # Clone Query to define key, utinity, and value matrices\n",
        "        k = q.clone()\n",
        "        l = q.clone()\n",
        "        v = q.clone()\n",
        "        mask_2D = None\n",
        "        mask_3DMP = None\n",
        "\n",
        "        # Unpack the masks\n",
        "        if mask is not None:\n",
        "          mask_2D = mask[0]\n",
        "          #print(f'mask_2D{mask_2D}, shape = {mask_2D.shape}')\n",
        "          mask_3DMP = mask[1]\n",
        "          #print(f'mask_3DMP{mask_3DMP}, shape = {mask_3DMP.shape}')\n",
        "\n",
        "        Query_2D = self.split_heads(self.W_q(q))\n",
        "        Key_2D  = self.split_heads(self.W_k(k))\n",
        "        Value_2D = self.split_heads(self.W_v(v))\n",
        "\n",
        "        # Run the 2D self-attention\n",
        "        attn_outputs_2D = self.self_attention2D(Query_2D, Key_2D, Value_2D, mask = mask_2D) # (batch_size, num_heads, len_seq, head_params)\n",
        "\n",
        "        # Define the Query, Key, Utinity and Value matrices for Multi-Ped 3D\n",
        "        Query_3D = self.split_sequence(Query_2D)\n",
        "        Key_3D  = self.split_sequence(Key_2D)\n",
        "        Value_3D = self.split_sequence(Value_2D)\n",
        "        L_matrix = self.split_sequence(self.split_heads(self.W_l(l)))\n",
        "\n",
        "        # Run MultiPed 3D attention\n",
        "        attn_outputs_3DMultiPed= self.self_attention3D(Query_3D, Key_3D, L_matrix, Value_3D, mask = mask_3DMP) # (batch_size, num_peds, num_heads, len_ped, head_params)\n",
        "        #print(f'shape of attention 2D output: {attn_outputs_3DMultiPed.shape}')\n",
        "\n",
        "        # Combine the peds\n",
        "        batch_size, num_peds, num_heads, len_ped ,len_head = attn_outputs_3DMultiPed.shape\n",
        "        attn_outputs_3DMultiPed = attn_outputs_3DMultiPed.contiguous().view(batch_size, num_heads, num_peds*len_ped, len_head)\n",
        "        #print(f'shape of attention 3D multi-ped output after combining heads {attn_outputs_3DMultiPed.shape}')\n",
        "\n",
        "        # Combine the attentions\n",
        "        attn_outputs_combined = self.norm1(attn_outputs_2D) + self.norm2(attn_outputs_3DMultiPed)\n",
        "\n",
        "        # Concatenate the heads\n",
        "        batch_size, _,seq_len, _= attn_outputs_combined.shape\n",
        "        attn_outputs_combined = attn_outputs_combined.transpose(1,2).contiguous().view(batch_size, seq_len, self.d_model) #shape = (#samples, len_seq, d_model)\n",
        "\n",
        "        # Apply through the last linear layer\n",
        "        result_final = self.W_o(attn_outputs_combined) #shape = (#samples, len_seq, d_model)\n",
        "\n",
        "        return result_final"
      ],
      "metadata": {
        "id": "STh3FSsxdKjD"
      },
      "id": "STh3FSsxdKjD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dual 3D Attns"
      ],
      "metadata": {
        "id": "juzsohBvor3g"
      },
      "id": "juzsohBvor3g"
    },
    {
      "cell_type": "code",
      "source": [
        "class Attn3D_MultiPed_Dual(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-ped 3D Attention with two parallel attention modules\n",
        "    whose outputs are summed together.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, d_model, block_size, stride):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_params = d_model // num_heads\n",
        "        self.block_size = block_size\n",
        "        self.stride = stride\n",
        "\n",
        "        # First attention set\n",
        "        self.W_q1 = nn.Linear(d_model, d_model)\n",
        "        self.W_k1 = nn.Linear(d_model, d_model)\n",
        "        self.W_l1 = nn.Linear(d_model, d_model)\n",
        "        self.W_v1 = nn.Linear(d_model, d_model)\n",
        "        self.W_o1 = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # Second attention set\n",
        "        self.W_q2 = nn.Linear(d_model, d_model)\n",
        "        self.W_k2 = nn.Linear(d_model, d_model)\n",
        "        self.W_l2 = nn.Linear(d_model, d_model)\n",
        "        self.W_v2 = nn.Linear(d_model, d_model)\n",
        "        self.W_o2 = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def self_attention(self, q, k, l, v, mask=None):\n",
        "        mul_qkl = torch.einsum(\n",
        "            'abcid,abcjd,abckd -> abcijk', q, k, l\n",
        "        ) / (self.head_params ** 0.5)\n",
        "\n",
        "        if mask is not None:\n",
        "            mul_qkl = mul_qkl.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        attention_scores = softmax_5d(mul_qkl, axis=(-1, -2))\n",
        "\n",
        "        vtilde = torch.einsum('abcil,abcjl->abcijl', v, v)\n",
        "        result = torch.einsum('abcijk,abcjkl->abcil', attention_scores, vtilde)\n",
        "        return result\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        bsz, num_peds, len_peds, d_model = x.size()\n",
        "        return x.reshape(bsz, num_peds, len_peds, self.num_heads, self.head_params).transpose(2, 3)\n",
        "\n",
        "    def _sliding_blocks(self, x):\n",
        "        l = self.block_size\n",
        "        d = self.stride\n",
        "        B, L, D = x.shape\n",
        "        num_blocks = (L - l) // d + 1\n",
        "\n",
        "        shape = (B, num_blocks, l, D)\n",
        "        strides = (\n",
        "            x.stride(0),\n",
        "            d * x.stride(1),\n",
        "            x.stride(1),\n",
        "            x.stride(2),\n",
        "        )\n",
        "\n",
        "        blocks = x.as_strided(shape, strides)\n",
        "        return blocks.contiguous()\n",
        "\n",
        "    def _reconstruct_from_blocks(self, blocks, seq_len):\n",
        "        l = self.block_size\n",
        "        d = self.stride\n",
        "        B, num_blocks, _, D = blocks.shape\n",
        "        device = blocks.device\n",
        "\n",
        "        start_idx = torch.arange(num_blocks, device=device) * d\n",
        "        block_offsets = torch.arange(l, device=device)\n",
        "        positions = start_idx[:, None] + block_offsets[None, :]  # (num_blocks, block_size)\n",
        "\n",
        "        positions = positions.unsqueeze(0).expand(B, -1, -1)  # (B, num_blocks, block_size)\n",
        "        flat_pos = positions.reshape(B, -1)\n",
        "        flat_blocks = blocks.reshape(B, -1, D)\n",
        "\n",
        "        out = torch.zeros(B, seq_len, D, device=device)\n",
        "        counts = torch.zeros(B, seq_len, 1, device=device)\n",
        "\n",
        "        out.scatter_add_(1, flat_pos.unsqueeze(-1).expand(-1, -1, D), flat_blocks)\n",
        "        counts.scatter_add_(1, flat_pos.unsqueeze(-1), torch.ones_like(flat_pos, device=device).unsqueeze(-1).float())\n",
        "\n",
        "        return out / counts\n",
        "\n",
        "    def _run_attention_set(self, q, W_q, W_k, W_l, W_v, W_o, mask=None):\n",
        "        k = q.clone()\n",
        "        l = q.clone()\n",
        "        v = q.clone()\n",
        "\n",
        "        Query = self.split_heads(self._sliding_blocks(W_q(q)))\n",
        "        Key = self.split_heads(self._sliding_blocks(W_k(k)))\n",
        "        L_matrix = self.split_heads(self._sliding_blocks(W_l(l)))\n",
        "        Value = self.split_heads(self._sliding_blocks(W_v(v)))\n",
        "\n",
        "        attn_output = self.self_attention(Query, Key, L_matrix, Value, mask=mask)\n",
        "\n",
        "        batch_size, num_peds, num_heads, len_ped, len_head = attn_output.shape\n",
        "        attn_output = attn_output.contiguous().view(batch_size, num_peds, len_ped, self.d_model)\n",
        "\n",
        "        len_seq = q.shape[1]\n",
        "        attn_output = self._reconstruct_from_blocks(attn_output, len_seq)\n",
        "\n",
        "        return W_o(attn_output)\n",
        "\n",
        "    def forward(self, q, mask=None):\n",
        "        # Run first attention set\n",
        "        out1 = self._run_attention_set(q, self.W_q1, self.W_k1, self.W_l1, self.W_v1, self.W_o1, mask)\n",
        "\n",
        "        # Run second attention set\n",
        "        out2 = self._run_attention_set(q, self.W_q2, self.W_k2, self.W_l2, self.W_v2, self.W_o2, mask)\n",
        "\n",
        "        # Add results\n",
        "        return out1 + out2\n"
      ],
      "metadata": {
        "id": "nMAaC7rrovOI"
      },
      "execution_count": null,
      "outputs": [],
      "id": "nMAaC7rrovOI"
    },
    {
      "cell_type": "code",
      "source": [
        "class Attn3DLinformer_Dual(nn.Module):\n",
        "    \"\"\"\n",
        "    Dual Attention 3D Linformer:\n",
        "    Runs two independent attention mechanisms (with separate weights)\n",
        "    and sums their outputs.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_heads, d_model, k, len_seq):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        self.head_dim = d_model // num_heads\n",
        "\n",
        "        # First attention weights\n",
        "        self.W_q1 = nn.Linear(d_model, d_model)\n",
        "        self.W_k1 = nn.Linear(d_model, d_model)\n",
        "        self.W_l1 = nn.Linear(d_model, d_model)\n",
        "        self.W_v1 = nn.Linear(d_model, d_model)\n",
        "        self.W_o1 = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.E1 = nn.Linear(len_seq, k)\n",
        "        self.F1 = nn.Linear(len_seq, k)\n",
        "        self.G1 = nn.Linear(len_seq, k)\n",
        "\n",
        "        # Second attention weights\n",
        "        self.W_q2 = nn.Linear(d_model, d_model)\n",
        "        self.W_k2 = nn.Linear(d_model, d_model)\n",
        "        self.W_l2 = nn.Linear(d_model, d_model)\n",
        "        self.W_v2 = nn.Linear(d_model, d_model)\n",
        "        self.W_o2 = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.E2 = nn.Linear(len_seq, k)\n",
        "        self.F2 = nn.Linear(len_seq, k)\n",
        "        self.G2 = nn.Linear(len_seq, k)\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        B, L, D = x.size()\n",
        "        x = x.view(B, L, self.num_heads, self.head_dim)\n",
        "        return x.transpose(1, 2)  # [B, H, L, D]\n",
        "\n",
        "    def self_attention(self, q, k, l, v, mask):\n",
        "        scores = torch.einsum('bhid,bhjd,bhkd->bhijk', q, k, l) / (self.head_dim ** 0.5)\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "        attn_weights = softmax_5d(scores, axis=(-1, -2))\n",
        "\n",
        "        v_3d = torch.einsum('bhld,bhLd->bhlLd', v, v)\n",
        "        output = torch.einsum('bhijk,bhjkl->bhil', attn_weights, v_3d)\n",
        "        return output\n",
        "\n",
        "    def forward_path(self, q, k, l, v, W_q, W_k, W_l, W_v, W_o, E, F, G, mask):\n",
        "        Q = self.split_heads(W_q(q))\n",
        "\n",
        "        def project(x, linear_proj, W):\n",
        "            x = W(x).permute(0, 2, 1)\n",
        "            x = linear_proj(x).permute(0, 2, 1)\n",
        "            return self.split_heads(x)\n",
        "\n",
        "        K = project(k, E, W_k)\n",
        "        L = project(l, F, W_l)\n",
        "        V = project(v, G, W_v)\n",
        "\n",
        "        attn_out = self.self_attention(Q, K, L, V, mask)\n",
        "        B, H, L_seq, D = attn_out.shape\n",
        "        concat = attn_out.transpose(1, 2).contiguous().view(B, L_seq, self.d_model)\n",
        "        return W_o(concat)\n",
        "\n",
        "    def forward(self, q, mask=None):\n",
        "        k, l, v = q.clone(), q.clone(), q.clone()\n",
        "\n",
        "        out1 = self.forward_path(q, k, l, v, self.W_q1, self.W_k1, self.W_l1, self.W_v1, self.W_o1, self.E1, self.F1, self.G1, mask)\n",
        "        out2 = self.forward_path(q, k, l, v, self.W_q2, self.W_k2, self.W_l2, self.W_v2, self.W_o2, self.E2, self.F2, self.G2, mask)\n",
        "\n",
        "        return out1 + out2\n"
      ],
      "metadata": {
        "id": "9a59_ToApFaJ"
      },
      "execution_count": null,
      "outputs": [],
      "id": "9a59_ToApFaJ"
    },
    {
      "cell_type": "code",
      "source": [
        "class Attn3DLinformerMultiPed_Dual(nn.Module):\n",
        "    \"\"\"\n",
        "    Dual Attention version of MultiPed + Linformer 3D attention.\n",
        "    Runs two independent attention mechanisms (with separate weights)\n",
        "    and sums their outputs.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_heads, d_model, k, len_seq, block_size, stride):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        self.head_dim = d_model // num_heads\n",
        "        self.block_size = block_size\n",
        "        self.stride = stride\n",
        "\n",
        "        # First attention weights\n",
        "        self.W_q1 = nn.Linear(d_model, d_model)\n",
        "        self.W_k1 = nn.Linear(d_model, d_model)\n",
        "        self.W_l1 = nn.Linear(d_model, d_model)\n",
        "        self.W_v1 = nn.Linear(d_model, d_model)\n",
        "        self.W_o1 = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.E1 = nn.Linear(block_size, k)\n",
        "        self.F1 = nn.Linear(block_size, k)\n",
        "        self.G1 = nn.Linear(block_size, k)\n",
        "\n",
        "        # Second attention weights\n",
        "        self.W_q2 = nn.Linear(d_model, d_model)\n",
        "        self.W_k2 = nn.Linear(d_model, d_model)\n",
        "        self.W_l2 = nn.Linear(d_model, d_model)\n",
        "        self.W_v2 = nn.Linear(d_model, d_model)\n",
        "        self.W_o2 = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.E2 = nn.Linear(block_size, k)\n",
        "        self.F2 = nn.Linear(block_size, k)\n",
        "        self.G2 = nn.Linear(block_size, k)\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        bsz, num_peds, len_peds, d_model = x.size()\n",
        "        return x.reshape(bsz, num_peds, len_peds, self.num_heads, self.head_dim).transpose(2, 3)\n",
        "\n",
        "    def self_attention(self, q, k, l, v, mask=None):\n",
        "        mul_qkl = torch.einsum('abcid,abcjd,abckd -> abcijk', q, k, l) /(self.head_dim**0.5)\n",
        "        if mask is not None:\n",
        "            mul_qkl = mul_qkl.masked_fill(mask == 0, -1e9)\n",
        "        attention_scores = softmax_5d(mul_qkl, axis=(-1, -2))\n",
        "        vtilde = torch.einsum('abcid,abcjd->abcijd', v, v)\n",
        "        result = torch.einsum('abcijk,abcjkl->abcil', attention_scores, vtilde)\n",
        "        return result\n",
        "\n",
        "    def _sliding_blocks(self, x):\n",
        "        l = self.block_size\n",
        "        d = self.stride\n",
        "        B, L, D = x.shape\n",
        "        num_blocks = (L - l) // d + 1\n",
        "        shape = (B, num_blocks, l, D)\n",
        "        strides = (x.stride(0), d * x.stride(1), x.stride(1), x.stride(2))\n",
        "        blocks = x.as_strided(shape, strides)\n",
        "        return blocks.contiguous()\n",
        "\n",
        "    def _reconstruct_from_blocks(self, blocks, seq_len):\n",
        "        l = self.block_size\n",
        "        d = self.stride\n",
        "        B, num_blocks, _, D = blocks.shape\n",
        "        device = blocks.device\n",
        "\n",
        "        start_idx = torch.arange(num_blocks, device=device) * d\n",
        "        block_offsets = torch.arange(l, device=device)\n",
        "        positions = start_idx[:, None] + block_offsets[None, :]\n",
        "        positions = positions.unsqueeze(0).expand(B, -1, -1)\n",
        "\n",
        "        flat_pos = positions.reshape(B, -1)\n",
        "        flat_blocks = blocks.reshape(B, -1, D)\n",
        "\n",
        "        out = torch.zeros(B, seq_len, D, device=device)\n",
        "        counts = torch.zeros(B, seq_len, 1, device=device)\n",
        "\n",
        "        out.scatter_add_(1, flat_pos.unsqueeze(-1).expand(-1, -1, D), flat_blocks)\n",
        "        counts.scatter_add_(1, flat_pos.unsqueeze(-1), torch.ones_like(flat_pos, device=device).unsqueeze(-1).float())\n",
        "\n",
        "        return out / counts\n",
        "\n",
        "    def forward_path(self, q, W_q, W_k, W_l, W_v, W_o, E, F, G, mask):\n",
        "        k, l, v = q.clone(), q.clone(), q.clone()\n",
        "        Q = self.split_heads(self._sliding_blocks(W_q(q)))\n",
        "\n",
        "        def project(x, linear_proj, W):\n",
        "            x = self._sliding_blocks(W(x))\n",
        "            x = x.permute(0, 1, 3, 2)\n",
        "            x = linear_proj(x)\n",
        "            x = x.permute(0, 1, 3, 2)\n",
        "            return self.split_heads(x)\n",
        "\n",
        "        K = project(k, E, W_k)\n",
        "        L = project(l, F, W_l)\n",
        "        V = project(v, G, W_v)\n",
        "\n",
        "        attn_output = self.self_attention(Q, K, L, V, mask)\n",
        "        batch_size, num_peds, num_heads, len_ped, len_head = attn_output.shape\n",
        "        attn_output = attn_output.contiguous().view(batch_size, num_peds, len_ped, self.d_model)\n",
        "        attn_output = self._reconstruct_from_blocks(attn_output, q.shape[1])\n",
        "        return W_o(attn_output)\n",
        "\n",
        "    def forward(self, q, mask=None):\n",
        "        out1 = self.forward_path(q, self.W_q1, self.W_k1, self.W_l1, self.W_v1, self.W_o1, self.E1, self.F1, self.G1, mask)\n",
        "        out2 = self.forward_path(q, self.W_q2, self.W_k2, self.W_l2, self.W_v2, self.W_o2, self.E2, self.F2, self.G2, mask)\n",
        "        return out1 + out2\n"
      ],
      "metadata": {
        "id": "eDjGGWZupbJd"
      },
      "execution_count": null,
      "outputs": [],
      "id": "eDjGGWZupbJd"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Attn2DMultiPed3Dselected_Dual(nn.Module):\n",
        "    \"\"\"\n",
        "    Two completely separate Attention Mechanisms with 2D and selected 3D interactions.\n",
        "    Each branch has its own Q, K, V, L projections.\n",
        "    The outputs of the two branches are added together.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_heads: int, d_model: int, stride: int = 10, block_size: int = 40, k=4, k_prime=2):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "        self.stride = stride\n",
        "        self.block_size = block_size\n",
        "        self.k = k\n",
        "        self.k_prime = k_prime\n",
        "\n",
        "        # Branch 1 projections\n",
        "        self.W_q1 = nn.Linear(d_model, d_model)\n",
        "        self.W_k1 = nn.Linear(d_model, d_model)\n",
        "        self.W_v1 = nn.Linear(d_model, d_model)\n",
        "        self.W_l1 = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # Branch 2 projections\n",
        "        self.W_q2 = nn.Linear(d_model, d_model)\n",
        "        self.W_k2 = nn.Linear(d_model, d_model)\n",
        "        self.W_v2 = nn.Linear(d_model, d_model)\n",
        "        self.W_l2 = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # Output projection (shared after adding the two branches)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # Fusion layer (per-branch fusion of 2D and 3D outputs)\n",
        "        self.fusion_layer = nn.Linear(2 * self.head_dim, self.head_dim)\n",
        "\n",
        "    #### Helper functions (same as before) ####\n",
        "    def _split_heads(self, x):\n",
        "        B, Blk, L, D = x.size()\n",
        "        return x.view(B, Blk, L, self.num_heads, self.head_dim).transpose(2, 3)\n",
        "\n",
        "    def _sliding_blocks(self, x):\n",
        "        l, d = self.block_size, self.stride\n",
        "        B, L, D = x.shape\n",
        "        num_blocks = (L - l) // d + 1\n",
        "        shape = (B, num_blocks, l, D)\n",
        "        strides = (x.stride(0), d * x.stride(1), x.stride(1), x.stride(2))\n",
        "        return x.as_strided(shape, strides).contiguous()\n",
        "\n",
        "    def _topk_submats_adv(self, attn_scores: torch.Tensor, topk_idx: torch.Tensor):\n",
        "        B, Blk, H, L, _ = attn_scores.shape\n",
        "        K = topk_idx.shape[-1]\n",
        "        device = attn_scores.device\n",
        "        topk_idx = topk_idx.long().to(device)\n",
        "\n",
        "        # Select rows\n",
        "        b = torch.arange(B, device=device)[:, None, None, None, None].expand(B, Blk, H, L, K)\n",
        "        bl = torch.arange(Blk, device=device)[None, :, None, None, None].expand(B, Blk, H, L, K)\n",
        "        h = torch.arange(H, device=device)[None, None, :, None, None].expand(B, Blk, H, L, K)\n",
        "        q = torch.arange(L, device=device)[None, None, None, :, None].expand(B, Blk, H, L, K)\n",
        "        row_selected = attn_scores[b, bl, h, topk_idx, :]  # (B, Blk, H, L, K, L)\n",
        "\n",
        "        # Select cols\n",
        "        b2 = torch.arange(B, device=device)[:, None, None, None, None, None].expand(B, Blk, H, L, K, K)\n",
        "        bl2 = torch.arange(Blk, device=device)[None, :, None, None, None, None].expand(B, Blk, H, L, K, K)\n",
        "        h2 = torch.arange(H, device=device)[None, None, :, None, None, None].expand(B, Blk, H, L, K, K)\n",
        "        q2 = torch.arange(L, device=device)[None, None, None, :, None, None].expand(B, Blk, H, L, K, K)\n",
        "        cols = topk_idx.unsqueeze(-2).expand(B, Blk, H, L, K, K)\n",
        "\n",
        "        sub = row_selected[b2, bl2, h2, q2, torch.arange(K, device=device)[None, None, None, None, :, None], cols]\n",
        "        return sub\n",
        "\n",
        "    def _reconstruct_from_blocks(self, blocks, seq_len):\n",
        "        l, d = self.block_size, self.stride\n",
        "        B, num_blocks, _, D = blocks.shape\n",
        "        device = blocks.device\n",
        "        start_idx = torch.arange(num_blocks, device=device) * d\n",
        "        block_offsets = torch.arange(l, device=device)\n",
        "        positions = (start_idx[:, None] + block_offsets).unsqueeze(0).expand(B, -1, -1)\n",
        "        flat_pos, flat_blocks = positions.reshape(B, -1), blocks.reshape(B, -1, D)\n",
        "        out = torch.zeros(B, seq_len, D, device=device)\n",
        "        counts = torch.zeros(B, seq_len, 1, device=device)\n",
        "        out.scatter_add_(1, flat_pos.unsqueeze(-1).expand(-1, -1, D), flat_blocks)\n",
        "        counts.scatter_add_(1, flat_pos.unsqueeze(-1), torch.ones_like(flat_pos, device=device).unsqueeze(-1).float())\n",
        "        return out / counts\n",
        "\n",
        "    def _gather_partners(self, tensor, partner_idx):\n",
        "        return torch.gather(tensor, 4, partner_idx.unsqueeze(-1).expand(*partner_idx.shape, self.head_dim))\n",
        "\n",
        "    def _self_attention_2d(self, q, k, v, mask=None):\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask.unsqueeze(2).unsqueeze(4) == 0, -1e9)\n",
        "        attn_scores = torch.softmax(scores, dim=-1)\n",
        "        return attn_scores, torch.matmul(attn_scores, v)\n",
        "\n",
        "    def compute_selected_mask(self, mask, partner1, partner2, L):\n",
        "        if mask is None:\n",
        "            return None\n",
        "        mask_reshaped1 = mask.unsqueeze(2).expand(-1, -1, self.num_heads, -1)\n",
        "        mask_reshaped2 = mask_reshaped1.unsqueeze(-2).expand(-1, -1, -1, L, -1)\n",
        "        mask1_comp = torch.gather(mask_reshaped2, 4, partner1)\n",
        "        mask2_comp = torch.gather(mask_reshaped2, 4, partner2)\n",
        "        two_way_interac_mask = mask1_comp * mask2_comp\n",
        "        mask_comp3 = mask_reshaped1.unsqueeze(-1).expand(-1, -1, -1, -1, partner1.size(-1))\n",
        "        return mask_comp3 * two_way_interac_mask\n",
        "\n",
        "    #### Branch-specific computation ####\n",
        "    def _branch_attention(self, Q, Key, V, L_mat, mask):\n",
        "        attn_scores, attn_out_2d = self._self_attention_2d(Q, Key, V, mask=mask)\n",
        "\n",
        "        topk_scores, topk_idx = torch.topk(attn_scores, self.k, dim=-1)\n",
        "        outer = topk_scores[..., :, None] * topk_scores[..., None, :]\n",
        "        selected = self._topk_submats_adv(attn_scores, topk_idx)\n",
        "        res = outer * selected\n",
        "        B, Blk, H, L, K, _ = res.shape\n",
        "        _, topk_exp = torch.topk(res.view(B, Blk, H, L, -1), self.k_prime, dim=-1)\n",
        "        row_idx, col_idx = topk_exp // self.k, topk_exp % self.k_prime\n",
        "        partner1, partner2 = torch.gather(topk_idx, -1, row_idx), torch.gather(topk_idx, -1, col_idx)\n",
        "\n",
        "        K_exp = Key.unsqueeze(-3).expand(-1, -1, -1, L, -1, -1)\n",
        "        L_exp = L_mat.unsqueeze(-3).expand(-1, -1, -1, L, -1, -1)\n",
        "        V_exp = V.unsqueeze(-3).expand(-1, -1, -1, L, -1, -1)\n",
        "\n",
        "        k_sel = self._gather_partners(K_exp, partner1)\n",
        "        l_sel = self._gather_partners(L_exp, partner2)\n",
        "        v1 = self._gather_partners(V_exp, partner1)\n",
        "        v2 = self._gather_partners(V_exp, partner2)\n",
        "\n",
        "        q_exp = Q.unsqueeze(-2)\n",
        "        dot_qkl = (q_exp * k_sel * l_sel).sum(-1)\n",
        "        final_mask = self.compute_selected_mask(mask, partner1, partner2, L)\n",
        "        if final_mask is not None:\n",
        "            dot_qkl = dot_qkl.masked_fill(final_mask == 0, -1e9)\n",
        "\n",
        "        attn_weights_3d = torch.softmax(dot_qkl, dim=-1)\n",
        "        v_sq = v1 * v2\n",
        "        dot_qkl_exp = attn_weights_3d.unsqueeze(-1).expand(-1, -1, -1, -1, -1, self.head_dim)\n",
        "        res_3d = (dot_qkl_exp * v_sq).sum(-2)\n",
        "\n",
        "        res_combined = torch.cat([attn_out_2d, res_3d], dim=-1)\n",
        "        return self.fusion_layer(res_combined)\n",
        "\n",
        "    #### Forward ####\n",
        "    def forward(self, x, mask=None):\n",
        "        B, L, D = x.shape\n",
        "\n",
        "        # Branch 1\n",
        "        q1, k1, v1, l1 = self.W_q1(x), self.W_k1(x), self.W_v1(x), self.W_l1(x)\n",
        "        Q1, K1, V1, L1 = map(lambda t: self._split_heads(self._sliding_blocks(t)), (q1, k1, v1, l1))\n",
        "        out1 = self._branch_attention(Q1, K1, V1, L1, mask)\n",
        "\n",
        "        # Branch 2\n",
        "        q2, k2, v2, l2 = self.W_q2(x), self.W_k2(x), self.W_v2(x), self.W_l2(x)\n",
        "        Q2, K2, V2, L2 = map(lambda t: self._split_heads(self._sliding_blocks(t)), (q2, k2, v2, l2))\n",
        "        out2 = self._branch_attention(Q2, K2, V2, L2, mask)\n",
        "\n",
        "        # Add branches\n",
        "        summed = out1 + out2\n",
        "        B, Blk, H, L_blk, hd = summed.shape\n",
        "        out = summed.transpose(2, 3).contiguous().view(B, Blk, L_blk, self.d_model)\n",
        "\n",
        "        # Reconstruct and project\n",
        "        recon = self._reconstruct_from_blocks(out, x.shape[1])\n",
        "        return self.W_o(recon)\n"
      ],
      "metadata": {
        "id": "JmUvg7C3q6ks"
      },
      "execution_count": null,
      "outputs": [],
      "id": "JmUvg7C3q6ks"
    },
    {
      "cell_type": "markdown",
      "id": "955FF5wm9tzS",
      "metadata": {
        "id": "955FF5wm9tzS"
      },
      "source": [
        "#### Combined 2D with 3D Multi-Ped Linformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Bq6n6gIU9tzS",
      "metadata": {
        "id": "Bq6n6gIU9tzS"
      },
      "outputs": [],
      "source": [
        "class Attn2D3D_MultiPedLinAlter(nn.Module):\n",
        "\n",
        "    \"\"\"\n",
        "    Combined 2D and 3D Multi-Ped Attention\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,num_heads, d_model, num_peds, len_seq, k):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_params = d_model//num_heads\n",
        "        self.num_peds = num_peds\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, d_model) # mapping to the same number of features\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_l = nn.Linear(d_model,d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # Linformer Projections\n",
        "        self.E = nn.Linear(len_seq, k)\n",
        "        self.F = nn.Linear(len_seq, k)\n",
        "        self.G = nn.Linear(len_seq, k)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(self.head_params)\n",
        "        self.norm2 = nn.LayerNorm(self.head_params)\n",
        "\n",
        "        self.linear = nn.Linear(2*self.head_params, self.head_params)\n",
        "\n",
        "    def self_attention2D(self, q, k, v, mask = None):\n",
        "        # Calculate the dot produce of Q and K\n",
        "        dotqk = torch.matmul(q, k.transpose(-2,-1))/(self.head_params**0.5) # shape = (#samples, num_heads, len_seq, len_seq)\n",
        "        #print(f'shape of dotqk 2D Attn: {dotqk.shape}')\n",
        "        if mask is not None:\n",
        "            #print(f'shape of the mask applied: {mask.shape}')\n",
        "            dotqk = dotqk.masked_fill(mask == 0, -1e9)\n",
        "            #print(f'mask applied in 2D attn')\n",
        "            #print(f'mask applied to 2D attention. Shape of the mask: {mask.shape}')\n",
        "        # Apply the Softmax to the attention weights\n",
        "        attention_scores = torch.softmax(dotqk, dim = -1)\n",
        "        # Multiply to the value matrix\n",
        "        result = torch.matmul(attention_scores, v)\n",
        "        return(result)\n",
        "\n",
        "    def self_attention3D(self, q, k,l, v,mask = None):\n",
        "        # Save the length of the sequence\n",
        "        seq_len = k.shape[2]\n",
        "        # Calculate the dot produce of Q and K\n",
        "        mul_qkl = torch.einsum('abcid,abcjd,abckd -> abcijk', q, k, l) /(self.head_params**0.5)# Shape: (#samples,num_heads,len_seq,len_seq, len_seq)\n",
        "        #print(f'shape of mul_qkl: {mul_qkl.shape}')\n",
        "        if mask is not None:\n",
        "            #print(f'shape of the mask applied: {mask.shape}')\n",
        "            mul_qkl = mul_qkl.masked_fill(mask == 0, -1e9)\n",
        "            #print(f'mask applied to 3D multiped Linformer; shape of the mask: {mask.shape}')\n",
        "        # Apply the Softmax to the attention weights\n",
        "        attention_scores = softmax_5d(mul_qkl, axis = (-1,-2))\n",
        "        # Calculate V^2\n",
        "        vtilde = torch.einsum('abcil,abcjl->abcijl', v, v)\n",
        "        # Multiply to the value matrix\n",
        "        result = torch.einsum('abcijk,abcjkl->abcil', attention_scores, vtilde)\n",
        "        #print(f'shape of attention results: {result.shape}')\n",
        "        return(result)\n",
        "\n",
        "    def split_heads(self, q):\n",
        "\n",
        "        \"\"\"\n",
        "        Reshaping\n",
        "        Input of shape : (#samples, seq_len, d_model)\n",
        "\n",
        "        output of shape: (#samples, num_heads, seq_len, head_parameters)\n",
        "        \"\"\"\n",
        "        samples, seq_len, d_model = q.shape\n",
        "        # Check if dimensions are valid\n",
        "        assert d_model % self.num_heads == 0, \"d_model is not divisable by the number of heads\"\n",
        "        return q.view(samples, seq_len, self.num_heads, self.head_params).transpose(1,2)\n",
        "\n",
        "\n",
        "    def split_sequence(self,q):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            q (Tensor): Input tensor of shape (samples, num_heads, seq_len, d_model).\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Reshaped tensor of shape (samples, num_heads, num_peds, seq_len // num_peds, d_model).\n",
        "        \"\"\"\n",
        "\n",
        "        samples, num_heads, seq_len, head_params = q.shape\n",
        "        assert seq_len % self.num_peds == 0, \"Sequence length is not divisible by number of peds\"\n",
        "        #print(f'shape of q: {q.shape}')\n",
        "\n",
        "        return q.reshape(samples, self.num_peds, num_heads, seq_len // self.num_peds, head_params)\n",
        "\n",
        "\n",
        "    def forward(self, q, mask = None):\n",
        "        #print(f'combined 2D and 3D multiped is activated')\n",
        "\n",
        "        # Clone Query to define key, utinity, and value matrices\n",
        "        k = q.clone()\n",
        "        l = q.clone()\n",
        "        v = q.clone()\n",
        "        mask_2D = None\n",
        "        mask_3DMP = None\n",
        "\n",
        "        # Unpack the masks\n",
        "        if mask is not None:\n",
        "          mask_2D = mask[0]\n",
        "          #print(f'mask_2D{mask_2D}, shape = {mask_2D.shape}')\n",
        "          mask_3DMP = mask[1]\n",
        "          #print(f'mask_3DMP{mask_3DMP}, shape = {mask_3DMP.shape}')\n",
        "\n",
        "        Query_2D = self.split_heads(self.W_q(q))\n",
        "        Key_2D  = self.split_heads(self.W_k(k))\n",
        "        Value_2D = self.split_heads(self.W_v(v))\n",
        "\n",
        "        # Run the 2D self-attention\n",
        "        attn_outputs_2D = self.self_attention2D(Query_2D, Key_2D, Value_2D, mask = mask_2D) # (batch_size, num_heads, len_seq, head_params)\n",
        "\n",
        "        # Define the Query, Key, Utinity and Value matrices for Multi-Ped 3D\n",
        "        Query_3D = self.split_sequence(Query_2D)\n",
        "        Key_3D  = self.split_sequence(Key_2D)\n",
        "        Value_3D = self.split_sequence(Value_2D)\n",
        "        L_matrix = self.split_sequence(self.split_heads(self.W_l(l)))\n",
        "\n",
        "        # Run MultiPed 3D attention\n",
        "        attn_outputs_3DMultiPed= self.self_attention3D(Query_3D, Key_3D, L_matrix, Value_3D, mask = mask_3DMP) # (batch_size, num_peds, num_heads, len_ped, head_params)\n",
        "        #print(f'shape of attention 2D output: {attn_outputs_3DMultiPed.shape}')\n",
        "\n",
        "        # Combine the peds\n",
        "        batch_size, num_peds, num_heads, len_ped ,len_head = attn_outputs_3DMultiPed.shape\n",
        "        attn_outputs_3DMultiPed = attn_outputs_3DMultiPed.contiguous().view(batch_size, num_heads, num_peds*len_ped, len_head)\n",
        "        #print(f'shape of attention 3D multi-ped output after combining heads {attn_outputs_3DMultiPed.shape}')\n",
        "\n",
        "        # Concatenate the attentions\n",
        "        attn_outputs_concat = torch.cat((attn_outputs_2D, attn_outputs_3DMultiPed), dim = -1)\n",
        "\n",
        "        # Combine the attentions\n",
        "        attn_outputs_combined = self.linear(attn_outputs_concat)\n",
        "        print(f'shape of attn_outputs_combined: {attn_outputs_combined.shape}')\n",
        "\n",
        "        # Concatenate the heads\n",
        "        batch_size, _,seq_len, _= attn_outputs_combined.shape\n",
        "        attn_outputs_combined = attn_outputs_combined.transpose(1,2).contiguous().view(batch_size, seq_len, self.d_model) #shape = (#samples, len_seq, d_model)\n",
        "\n",
        "        # Apply through the last linear layer\n",
        "        result_final = self.W_o(attn_outputs_combined) #shape = (#samples, len_seq, d_model)\n",
        "\n",
        "        return result_final"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "guQM6nb39tzS",
      "metadata": {
        "id": "guQM6nb39tzS"
      },
      "source": [
        "#### Attention 2D overlapping blocks and 3D selected"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ll433HFm9tzS",
      "metadata": {
        "id": "ll433HFm9tzS"
      },
      "outputs": [],
      "source": [
        "class Attn2DMultiPed3Dselected(nn.Module):\n",
        "    \"\"\"\n",
        "  Attention Mechanism with 2D and selected 3D interactions.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads: int, d_model: int, stride: int = 10, block_size: int = 40, k=4, k_prime=2):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "        self.stride = stride\n",
        "        self.block_size = block_size\n",
        "        self.k = k\n",
        "        self.k_prime = k_prime\n",
        "\n",
        "        # Projection layers\n",
        "        self.W_q, self.W_k, self.W_v, self.W_o, self.W_l = [nn.Linear(d_model, d_model) for _ in range(5)]\n",
        "\n",
        "        # Fusion layer for combining 2D & 3D attention\n",
        "        self.fusion_layer = nn.Linear(2 * self.head_dim, self.head_dim)\n",
        "\n",
        "    #### Helper functions\n",
        "\n",
        "    def _split_heads(self, x):\n",
        "        \"\"\"Split embeddings into multiple heads.\"\"\"\n",
        "        B, Blk, L, D = x.size()\n",
        "        return x.view(B, Blk, L, self.num_heads, self.head_dim).transpose(2, 3)\n",
        "\n",
        "    def _sliding_blocks(self, x):\n",
        "        \"\"\"Create sliding-window blocks from the sequence.\"\"\"\n",
        "        l, d, (B, L, D) = self.block_size, self.stride, x.shape\n",
        "        num_blocks = (L - l) // d + 1\n",
        "        shape = (B, num_blocks, l, D)\n",
        "        strides = (x.stride(0), d * x.stride(1), x.stride(1), x.stride(2))\n",
        "        return x.as_strided(shape, strides).contiguous()\n",
        "\n",
        "    def _topk_submats_adv(self, attn_scores: torch.Tensor, topk_idx: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Extract submatrices of attention for top-k indices.\n",
        "\n",
        "        Args:\n",
        "            attn_scores (Tensor): (B, Blk, H, L, L).\n",
        "            topk_idx (Tensor): (B, Blk, H, L, K).\n",
        "\n",
        "        Returns:\n",
        "            Tensor: (B, Blk, H, L, K, K).\n",
        "        \"\"\"\n",
        "        B, Blk, H, L, _ = attn_scores.shape\n",
        "        K = topk_idx.shape[-1]\n",
        "        device = attn_scores.device\n",
        "        topk_idx = topk_idx.long().to(device)\n",
        "\n",
        "        # Select rows\n",
        "        b = torch.arange(B, device=device)[:, None, None, None, None].expand(B, Blk, H, L, K)\n",
        "        bl = torch.arange(Blk, device=device)[None, :, None, None, None].expand(B, Blk, H, L, K)\n",
        "        h = torch.arange(H, device=device)[None, None, :, None, None].expand(B, Blk, H, L, K)\n",
        "        q = torch.arange(L, device=device)[None, None, None, :, None].expand(B, Blk, H, L, K)\n",
        "\n",
        "        row_selected = attn_scores[b, bl, h, topk_idx, :]  # (B, Blk, H, L, K, L)\n",
        "\n",
        "        # Select cols\n",
        "        b2 = torch.arange(B, device=device)[:, None, None, None, None, None].expand(B, Blk, H, L, K, K)\n",
        "        bl2 = torch.arange(Blk, device=device)[None, :, None, None, None, None].expand(B, Blk, H, L, K, K)\n",
        "        h2 = torch.arange(H, device=device)[None, None, :, None, None, None].expand(B, Blk, H, L, K, K)\n",
        "        q2 = torch.arange(L, device=device)[None, None, None, :, None, None].expand(B, Blk, H, L, K, K)\n",
        "\n",
        "        cols = topk_idx.unsqueeze(-2).expand(B, Blk, H, L, K, K)\n",
        "        sub = row_selected[b2, bl2, h2, q2, torch.arange(K, device=device)[None, None, None, None, :, None], cols]\n",
        "        return sub\n",
        "\n",
        "    def _reconstruct_from_blocks(self, blocks, seq_len):\n",
        "        \"\"\"Reconstruct sequence from overlapping sliding blocks.\"\"\"\n",
        "        l, d, (B, num_blocks, _, D) = self.block_size, self.stride, blocks.shape\n",
        "        device = blocks.device\n",
        "        start_idx = torch.arange(num_blocks, device=device) * d\n",
        "        block_offsets = torch.arange(l, device=device)\n",
        "        positions = (start_idx[:, None] + block_offsets).unsqueeze(0).expand(B, -1, -1)\n",
        "        flat_pos, flat_blocks = positions.reshape(B, -1), blocks.reshape(B, -1, D)\n",
        "        out, counts = torch.zeros(B, seq_len, D, device=device), torch.zeros(B, seq_len, 1, device=device)\n",
        "        out.scatter_add_(1, flat_pos.unsqueeze(-1).expand(-1, -1, D), flat_blocks)\n",
        "        counts.scatter_add_(1, flat_pos.unsqueeze(-1), torch.ones_like(flat_pos, device=device).unsqueeze(-1).float())\n",
        "        return out / counts\n",
        "\n",
        "    def _expand_for_gather(self, tensor, expand_dims, expand_sizes):\n",
        "        \"\"\"Utility for repeated unsqueeze + expand before gather.\"\"\"\n",
        "        for axis, size in zip(expand_dims, expand_sizes):\n",
        "            tensor = tensor.unsqueeze(axis).expand(*size)\n",
        "        return tensor\n",
        "\n",
        "    def _gather_partners(self, tensor, partner_idx):\n",
        "        \"\"\"Helper for gathering partner-specific selections (K, L, or V).\"\"\"\n",
        "        return torch.gather(tensor, 4, partner_idx.unsqueeze(-1).expand(*partner_idx.shape, self.head_dim))\n",
        "\n",
        "    def _self_attention_2d(self, q, k, v, mask=None):\n",
        "        \"\"\"Standard scaled dot-product attention (2D).\"\"\"\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask.unsqueeze(2).unsqueeze(4) == 0, -1e9)\n",
        "        attn_scores = torch.softmax(scores, dim=-1)\n",
        "        return attn_scores, torch.matmul(attn_scores, v)\n",
        "\n",
        "    def compute_selected_mask(self, mask, partner1, partner2, L):\n",
        "        \"\"\"Compute the final interaction mask for 3D attention.\"\"\"\n",
        "        if mask is None:\n",
        "            return None\n",
        "        mask_reshaped1 = mask.unsqueeze(2).expand(-1, -1, self.num_heads, -1)\n",
        "        mask_reshaped2 = mask_reshaped1.unsqueeze(-2).expand(-1, -1, -1, L, -1)\n",
        "        mask1_comp, mask2_comp = torch.gather(mask_reshaped2, 4, partner1), torch.gather(mask_reshaped2, 4, partner2)\n",
        "        two_way_interac_mask = mask1_comp * mask2_comp\n",
        "        mask_comp3 = mask_reshaped1.unsqueeze(-1).expand(-1, -1, -1, -1, partner1.size(-1))\n",
        "        return mask_comp3 * two_way_interac_mask\n",
        "\n",
        "    ####\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        \"\"\"Forward pass through Multi-Ped Attention.\"\"\"\n",
        "        #print(f'shape of x inputted: {x.shape}')\n",
        "        q, k, v, l = self.W_q(x), self.W_k(x), self.W_v(x), self.W_l(x)\n",
        "        Q, Key, V, L_mat = map(lambda t: self._split_heads(self._sliding_blocks(t)), (q, k, v, l))\n",
        "\n",
        "        # 2D Attention\n",
        "        attn_scores, attn_out_2d = self._self_attention_2d(Q, Key, V, mask=mask)\n",
        "\n",
        "        # Top-k selection\n",
        "        topk_scores, topk_idx = torch.topk(attn_scores, self.k, dim=-1)\n",
        "        outer = topk_scores[..., :, None] * topk_scores[..., None, :]\n",
        "        selected = self._topk_submats_adv(attn_scores, topk_idx)\n",
        "        res = outer * selected\n",
        "        B, Blk, H, L, K, _ = res.shape\n",
        "        _, topk_exp = torch.topk(res.view(B, Blk, H, L, -1), self.k_prime, dim=-1)\n",
        "        row_idx, col_idx = topk_exp // self.k, topk_exp % self.k_prime\n",
        "        partner1, partner2 = torch.gather(topk_idx, -1, row_idx), torch.gather(topk_idx, -1, col_idx)\n",
        "\n",
        "        # 3D Attention\n",
        "        K_exp, L_exp, V_exp = (Key, L_mat, V)\n",
        "        K_exp, L_exp, V_exp = (t.unsqueeze(-3).expand(-1, -1, -1, L, -1, -1) for t in (K_exp, L_exp, V_exp))\n",
        "        k_sel, l_sel, v1, v2 = (self._gather_partners(K_exp, partner1),\n",
        "                                self._gather_partners(L_exp, partner2),\n",
        "                                self._gather_partners(V_exp, partner1),\n",
        "                                self._gather_partners(V_exp, partner2))\n",
        "        q_exp = Q.unsqueeze(-2)\n",
        "        dot_qkl = (q_exp * k_sel * l_sel).sum(-1)\n",
        "        # print(f'shape of dotqkl: {dot_qkl.shape}')\n",
        "        final_mask = self.compute_selected_mask(mask, partner1, partner2, L)\n",
        "        if final_mask is not None:\n",
        "            dot_qkl = dot_qkl.masked_fill(final_mask == 0, -1e9)\n",
        "            # print(f'shape of mask: {final_mask.shape}')\n",
        "        attn_weights_3d = torch.softmax(dot_qkl, dim=-1)\n",
        "\n",
        "        # Apply to values\n",
        "        v_sq, dot_qkl_exp = v1 * v2, attn_weights_3d.unsqueeze(-1).expand(-1, -1, -1, -1, -1, self.head_dim)\n",
        "        res_3d = (dot_qkl_exp * v_sq).sum(-2)\n",
        "\n",
        "        # Fusion\n",
        "        res_combined = torch.cat([attn_out_2d, res_3d], dim=-1)\n",
        "        out = self.fusion_layer(res_combined).view(B, Blk, L, self.d_model)\n",
        "\n",
        "        # Reconstruct from blocks\n",
        "        return self.W_o(self._reconstruct_from_blocks(out, x.shape[1]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Vm5KwmNU9tzS",
      "metadata": {
        "id": "Vm5KwmNU9tzS"
      },
      "source": [
        "#### 3D Attention with Multi-Ped"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dN6ttuhz9tzS",
      "metadata": {
        "id": "dN6ttuhz9tzS"
      },
      "outputs": [],
      "source": [
        "class Attn3D_MultiPed(nn.Module):\n",
        "    \"\"\"\n",
        "    num_peds defaults to 10\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,num_heads, d_model, num_peds):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_params = d_model//num_heads\n",
        "        self.num_peds = num_peds\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, d_model) # mapping to the same number of features\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_l = nn.Linear(d_model,d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def self_attention(self, q, k,l, v,mask = None):\n",
        "        # Save the length of the sequence\n",
        "        seq_len = k.shape[2]\n",
        "        # Calculate the dot produce of Q and K\n",
        "        mul_qkl = torch.einsum('abcid,abcjd,abckd -> abcijk', q, k, l) /(self.head_params**0.5)# Shape: (#samples,num_heads,len_seq,len_seq, len_seq)\n",
        "        #print(f'shape of mul_qkl: {mul_qkl.shape}')\n",
        "        if mask is not None:\n",
        "            #print(f'mask given to Multi3D: {mask.shape}')\n",
        "            mul_qkl = mul_qkl.masked_fill(mask == 0, -1e9) ######changed\n",
        "            #print(f'mask applied')\n",
        "        # Apply the Softmax to the attention weights\n",
        "        attention_scores = softmax_5d(mul_qkl, axis = (-1,-2))\n",
        "        # Calculate V^2\n",
        "        vtilde = torch.einsum('abcil,abcjl->abcijl', v, v)\n",
        "        # Multiply to the value matrix\n",
        "        result = torch.einsum('abcijk,abcjkl->abcil', attention_scores, vtilde)\n",
        "        #print(f'shape of attention results: {result.shape}')\n",
        "        return(result)\n",
        "\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        # Shape: (batch, seq_len, d_model) --> (batch, num_heads, seq_len, head_dim)\n",
        "        bsz, num_peds, len_peds, d_model = x.size()\n",
        "        return x.reshape(bsz, num_peds, len_peds, self.num_heads, self.head_params).transpose(2,3)\n",
        "\n",
        "    def split_sequence(self, q):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            q (Tensor): Input tensor of shape (samples, num_heads, seq_len, d_model).\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Reshaped tensor of shape (samples, num_heads, num_peds, seq_len // num_peds, d_model).\n",
        "        \"\"\"\n",
        "\n",
        "        samples, seq_len, d_model = q.shape\n",
        "        assert seq_len % self.num_peds == 0, \"Sequence length is not divisible by number of peds\"\n",
        "        #print(f'shape of q: {q.shape}')\n",
        "\n",
        "        return q.reshape(samples, self.num_peds, seq_len // self.num_peds, d_model)\n",
        "\n",
        "\n",
        "    def forward(self, q,mask = None):\n",
        "        k = q.clone()\n",
        "        l = q.clone()\n",
        "        v = q.clone()\n",
        "        #print(f'Multi-Ped is activated')\n",
        "        # Define Query, Key, Value\n",
        "        Query = self.split_heads(self.split_sequence(self.W_q(q)))\n",
        "        Key  = self.split_heads(self.split_sequence(self.W_k(k)))\n",
        "        L_matrix = self.split_heads(self.split_sequence(self.W_l(l)))\n",
        "        Value = self.split_heads(self.split_sequence(self.W_v(v)))\n",
        "        #print(f'shape of Query: {Query.shape}')\n",
        "        #print(f'shape of Key: {Key.shape}')\n",
        "        #print(f'shape of Value: {Value.shape}')\n",
        "\n",
        "        # Run the self-attention\n",
        "        attn_output = self.self_attention(Query, Key,L_matrix, Value, mask = mask)\n",
        "        #print(f'shape of the attn outputs received:{attn_output.shape}')\n",
        "\n",
        "        batch_size, num_peds, num_heads, len_ped ,len_head = attn_output.shape\n",
        "        attn_output = attn_output.contiguous().view(batch_size, num_peds, len_ped, self.d_model)\n",
        "        #print(f'shape of attention output after combining heads {attn_output.shape}')\n",
        "\n",
        "        # Combine peds -> (batch_size, len_peds * num_peds, d_model)\n",
        "        attn_output = attn_output.contiguous().view(batch_size, len_ped * num_peds, self.d_model)\n",
        "        #print(f'shape of attention output after combining the peds {attn_output.shape}')\n",
        "\n",
        "        # Apply through the last linear layer\n",
        "        result_final = self.W_o(attn_output) #shape = (#samples, len_seq, d_model)\n",
        "        #print(f'shape of final output: {result_final.shape}')\n",
        "\n",
        "        return result_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oEqnbjAA9tzS",
      "metadata": {
        "id": "oEqnbjAA9tzS"
      },
      "outputs": [],
      "source": [
        "class Attn3D_MultiPed(nn.Module):\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,num_heads, d_model, block_size, stride):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_params = d_model//num_heads\n",
        "        self.block_size = block_size\n",
        "        self.stride = stride\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, d_model) # mapping to the same number of features\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_l = nn.Linear(d_model,d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def self_attention(self, q, k,l, v,mask = None):\n",
        "        # Save the length of the sequence\n",
        "        seq_len = k.shape[2]\n",
        "        # Calculate the dot produce of Q and K\n",
        "        mul_qkl = torch.einsum('abcid,abcjd,abckd -> abcijk', q, k, l) /(self.head_params**0.5)# Shape: (#samples,num_heads,len_seq,len_seq, len_seq)\n",
        "        #print(f'shape of mul_qkl: {mul_qkl.shape}')\n",
        "        if mask is not None:\n",
        "            mul_qkl = mul_qkl.masked_fill(mask == 0, -1e9) ######changed\n",
        "            #print(f'mask applied in the sliding blocks Multiped 3D')\n",
        "            #print(f'shape of mask applied: {mask.shape}')\n",
        "        # Apply the Softmax to the attention weights\n",
        "        attention_scores = softmax_5d(mul_qkl, axis = (-1,-2))\n",
        "        # Calculate V^2\n",
        "        vtilde = torch.einsum('abcil,abcjl->abcijl', v, v)\n",
        "        # Multiply to the value matrix\n",
        "        result = torch.einsum('abcijk,abcjkl->abcil', attention_scores, vtilde)\n",
        "        #print(f'shape of attention results: {result.shape}')\n",
        "        return(result)\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        # Shape: (batch, seq_len, d_model) --> (batch, num_heads, seq_len, head_dim)\n",
        "        bsz, num_peds, len_peds, d_model = x.size()\n",
        "        return x.reshape(bsz, num_peds, len_peds, self.num_heads, self.head_params).transpose(2,3)\n",
        "\n",
        "    def _sliding_blocks(self, x):\n",
        "        \"\"\"\n",
        "        Create sliding-window blocks from the sequence.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): (B, L, D).\n",
        "\n",
        "        Returns:\n",
        "            Tensor: (B, num_blocks, block_size, D).\n",
        "        \"\"\"\n",
        "        l = self.block_size\n",
        "        d = self.stride\n",
        "        B, L, D = x.shape\n",
        "\n",
        "        num_blocks = (L - l) // d + 1\n",
        "\n",
        "        # Compute new shape and strides\n",
        "        shape = (B, num_blocks, l, D)\n",
        "        strides = (\n",
        "            x.stride(0),        # batch stride\n",
        "            d * x.stride(1),    # jump d steps for each block\n",
        "            x.stride(1),        # step 1 inside a block\n",
        "            x.stride(2),        # feature stride\n",
        "        )\n",
        "\n",
        "        blocks = x.as_strided(shape, strides)\n",
        "        return blocks.contiguous()\n",
        "\n",
        "    def _reconstruct_from_blocks(self, blocks, seq_len):\n",
        "        \"\"\"\n",
        "        Reconstruct sequence from overlapping sliding blocks.\n",
        "\n",
        "        Args:\n",
        "            blocks (Tensor): (B, num_blocks, block_size, D).\n",
        "            seq_len (int): Original sequence length.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: (B, seq_len, D).\n",
        "        \"\"\"\n",
        "        l = self.block_size\n",
        "        d = self.stride\n",
        "        B, num_blocks, _, D = blocks.shape\n",
        "\n",
        "        device = blocks.device\n",
        "\n",
        "        # Build index map: (num_blocks, block_size) -> positions in seq_len\n",
        "        start_idx = torch.arange(num_blocks, device=device) * d\n",
        "        block_offsets = torch.arange(l, device=device)\n",
        "        positions = start_idx[:, None] + block_offsets[None, :]  # (num_blocks, block_size)\n",
        "\n",
        "        # Expand to batch and feature dims\n",
        "        positions = positions.unsqueeze(0).expand(B, -1, -1)  # (B, num_blocks, block_size)\n",
        "\n",
        "        # Flatten everything for scatter\n",
        "        flat_pos = positions.reshape(B, -1)  # (B, num_blocks * block_size)\n",
        "        flat_blocks = blocks.reshape(B, -1, D)  # (B, num_blocks * block_size, D)\n",
        "\n",
        "        # Allocate output\n",
        "        out = torch.zeros(B, seq_len, D, device=device)\n",
        "        counts = torch.zeros(B, seq_len, 1, device=device)\n",
        "\n",
        "        # Scatter add the block values into the right positions\n",
        "        out.scatter_add_(1, flat_pos.unsqueeze(-1).expand(-1, -1, D), flat_blocks)\n",
        "        counts.scatter_add_(1, flat_pos.unsqueeze(-1), torch.ones_like(flat_pos, device=device).unsqueeze(-1).float())\n",
        "\n",
        "        return out / counts\n",
        "\n",
        "    def forward(self, q,mask = None):\n",
        "        k = q.clone()\n",
        "        l = q.clone()\n",
        "        v = q.clone()\n",
        "        # Define Query, Key, Value\n",
        "        Query = self.split_heads(self._sliding_blocks(self.W_q(q)))\n",
        "        Key  = self.split_heads(self._sliding_blocks(self.W_k(k)))\n",
        "        L_matrix = self.split_heads(self._sliding_blocks(self.W_l(l)))\n",
        "        Value = self.split_heads(self._sliding_blocks(self.W_v(v)))\n",
        "        #print(f'shape of Query: {Query.shape}')\n",
        "        #print(f'shape of Key: {Key.shape}')\n",
        "        #print(f'shape of Value: {Value.shape}')\n",
        "\n",
        "        # Run the self-attention\n",
        "        attn_output = self.self_attention(Query, Key,L_matrix, Value, mask = mask)\n",
        "        #print(f'shape of the attn outputs received:{attn_output.shape}')\n",
        "\n",
        "        batch_size, num_peds, num_heads, len_ped ,len_head = attn_output.shape\n",
        "        attn_output = attn_output.contiguous().view(batch_size, num_peds, len_ped, self.d_model)\n",
        "        #print(f'shape of attention output after combining heads {attn_output.shape}')\n",
        "\n",
        "        # Combine peds -> (batch_size, len_peds * num_peds, d_model)\n",
        "        len_seq = q.shape[1]\n",
        "        attn_output = self._reconstruct_from_blocks(attn_output,len_seq)\n",
        "        #print(f'shape of attention output after combining the peds {attn_output.shape}')\n",
        "\n",
        "        # Apply through the last linear layer\n",
        "        result_final = self.W_o(attn_output) #shape = (#samples, len_seq, d_model)\n",
        "        #print(f'shape of final output: {result_final.shape}')\n",
        "\n",
        "        return result_final"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "j1_CopA59tzT",
      "metadata": {
        "id": "j1_CopA59tzT"
      },
      "source": [
        "#### 3D Linformer Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QGcQTn_69tzT",
      "metadata": {
        "id": "QGcQTn_69tzT"
      },
      "outputs": [],
      "source": [
        "class Attn3DLinformer(nn.Module):\n",
        "    \"\"\"\n",
        "    Modified Multi-Head Attention with 3D product using E, F, G projections.\n",
        "\n",
        "    Args:\n",
        "        num_heads (int): Number of attention heads\n",
        "        d_model (int): Input dimension\n",
        "        k (int): Latent projection size for E, F, G\n",
        "        len_seq (int): Sequence length\n",
        "\n",
        "        Note that the length of the sequence must be inputted correctly here.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_heads, d_model, k, len_seq):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        self.head_dim = d_model // num_heads\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_l = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.E = nn.Linear(len_seq, k)\n",
        "        self.F = nn.Linear(len_seq, k)\n",
        "        self.G = nn.Linear(len_seq, k)\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        B, L, D = x.size()\n",
        "        x = x.view(B, L, self.num_heads, self.head_dim)\n",
        "        return x.transpose(1, 2)  # [B, H, L, D]\n",
        "\n",
        "    def self_attention(self, q, k, l, v,mask=None):\n",
        "        # Compute trilinear attention scores\n",
        "        scores = torch.einsum('bhid,bhjd,bhkd->bhijk', q, k, l) / (self.head_dim ** 0.5)\n",
        "        if mask is not None:\n",
        "            #print(f'shape of mask given Linformer3D : {mask.shape}')\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "            #print(f'mask applied in linformer 3d')\n",
        "\n",
        "        attn_weights = softmax_5d(scores, axis=(-1, -2))\n",
        "\n",
        "        v_3d = torch.einsum('bhld,bhLd->bhlLd', v, v)  # Could be simplified\n",
        "        output = torch.einsum('bhijk,bhjkl->bhil', attn_weights, v_3d)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def forward(self, q,mask = None):\n",
        "        k = q.clone()\n",
        "        l = q.clone()\n",
        "        v = q.clone()\n",
        "        Q = self.split_heads(self.W_q(q))\n",
        "\n",
        "        def project(x, linear_proj, W):\n",
        "            x = W(x).permute(0, 2, 1)\n",
        "            x = linear_proj(x).permute(0, 2, 1)\n",
        "            return self.split_heads(x)\n",
        "\n",
        "        K = project(k, self.E, self.W_k)\n",
        "        L = project(l, self.F, self.W_l)\n",
        "        V = project(v, self.G, self.W_v)\n",
        "\n",
        "        attn_out = self.self_attention(Q, K, L, V,mask)\n",
        "\n",
        "        B, H, L, D = attn_out.shape\n",
        "        concat = attn_out.transpose(1, 2).contiguous().view(B, L, self.d_model)\n",
        "        return self.W_o(concat)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8wHzgGwh9tzT",
      "metadata": {
        "id": "8wHzgGwh9tzT"
      },
      "source": [
        "#### Multi-ped + Linformer 3D Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HLu_T9Ah9tzT",
      "metadata": {
        "id": "HLu_T9Ah9tzT"
      },
      "outputs": [],
      "source": [
        "class Attn3DLinformerMultiPed(nn.Module):\n",
        "    \"\"\"\n",
        "    Modified Multi-Head Attention with 3D product using E, F, G projections.\n",
        "\n",
        "    Args:\n",
        "        num_heads (int): Number of attention heads\n",
        "        d_model (int): Input dimension\n",
        "        k (int): Latent projection size for E, F, G\n",
        "        len_seq (int): Sequence length\n",
        "\n",
        "        Note that the length of the sequence must be inputted correctly here.\n",
        "        first multi-ped then linformer attention\n",
        "    \"\"\"\n",
        "    def __init__(self, num_heads, d_model, k, len_seq, block_size, stride):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        self.head_dim = d_model // num_heads\n",
        "        self.block_size = block_size\n",
        "        self.stride = stride\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_l = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.E = nn.Linear(block_size, k)\n",
        "        self.F = nn.Linear(block_size, k)\n",
        "        self.G = nn.Linear(block_size, k)\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        # Shape: (batch, seq_len, d_model) --> (batch, num_heads, seq_len, head_dim)\n",
        "        bsz, num_peds, len_peds, d_model = x.size()\n",
        "        return x.reshape(bsz, num_peds, len_peds, self.num_heads, self.head_dim).transpose(2,3)\n",
        "\n",
        "    def self_attention(self, q, k,l, v, mask = None):\n",
        "        # Save the length of the sequence\n",
        "        seq_len = k.shape[2]\n",
        "        # Calculate the dot produce of Q and K\n",
        "        mul_qkl = torch.einsum('abcid,abcjd,abckd -> abcijk', q, k, l) /(self.head_dim**0.5)# Shape: (#samples,num_heads,len_seq,k, k)\n",
        "        #print(f'shape of the attention scores: {mul_qkl.shape}')\n",
        "        if mask is not None:\n",
        "            mul_qkl = mul_qkl.masked_fill(mask == 0, -1e9)\n",
        "            #print(f'mask is implemented in MultiLin3D')\n",
        "            #print(f'shape of the mask: {mask.shape}')\n",
        "        # Apply the Softmax to the attention weights\n",
        "        attention_scores = softmax_5d(mul_qkl, axis = (-1,-2))\n",
        "        # Calculate V^2\n",
        "        vtilde = torch.einsum('abcid,abcjd->abcijd', v, v)\n",
        "        # Multiply to the value matrix\n",
        "        result = torch.einsum('abcijk,abcjkl->abcil', attention_scores, vtilde)\n",
        "        #print(f'shape of attention results: {result.shape}')\n",
        "        return(result)\n",
        "\n",
        "    def _sliding_blocks(self, x):\n",
        "        \"\"\"\n",
        "        Create sliding-window blocks from the sequence.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): (B, L, D).\n",
        "\n",
        "        Returns:\n",
        "            Tensor: (B, num_blocks, block_size, D).\n",
        "        \"\"\"\n",
        "        l = self.block_size\n",
        "        d = self.stride\n",
        "        B, L, D = x.shape\n",
        "\n",
        "        num_blocks = (L - l) // d + 1\n",
        "\n",
        "        # Compute new shape and strides\n",
        "        shape = (B, num_blocks, l, D)\n",
        "        strides = (\n",
        "            x.stride(0),        # batch stride\n",
        "            d * x.stride(1),    # jump d steps for each block\n",
        "            x.stride(1),        # step 1 inside a block\n",
        "            x.stride(2),        # feature stride\n",
        "        )\n",
        "\n",
        "        blocks = x.as_strided(shape, strides)\n",
        "        return blocks.contiguous()\n",
        "\n",
        "    def _reconstruct_from_blocks(self, blocks, seq_len):\n",
        "        \"\"\"\n",
        "        Reconstruct sequence from overlapping sliding blocks.\n",
        "\n",
        "        Args:\n",
        "            blocks (Tensor): (B, num_blocks, block_size, D).\n",
        "            seq_len (int): Original sequence length.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: (B, seq_len, D).\n",
        "        \"\"\"\n",
        "        l = self.block_size\n",
        "        d = self.stride\n",
        "        B, num_blocks, _, D = blocks.shape\n",
        "\n",
        "        device = blocks.device\n",
        "\n",
        "        # Build index map: (num_blocks, block_size) -> positions in seq_len\n",
        "        start_idx = torch.arange(num_blocks, device=device) * d\n",
        "        block_offsets = torch.arange(l, device=device)\n",
        "        positions = start_idx[:, None] + block_offsets[None, :]  # (num_blocks, block_size)\n",
        "\n",
        "        # Expand to batch and feature dims\n",
        "        positions = positions.unsqueeze(0).expand(B, -1, -1)  # (B, num_blocks, block_size)\n",
        "\n",
        "        # Flatten everything for scatter\n",
        "        flat_pos = positions.reshape(B, -1)  # (B, num_blocks * block_size)\n",
        "        flat_blocks = blocks.reshape(B, -1, D)  # (B, num_blocks * block_size, D)\n",
        "\n",
        "        # Allocate output\n",
        "        out = torch.zeros(B, seq_len, D, device=device)\n",
        "        counts = torch.zeros(B, seq_len, 1, device=device)\n",
        "\n",
        "        # Scatter add the block values into the right positions\n",
        "        out.scatter_add_(1, flat_pos.unsqueeze(-1).expand(-1, -1, D), flat_blocks)\n",
        "        counts.scatter_add_(1, flat_pos.unsqueeze(-1), torch.ones_like(flat_pos, device=device).unsqueeze(-1).float())\n",
        "\n",
        "        return out / counts\n",
        "\n",
        "    def forward(self, q,mask = None):\n",
        "        k = q.clone()\n",
        "        l = q.clone()\n",
        "        v = q.clone()\n",
        "        Q = self.split_heads(self._sliding_blocks(self.W_q(q)))\n",
        "        #print(f'shape of Q: {Q.shape}')\n",
        "\n",
        "        def project(x, linear_proj, W):\n",
        "            '''\n",
        "            Expected input shape: ([batch_size, len_seq, d_model])\n",
        "\n",
        "            '''\n",
        "            x = self._sliding_blocks(W(x)) #shape: ([batch_size, num_peds, len_ped, d_model])\n",
        "            x = x.permute(0, 1, 3, 2) #shape: ([batch_size, num_peds, d_model, len_ped])\n",
        "            x = linear_proj(x) # shape: torch.Size([batch_size, num_peds, d_model, k])\n",
        "            x = x.permute(0, 1, 3, 2) # shape: torch.Size([batch_size, num_peds, k, d_model])\n",
        "            return self.split_heads((x)) # shape: torch.Size([batch_size, num_peds, num_heads, k , head_dim])\n",
        "\n",
        "        K = project(k, self.E, self.W_k)\n",
        "        #print(f'shape of K: {K.shape}')\n",
        "        L = project(l, self.F, self.W_l)\n",
        "        V = project(v, self.G, self.W_v)\n",
        "\n",
        "        attn_output = self.self_attention(Q, K, L, V,mask)\n",
        "\n",
        "        batch_size, num_peds, num_heads, len_ped ,len_head = attn_output.shape\n",
        "        attn_output = attn_output.contiguous().view(batch_size, num_peds, len_ped, self.d_model)\n",
        "        #print(f'shape of attention output after combining heads {attn_output.shape}')\n",
        "\n",
        "        # Combine peds -> (batch_size, len_peds * num_peds, d_model)\n",
        "        attn_output = self._reconstruct_from_blocks(attn_output, q.shape[1])\n",
        "        #print(f'shape of attention output after combining the peds {attn_output.shape}')\n",
        "\n",
        "        # Apply through the last linear layer\n",
        "        result_final = self.W_o(attn_output) #shape = (#samples, len_seq, d_model)\n",
        "        #print(f'shape of final output: {result_final.shape}')\n",
        "\n",
        "        return result_final"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ms8JLk449tzU",
      "metadata": {
        "id": "Ms8JLk449tzU"
      },
      "source": [
        "#### Plain 3D Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LCUV7vy49tzU",
      "metadata": {
        "id": "LCUV7vy49tzU"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttnMod3D(nn.Module):\n",
        "    \"\"\"\n",
        "    Encoder Attention:\n",
        "    Input is mapped to Query, Key, and L-matrix, each matrix is (d_model x d_model).\n",
        "    Query, Key, and L-matrix are multiplied to get a len_seq x len_seq x len_seq matrix:\n",
        "        - shows correlation between 3 elements.\n",
        "    Softmax is taken across the last two dimensions (defined by 5D Softmax)\n",
        "    Value is calculated by multiplying input sequence by another matrix of d_model x d_model -> dim = len_seq x d_model\n",
        "    Attention scores after softmax are multiplied by Value\n",
        "            -> (#samples, num_heads, head_param, head_param, head_param) x (//, //, len_seq, head_param)\n",
        "            = (//,//, len_seq, head_param)\n",
        "    mask: padding mask\n",
        "\n",
        "\n",
        "    Decoder Attention\n",
        "    Decoder input mapped to Query\n",
        "    Encoder output mapped to Key, L-matrix and Value\n",
        "    mask: look-ahead mask and the padding mask\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,num_heads, d_model):\n",
        "        super(MultiHeadAttnMod3D,self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_params = d_model//num_heads\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, d_model) # mapping to the same number of features\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_l = nn.Linear(d_model,d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # Check if the number of len_emb (d_model) is divisable by num_heads\n",
        "        assert d_model % num_heads == 0, \"d_model is not divisable by the number of heads\"\n",
        "\n",
        "\n",
        "    def self_attention(self, q, k,l, v, mask):\n",
        "        # Save the length of the sequence\n",
        "        seq_len = k.shape[2]\n",
        "        # Calculate the dot produce of Q and K ## multiplication occurs across the last dim (embedding dim)\n",
        "        mul_qkl = torch.einsum('abcd,abed,abfd -> abcef', q, k, l) /(self.head_params**0.5) # Shape: (#samples,num_heads,len_seq,len_seq, len_seq)\n",
        "        #print(f'shape of mul_qkl: {mul_qkl.shape}')\n",
        "        if mask is not None:\n",
        "            #print(f'mask shape 3D attn : {mask.shape}')\n",
        "            mul_qkl = mul_qkl.masked_fill(mask == 0, -1e9)\n",
        "            print(f'mask applied in 3d plain')\n",
        "\n",
        "        # Apply the Softmax to the attention weights\n",
        "        attention_scores = softmax_5d(mul_qkl, axis = (-1,-2))\n",
        "        # Multiply v by v -> Dim = NxNxd\n",
        "        v_3d = torch.einsum('shld, shLd -> shlLd', v,v) # shape: torch.Size([#samples, #heads, len_seq, len_seq, d_model/#heads])\n",
        "        # Multiply to the value matrix\n",
        "        result = torch.einsum('shijk,shjkl->shil', attention_scores, v_3d)\n",
        "        #return(attention_scores,result)\n",
        "        return(result)\n",
        "\n",
        "    def split_heads(self, q):\n",
        "\n",
        "        \"\"\"\n",
        "        Reshaping\n",
        "        Input of shape : (#samples, seq_len, d_model)\n",
        "\n",
        "        to\n",
        "        output of shape: (#samples, num_heads, seq_len, head_parameters)\n",
        "        \"\"\"\n",
        "        samples, seq_len, _ = q.shape\n",
        "        return q.view(samples, seq_len, self.num_heads, self.head_params).transpose(1,2)\n",
        "\n",
        "    def forward(self, q, mask = None):\n",
        "        k = q.clone()\n",
        "        l = q.clone()\n",
        "        v = q.clone()\n",
        "\n",
        "        # Define Query, Key, Value\n",
        "        Query = self.split_heads(self.W_q(q))\n",
        "        Key  = self.split_heads(self.W_k(k))\n",
        "        L_matrix = self.split_heads(self.W_l(l))\n",
        "        Value = self.split_heads(self.W_v(v))\n",
        "\n",
        "        # Run the self-attention\n",
        "        attn_output = self.self_attention(Query, Key,L_matrix, Value, mask)\n",
        "\n",
        "        # Concatenate the heads\n",
        "        batch_size, _,seq_len, _= attn_output.shape\n",
        "        attn_output = attn_output.transpose(1,2).contiguous().view(batch_size, seq_len, self.d_model) #shape = (#samples, len_seq, d_model)\n",
        "\n",
        "        # Apply through the last linear layer\n",
        "        result_final = self.W_o(attn_output) #shape = (#samples, len_seq, d_model)\n",
        "\n",
        "        return result_final"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "i7h1ykMh9DEg",
      "metadata": {
        "id": "i7h1ykMh9DEg"
      },
      "source": [
        "#### Get Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zcFoE5rJ08sA",
      "metadata": {
        "id": "zcFoE5rJ08sA"
      },
      "outputs": [],
      "source": [
        "def get_attention(attn_type, **kwargs):\n",
        "  \"\"\"\n",
        "  Attention options include:\n",
        "  - Plain2D\n",
        "  - Linformer2D\n",
        "  - Multiped2D\n",
        "\n",
        "  - Plain3D\n",
        "  - Linformer3D\n",
        "  - Multiped3D\n",
        "  - Combined3D\n",
        "  - Attn3DLinformerMultiPed\n",
        "\n",
        "  Combined2D3D\n",
        "  Combined2D3DLinformer\n",
        "  Combined2D3DMultiPed\n",
        "  Combined2D3DMultiPedLinformer\n",
        "\n",
        "  \"\"\"\n",
        "  if attn_type == \"Plain3D\":\n",
        "      print(f'Plain3D is activated')\n",
        "      return MultiHeadAttnMod3D(d_model=kwargs[\"d_model\"], num_heads=kwargs[\"num_heads\"])\n",
        "  elif attn_type == \"Linformer3D\":\n",
        "      print(f'Linformer3D is activated')\n",
        "      return Attn3DLinformer(num_heads = kwargs['num_heads'], d_model=kwargs[\"d_model\"], k=kwargs[\"k\"], len_seq=kwargs[\"len_seq\"])\n",
        "  elif attn_type == \"Combined2D3D\":\n",
        "      print(f'Combined2D3D is activated')\n",
        "      return Attn_2D3DC(num_heads=kwargs[\"num_heads\"], d_model=kwargs[\"d_model\"])\n",
        "  elif attn_type == \"Multiped3D\":\n",
        "      print(f'Multiped3D is activated')\n",
        "      return Attn3D_MultiPed(num_heads=kwargs[\"num_heads\"], d_model=kwargs[\"d_model\"], block_size = kwargs[\"block_size\"], stride =  kwargs[\"stride\"])\n",
        "  elif attn_type == \"Attn3DLinformerMultiPed\":\n",
        "      print(f'Attn3DLinformerMultiPed is activated')\n",
        "      return Attn3DLinformerMultiPed(num_heads=kwargs[\"num_heads\"], d_model=kwargs[\"d_model\"], k=kwargs[\"k\"], len_seq=kwargs[\"len_seq\"],block_size = kwargs[\"block_size\"], stride =  kwargs[\"stride\"])\n",
        "  elif attn_type == \"Plain2D\":\n",
        "      print(f'Plain2D is activated')\n",
        "      return MultiHeadAttn2D(d_model=kwargs[\"d_model\"], num_heads=kwargs[\"num_heads\"])\n",
        "  elif attn_type == \"Linformer2D\":\n",
        "      print(f'Linformer2D is activated')\n",
        "      return Attn2DLinformer(num_heads = kwargs['num_heads'], d_model=kwargs[\"d_model\"], k=kwargs[\"k\"], len_seq=kwargs[\"len_seq\"])\n",
        "  elif attn_type == \"Multiped2D\":\n",
        "      print(f'Multiped2D is activated')\n",
        "      return Attn2D_MultiPed(num_heads=kwargs[\"num_heads\"], d_model=kwargs[\"d_model\"], block_size = kwargs[\"block_size\"], stride =  kwargs[\"stride\"])\n",
        "  elif attn_type == \"MultiLin2D\":\n",
        "      print(f'MultiLin2D is activated')\n",
        "      return Attn2DLinformerMultiPed(num_heads=kwargs[\"num_heads\"], d_model=kwargs[\"d_model\"], block_size = kwargs[\"block_size\"], stride =  kwargs[\"stride\"],k=kwargs[\"k\"], len_seq=kwargs[\"len_seq\"])\n",
        "  elif attn_type == \"Attn2D3DLin\":\n",
        "      print(f'Attn2D3DLin is activated')\n",
        "      return Attn2D3DLin(num_heads = kwargs['num_heads'], d_model= kwargs['d_model'], k = kwargs['k'], len_seq = kwargs['len_seq'])\n",
        "  elif attn_type == 'Attn2D3D_MultiPed':\n",
        "      print(f'Attn2D3D_MultiPed is activated')\n",
        "      return Attn2D3D_MultiPed(num_heads=kwargs[\"num_heads\"], d_model=kwargs[\"d_model\"], num_peds=kwargs[\"num_peds\"])\n",
        "  elif attn_type == \"Attn2D3D_MultiPedLin\":\n",
        "      print(f'Attn2D3DMultiPed_Linformer is activated')\n",
        "      return Attn2D3D_MultiPedLin(num_heads= kwargs[\"num_heads\"], d_model= kwargs[\"d_model\"], num_peds=kwargs[\"num_peds\"], len_seq=kwargs[\"len_seq\"], k=kwargs[\"k\"])\n",
        "  elif attn_type == \"Attn2D3DLinAlter\":\n",
        "      print(f'Attn2D3DLinAlter is activated')\n",
        "      return Attn2D3DLinAlter(num_heads= kwargs[\"num_heads\"], d_model= kwargs[\"d_model\"], k= kwargs[\"k\"], len_seq= kwargs[\"len_seq\"])\n",
        "  elif attn_type == \"Attn2D3D_MultiPedAlter\":\n",
        "      print(f'Attn2D3D_MultiPedAlter is activated')\n",
        "      return Attn2D3D_MultiPedAlter(num_heads=kwargs[\"num_heads\"], d_model=kwargs[\"d_model\"], num_peds=kwargs[\"num_peds\"])\n",
        "  elif attn_type == \"Attn2D3D_MultiPedLinAlter\":\n",
        "      print(f'Attn2D3D_MultiPedLinAlter is activated')\n",
        "      return Attn2D3D_MultiPedLinAlter(num_heads= kwargs[\"num_heads\"], d_model= kwargs[\"d_model\"], num_peds=kwargs[\"num_peds\"], len_seq=kwargs[\"len_seq\"], k=kwargs[\"k\"])\n",
        "  elif attn_type == \"Attn2DMultiPed3Dselected\":\n",
        "      print(f'Attn2DMultiPed3Dselected is activated')\n",
        "      return Attn2DMultiPed3Dselected(num_heads=kwargs[\"num_heads\"], d_model=kwargs[\"d_model\"],block_size = kwargs[\"block_size\"], stride =  kwargs[\"stride\"])\n",
        "  else:\n",
        "      raise ValueError(f\"Unknown attention type: {attn_type}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_attention(attn_type, **kwargs):\n",
        "  \"\"\"\n",
        "  Attention options include:\n",
        "  - Plain2D\n",
        "  - Linformer2D\n",
        "  - Multiped2D\n",
        "\n",
        "  - Plain3D\n",
        "  - Linformer3D\n",
        "  - Multiped3D\n",
        "  - Combined3D\n",
        "  - Attn3DLinformerMultiPed\n",
        "\n",
        "  Combined2D3D\n",
        "  Combined2D3DLinformer\n",
        "  Combined2D3DMultiPed\n",
        "  Combined2D3DMultiPedLinformer\n",
        "\n",
        "  \"\"\"\n",
        "  if attn_type == \"Plain3D\":\n",
        "      print(f'Plain3D is activated')\n",
        "      return MultiHeadAttnMod3D(d_model=kwargs[\"d_model\"], num_heads=kwargs[\"num_heads\"])\n",
        "  elif attn_type == \"Linformer3D\":\n",
        "      print(f'Linformer3D is activated')\n",
        "      return Attn3DLinformer(num_heads = kwargs['num_heads'], d_model=kwargs[\"d_model\"], k=kwargs[\"k\"], len_seq=kwargs[\"len_seq\"])\n",
        "  elif attn_type == \"Combined2D3D\":\n",
        "      print(f'Combined2D3D is activated')\n",
        "      return Attn_2D3DC(num_heads=kwargs[\"num_heads\"], d_model=kwargs[\"d_model\"])\n",
        "  elif attn_type == \"Multiped3D\":\n",
        "      print(f'Multiped3D is activated')\n",
        "      return Attn3D_MultiPed(num_heads=kwargs[\"num_heads\"], d_model=kwargs[\"d_model\"], block_size = kwargs[\"block_size\"], stride=kwargs[\"stride\"])\n",
        "  elif attn_type == \"Attn3DLinformerMultiPed\":\n",
        "      print(f'Attn3DLinformerMultiPed is activated')\n",
        "      return Attn3DLinformerMultiPed(num_heads=kwargs[\"num_heads\"], d_model=kwargs[\"d_model\"], k=kwargs[\"k\"], len_seq=kwargs[\"len_seq\"], block_size = kwargs[\"block_size\"], stride=kwargs[\"stride\"])\n",
        "  elif attn_type == \"Plain2D\":\n",
        "      print(f'Plain2D is activated')\n",
        "      return MultiHeadAttn2D(d_model=kwargs[\"d_model\"], num_heads=kwargs[\"num_heads\"])\n",
        "  elif attn_type == \"Linformer2D\":\n",
        "      print(f'Linformer2D is activated')\n",
        "      return Attn2DLinformer(num_heads = kwargs['num_heads'], d_model=kwargs[\"d_model\"], k=kwargs[\"k\"], len_seq=kwargs[\"len_seq\"])\n",
        "  elif attn_type == \"Multiped2D\":\n",
        "      print(f'Multiped2D is activated')\n",
        "      return Attn2D_MultiPed(num_heads=kwargs[\"num_heads\"], d_model=kwargs[\"d_model\"], block_size = kwargs[\"block_size\"], stride = kwargs[\"stride\"])\n",
        "  elif attn_type == \"MultiLin2D\":\n",
        "      print(f'MultiLin2D is activated')\n",
        "      return Attn2DLinformerMultiPed(num_heads=kwargs[\"num_heads\"], d_model=kwargs[\"d_model\"],k=kwargs[\"k\"], len_seq=kwargs[\"len_seq\"], stride = kwargs[\"stride\"], block_size = kwargs[\"block_size\"])\n",
        "  elif attn_type == \"Attn2D3DLin\":\n",
        "      print(f'Attn2D3DLin is activated')\n",
        "      return Attn2D3DLin(num_heads = kwargs['num_heads'], d_model= kwargs['d_model'], k = kwargs['k'], len_seq = kwargs['len_seq'])\n",
        "  elif attn_type == 'Attn2D3D_MultiPed':\n",
        "      print(f'Attn2D3D_MultiPed is activated')\n",
        "      return Attn2D3D_MultiPed(num_heads=kwargs[\"num_heads\"], d_model=kwargs[\"d_model\"], num_peds=kwargs[\"num_peds\"])\n",
        "  elif attn_type == \"Attn2D3D_MultiPedLin\":\n",
        "      print(f'Attn2D3DMultiPed_Linformer is activated')\n",
        "      return Attn2D3D_MultiPedLin(num_heads= kwargs[\"num_heads\"], d_model= kwargs[\"d_model\"], num_peds=kwargs[\"num_peds\"], len_seq=kwargs[\"len_seq\"], k=kwargs[\"k\"])\n",
        "  elif attn_type ==\"Attn2DMultiPed3Dselected\":\n",
        "    return Attn2DMultiPed3Dselected(num_heads = kwargs[\"num_heads\"], d_model = kwargs[\"d_model\"], stride = kwargs[\"stride\"], block_size=kwargs[\"block_size\"])\n",
        "  elif attn_type == \"Linformer3D_Dual\":\n",
        "      print(f'Linformer3D Dual is activated')\n",
        "      return Attn3DLinformer_Dual(num_heads = kwargs['num_heads'], d_model=kwargs[\"d_model\"], k=kwargs[\"k\"], len_seq=kwargs[\"len_seq\"])\n",
        "  elif attn_type == \"Multiped3D_Dual\":\n",
        "      print(f'Multiped3D Dual is activated')\n",
        "      return Attn3D_MultiPed_Dual(num_heads=kwargs[\"num_heads\"], d_model=kwargs[\"d_model\"], block_size = kwargs[\"block_size\"], stride=kwargs[\"stride\"])\n",
        "  elif attn_type == \"MultiLin3D_Dual\":\n",
        "      print(f'MultiLin3D Dual is activated')\n",
        "      return Attn3DLinformerMultiPed_Dual(num_heads=kwargs[\"num_heads\"], d_model=kwargs[\"d_model\"], k=kwargs[\"k\"], len_seq=kwargs[\"len_seq\"], block_size = kwargs[\"block_size\"], stride=kwargs[\"stride\"])\n",
        "  elif attn_type == 'Attn2DMultiPed3Dselected_Dual':\n",
        "    print(f'Attn2DMultiPed3Dselected_Dual is activated')\n",
        "    return Attn2DMultiPed3Dselected_Dual(num_heads = kwargs[\"num_heads\"], d_model = kwargs[\"d_model\"], stride = kwargs[\"stride\"], block_size=kwargs[\"block_size\"])\n",
        "  else:\n",
        "      raise ValueError(f\"Unknown attention type: {attn_type}\")\n",
        "\n",
        "def softmax_5d(X, axis):\n",
        "    \"\"\"\n",
        "    Compute softmax for a 5D tensor along the specified axis.\n",
        "\n",
        "    Parameters:\n",
        "        X (numpy.ndarray or torch.Tensor): Input tensor of shape (sample_size, heads, n, n, n)\n",
        "        axis (int or tuple): Axis along which to apply softmax\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray or torch.Tensor: Softmax-applied tensor of the same shape as X.\n",
        "    \"\"\"\n",
        "    X_exp = torch.exp(X - torch.amax(X, dim=axis, keepdim=True))  # Stability trick\n",
        "    return X_exp / torch.sum(X_exp, dim=axis, keepdim=True)\n"
      ],
      "metadata": {
        "id": "DhAuwvYYdSIy"
      },
      "id": "DhAuwvYYdSIy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "xiOxcC-I08sA",
      "metadata": {
        "id": "xiOxcC-I08sA"
      },
      "source": [
        "#### Training Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CWHSa-JM08sA",
      "metadata": {
        "id": "CWHSa-JM08sA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import contextlib\n",
        "import io\n",
        "import numpy as np\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "\n",
        "def compute_spearmanr(preds, targets):\n",
        "    \"\"\"\n",
        "    Compute Spearman correlation between predicted and true values.\n",
        "\n",
        "    Args:\n",
        "        preds: torch.Tensor of shape [N, 1] or [N]\n",
        "        targets: torch.Tensor of shape [N, 1] or [N]\n",
        "    Returns:\n",
        "        Spearman correlation coefficient (float)\n",
        "    \"\"\"\n",
        "    preds = preds.detach().cpu().numpy().flatten()\n",
        "    targets = targets.detach().cpu().numpy().flatten()\n",
        "    return spearmanr(preds, targets).correlation\n",
        "\n",
        "\n",
        "\n",
        "def evaluate_model(model, eval_loader):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "    total_loss = 0.0\n",
        "    criterion = torch.nn.MSELoss()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in eval_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            #print(f'shape of input_ids: {input_ids.shape}')\n",
        "            targets = batch['targets'].to(device)\n",
        "            #print(f'shape of targets: {targets.shape}')\n",
        "\n",
        "            outputs = model(input_ids) #mask is defined internally\n",
        "            loss = criterion(outputs, targets)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            all_preds.append(outputs)\n",
        "            all_targets.append(targets)\n",
        "\n",
        "    # Concatenate all batches\n",
        "    all_preds = torch.cat(all_preds)\n",
        "    all_targets = torch.cat(all_targets)\n",
        "\n",
        "    # Compute the Spearman_score\n",
        "    spearman_score = compute_spearmanr(all_preds, all_targets)\n",
        "    print(f\"Test Spearman's : {spearman_score:.4f}\")\n",
        "\n",
        "    avg_loss = total_loss / len(eval_loader)\n",
        "\n",
        "    return avg_loss, spearman_score\n",
        "\n",
        "def train_model(model_class, model_kwargs, train_loader, val_loader,\n",
        "                epochs=10, lr=1e-4, continue_train=False, trained_model=None, trained_optimizer=None):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # === Model initialization or continuation ===\n",
        "    if continue_train and trained_model is not None:\n",
        "        print(\"\\nContinuing training from the provided model...\\n\")\n",
        "        model = trained_model.to(device)\n",
        "    else:\n",
        "        print(\"\\nStarting training from scratch...\\n\")\n",
        "        model = model_class(**model_kwargs).to(device)\n",
        "\n",
        "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"Total trainable parameters: {total_params:,}\\n\")\n",
        "\n",
        "    # === Optimizer initialization or continuation ===\n",
        "    if continue_train and trained_optimizer is not None:\n",
        "        optimizer = trained_optimizer\n",
        "    else:\n",
        "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    criterion = torch.nn.MSELoss()\n",
        "    epoch_losses, epoch_times = [], []\n",
        "    latest_state_dict = None\n",
        "\n",
        "    # === Training loop ===\n",
        "    for epoch in range(epochs):\n",
        "        all_preds, all_targets = [], []\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        epoch_start_time = time.time()\n",
        "\n",
        "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
        "        for batch in pbar:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            targets = batch['targets'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(input_ids)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            pbar.set_postfix(loss=loss.item())\n",
        "            all_preds.append(outputs)\n",
        "            all_targets.append(targets)\n",
        "\n",
        "        # Spearman on training\n",
        "        all_preds = torch.cat(all_preds)\n",
        "        all_targets = torch.cat(all_targets)\n",
        "        train_spearman_score = compute_spearmanr(all_preds, all_targets)\n",
        "\n",
        "        # Validation\n",
        "        with contextlib.redirect_stdout(io.StringIO()):\n",
        "            avg_val_loss, val_spearman_score = evaluate_model(model, val_loader)\n",
        "\n",
        "        avg_epoch_loss = total_loss / len(train_loader)\n",
        "        epoch_losses.append(avg_val_loss)\n",
        "        epoch_duration = time.time() - epoch_start_time\n",
        "        epoch_times.append(epoch_duration)\n",
        "\n",
        "        print(f\" Epoch {epoch+1} | Train Loss: {avg_epoch_loss:.4f} | Train_ss: {train_spearman_score:.4f} \"\n",
        "              f\"| Val Loss: {avg_val_loss:.4f} | Val_ss: {val_spearman_score:.4f}\")\n",
        "\n",
        "        latest_state_dict = model.state_dict()\n",
        "\n",
        "    print(f\"Average training time per epoch: {np.mean(epoch_times):.2f}s\")\n",
        "\n",
        "    if latest_state_dict is not None:\n",
        "        model.load_state_dict(latest_state_dict)\n",
        "\n",
        "    return model, optimizer, epoch_losses\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3u1Bg031oPbg",
      "metadata": {
        "id": "3u1Bg031oPbg"
      },
      "source": [
        "## Training Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VR1bNN76v0jJ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VR1bNN76v0jJ",
        "outputId": "92d86f14-ee36-4689-ac07-9ae12e28548e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Starting training from scratch...\n",
            "\n",
            "Plain2D is activated\n",
            "Plain2D is activated\n",
            "Plain2D is activated\n",
            "Plain2D is activated\n",
            "Plain2D is activated\n",
            "Plain2D is activated\n",
            "Plain2D is activated\n",
            "Plain2D is activated\n",
            "Plain2D is activated\n",
            "Plain2D is activated\n",
            "Plain2D is activated\n",
            "Plain2D is activated\n",
            "Total trainable parameters: 20,877,569\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|| 1341/1341 [02:08<00:00, 10.45it/s, loss=1.71]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 1 | Train Loss: 0.8447 | Train_ss: 0.0265 | Val Loss: 6.0868 | Val_ss: 0.5885\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|| 1341/1341 [02:08<00:00, 10.44it/s, loss=1.16]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 2 | Train Loss: 0.8110 | Train_ss: 0.0558 | Val Loss: 4.9881 | Val_ss: 0.6127\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|| 1341/1341 [02:08<00:00, 10.43it/s, loss=0.318]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 3 | Train Loss: 0.8039 | Train_ss: 0.0558 | Val Loss: 9.4796 | Val_ss: 0.6282\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|| 1341/1341 [02:08<00:00, 10.43it/s, loss=0.915]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 4 | Train Loss: 0.7943 | Train_ss: 0.0663 | Val Loss: 12.3462 | Val_ss: 0.6229\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5: 100%|| 1341/1341 [02:08<00:00, 10.44it/s, loss=0.485]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 5 | Train Loss: 0.7832 | Train_ss: 0.0796 | Val Loss: 9.6797 | Val_ss: 0.6237\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6: 100%|| 1341/1341 [02:08<00:00, 10.44it/s, loss=0.112]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 6 | Train Loss: 0.7804 | Train_ss: 0.0926 | Val Loss: 7.8673 | Val_ss: 0.6185\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7: 100%|| 1341/1341 [02:08<00:00, 10.44it/s, loss=0.948]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 7 | Train Loss: 0.7616 | Train_ss: 0.1169 | Val Loss: 8.4649 | Val_ss: 0.6131\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8: 100%|| 1341/1341 [02:08<00:00, 10.43it/s, loss=1.56]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 8 | Train Loss: 0.7528 | Train_ss: 0.1334 | Val Loss: 5.6565 | Val_ss: 0.6047\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9: 100%|| 1341/1341 [02:08<00:00, 10.43it/s, loss=0.103]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 9 | Train Loss: 0.7353 | Train_ss: 0.1631 | Val Loss: 6.3208 | Val_ss: 0.5702\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10: 100%|| 1341/1341 [02:08<00:00, 10.44it/s, loss=0.077]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 10 | Train Loss: 0.7220 | Train_ss: 0.1890 | Val Loss: 3.7611 | Val_ss: 0.5635\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11: 100%|| 1341/1341 [02:08<00:00, 10.44it/s, loss=0.636]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 11 | Train Loss: 0.6957 | Train_ss: 0.2253 | Val Loss: 3.6771 | Val_ss: 0.5283\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12: 100%|| 1341/1341 [02:08<00:00, 10.43it/s, loss=0.246]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 12 | Train Loss: 0.6778 | Train_ss: 0.2548 | Val Loss: 4.0455 | Val_ss: 0.5602\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 13: 100%|| 1341/1341 [02:08<00:00, 10.43it/s, loss=1.27]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 13 | Train Loss: 0.6601 | Train_ss: 0.2742 | Val Loss: 3.2614 | Val_ss: 0.5579\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 14: 100%|| 1341/1341 [02:08<00:00, 10.43it/s, loss=1.04]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 14 | Train Loss: 0.6253 | Train_ss: 0.3105 | Val Loss: 2.6636 | Val_ss: 0.5572\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 15: 100%|| 1341/1341 [02:08<00:00, 10.43it/s, loss=0.159]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 15 | Train Loss: 0.6113 | Train_ss: 0.3201 | Val Loss: 2.9124 | Val_ss: 0.6037\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 16: 100%|| 1341/1341 [02:08<00:00, 10.43it/s, loss=0.149]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 16 | Train Loss: 0.5909 | Train_ss: 0.3381 | Val Loss: 1.7422 | Val_ss: 0.6000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 17: 100%|| 1341/1341 [02:08<00:00, 10.43it/s, loss=1.68]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 17 | Train Loss: 0.5688 | Train_ss: 0.3649 | Val Loss: 0.9099 | Val_ss: 0.5792\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 18: 100%|| 1341/1341 [02:08<00:00, 10.43it/s, loss=0.745]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 18 | Train Loss: 0.5487 | Train_ss: 0.3869 | Val Loss: 0.5443 | Val_ss: 0.6028\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 19: 100%|| 1341/1341 [02:08<00:00, 10.43it/s, loss=0.38]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 19 | Train Loss: 0.5346 | Train_ss: 0.3949 | Val Loss: 0.4106 | Val_ss: 0.6171\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 20: 100%|| 1341/1341 [02:08<00:00, 10.43it/s, loss=0.182]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 20 | Train Loss: 0.5138 | Train_ss: 0.4159 | Val Loss: 0.6115 | Val_ss: 0.6223\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 21: 100%|| 1341/1341 [02:08<00:00, 10.43it/s, loss=0.659]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 21 | Train Loss: 0.4998 | Train_ss: 0.4327 | Val Loss: 0.5907 | Val_ss: 0.6262\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 22: 100%|| 1341/1341 [02:08<00:00, 10.44it/s, loss=0.295]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 22 | Train Loss: 0.4820 | Train_ss: 0.4472 | Val Loss: 0.3275 | Val_ss: 0.6170\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 23: 100%|| 1341/1341 [02:08<00:00, 10.44it/s, loss=0.273]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 23 | Train Loss: 0.4676 | Train_ss: 0.4541 | Val Loss: 0.3074 | Val_ss: 0.6257\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 24: 100%|| 1341/1341 [02:08<00:00, 10.44it/s, loss=0.498]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 24 | Train Loss: 0.4588 | Train_ss: 0.4743 | Val Loss: 0.3200 | Val_ss: 0.6258\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 25: 100%|| 1341/1341 [02:08<00:00, 10.43it/s, loss=0.581]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 25 | Train Loss: 0.4536 | Train_ss: 0.4692 | Val Loss: 0.3385 | Val_ss: 0.6411\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 26: 100%|| 1341/1341 [02:08<00:00, 10.43it/s, loss=0.651]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 26 | Train Loss: 0.4374 | Train_ss: 0.4872 | Val Loss: 0.2922 | Val_ss: 0.6239\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 27: 100%|| 1341/1341 [02:08<00:00, 10.43it/s, loss=0.339]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 27 | Train Loss: 0.4270 | Train_ss: 0.4938 | Val Loss: 0.3158 | Val_ss: 0.6438\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 28: 100%|| 1341/1341 [02:08<00:00, 10.44it/s, loss=1.24]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 28 | Train Loss: 0.4266 | Train_ss: 0.4981 | Val Loss: 0.2547 | Val_ss: 0.6446\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 29: 100%|| 1341/1341 [02:08<00:00, 10.44it/s, loss=0.218]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 29 | Train Loss: 0.4211 | Train_ss: 0.5019 | Val Loss: 0.2521 | Val_ss: 0.6499\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 30: 100%|| 1341/1341 [02:08<00:00, 10.42it/s, loss=0.574]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 30 | Train Loss: 0.4028 | Train_ss: 0.5080 | Val Loss: 0.2417 | Val_ss: 0.6477\n",
            "Average training time per epoch: 138.26s\n",
            "Test Spearman's : 0.6151\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.9867896825298405, np.float64(0.6150809097369544))"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Start Training\n",
        "model, model_kwargs = FluorescencePredict, {\n",
        "    'vocab_size': tokenizer.vocab_size,\n",
        "    'd_model': 256,\n",
        "    'num_layers': 12,\n",
        "    'dim_feedforward': 128,\n",
        "    'p': 0.4,\n",
        "    'len_seq': 512,\n",
        "    'num_heads': 8,\n",
        "    'attn_mechanism': 'Plain2D',\n",
        "    'num_peds': 30\n",
        "}\n",
        "trained_model, optimizer_Plain2D, avg_val_loss_Plain2D = train_model(model, model_kwargs, train_loader, valid_loader, epochs=30, lr=1e-5)\n",
        "evaluate_model(trained_model, test_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ptK3KxO5v0mA",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptK3KxO5v0mA",
        "outputId": "c032f69c-85bc-4ef9-832f-d806dc60b94a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Starting training from scratch...\n",
            "\n",
            "Multiped2D is activated\n",
            "Multiped2D is activated\n",
            "Multiped2D is activated\n",
            "Multiped2D is activated\n",
            "Multiped2D is activated\n",
            "Multiped2D is activated\n",
            "Multiped2D is activated\n",
            "Multiped2D is activated\n",
            "Multiped2D is activated\n",
            "Multiped2D is activated\n",
            "Multiped2D is activated\n",
            "Multiped2D is activated\n",
            "Total trainable parameters: 21,306,881\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|| 1341/1341 [01:34<00:00, 14.17it/s, loss=0.439]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 1 | Train Loss: 0.8346 | Train_ss: 0.0449 | Val Loss: 5.8491 | Val_ss: 0.5885\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|| 1341/1341 [01:34<00:00, 14.21it/s, loss=1.27]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 2 | Train Loss: 0.8165 | Train_ss: 0.0376 | Val Loss: 3.6778 | Val_ss: 0.6238\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|| 1341/1341 [01:34<00:00, 14.18it/s, loss=0.445]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 3 | Train Loss: 0.8130 | Train_ss: 0.0391 | Val Loss: 7.7253 | Val_ss: 0.6219\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|| 1341/1341 [01:34<00:00, 14.14it/s, loss=0.867]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 4 | Train Loss: 0.7995 | Train_ss: 0.0701 | Val Loss: 8.0696 | Val_ss: 0.6221\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5: 100%|| 1341/1341 [01:34<00:00, 14.22it/s, loss=0.407]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 5 | Train Loss: 0.7890 | Train_ss: 0.0746 | Val Loss: 7.1743 | Val_ss: 0.6193\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6: 100%|| 1341/1341 [01:34<00:00, 14.20it/s, loss=0.0871]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 6 | Train Loss: 0.7842 | Train_ss: 0.0966 | Val Loss: 6.5927 | Val_ss: 0.6173\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7: 100%|| 1341/1341 [01:34<00:00, 14.21it/s, loss=0.814]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 7 | Train Loss: 0.7701 | Train_ss: 0.1171 | Val Loss: 6.2173 | Val_ss: 0.6162\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8: 100%|| 1341/1341 [01:34<00:00, 14.23it/s, loss=0.992]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 8 | Train Loss: 0.7653 | Train_ss: 0.1199 | Val Loss: 4.9493 | Val_ss: 0.6142\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9: 100%|| 1341/1341 [01:34<00:00, 14.21it/s, loss=1.28]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 9 | Train Loss: 0.7545 | Train_ss: 0.1434 | Val Loss: 5.8439 | Val_ss: 0.6136\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10: 100%|| 1341/1341 [01:34<00:00, 14.22it/s, loss=0.657]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 10 | Train Loss: 0.7441 | Train_ss: 0.1596 | Val Loss: 4.6471 | Val_ss: 0.6142\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11: 100%|| 1341/1341 [01:34<00:00, 14.21it/s, loss=0.291]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 11 | Train Loss: 0.7333 | Train_ss: 0.1713 | Val Loss: 4.1490 | Val_ss: 0.6134\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12: 100%|| 1341/1341 [01:34<00:00, 14.20it/s, loss=0.979]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 12 | Train Loss: 0.7128 | Train_ss: 0.2059 | Val Loss: 4.7717 | Val_ss: 0.6138\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 13: 100%|| 1341/1341 [01:34<00:00, 14.24it/s, loss=0.796]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 13 | Train Loss: 0.6963 | Train_ss: 0.2189 | Val Loss: 2.1994 | Val_ss: 0.6142\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 14: 100%|| 1341/1341 [01:34<00:00, 14.23it/s, loss=0.0981]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 14 | Train Loss: 0.6807 | Train_ss: 0.2407 | Val Loss: 3.6983 | Val_ss: 0.6143\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 15: 100%|| 1341/1341 [01:34<00:00, 14.24it/s, loss=0.808]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 15 | Train Loss: 0.6701 | Train_ss: 0.2667 | Val Loss: 2.3074 | Val_ss: 0.6125\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 16: 100%|| 1341/1341 [01:34<00:00, 14.23it/s, loss=0.72]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 16 | Train Loss: 0.6458 | Train_ss: 0.2994 | Val Loss: 2.5394 | Val_ss: 0.6141\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 17: 100%|| 1341/1341 [01:34<00:00, 14.21it/s, loss=0.457]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 17 | Train Loss: 0.6373 | Train_ss: 0.3086 | Val Loss: 2.1339 | Val_ss: 0.6075\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 18: 100%|| 1341/1341 [01:34<00:00, 14.22it/s, loss=1.42]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 18 | Train Loss: 0.6120 | Train_ss: 0.3359 | Val Loss: 1.4155 | Val_ss: 0.6128\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 19: 100%|| 1341/1341 [01:34<00:00, 14.24it/s, loss=0.53]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 19 | Train Loss: 0.5822 | Train_ss: 0.3653 | Val Loss: 1.2916 | Val_ss: 0.6100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 20: 100%|| 1341/1341 [01:34<00:00, 14.20it/s, loss=0.541]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 20 | Train Loss: 0.5642 | Train_ss: 0.3835 | Val Loss: 0.6151 | Val_ss: 0.6203\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 21: 100%|| 1341/1341 [01:34<00:00, 14.21it/s, loss=0.35]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 21 | Train Loss: 0.5462 | Train_ss: 0.3943 | Val Loss: 1.0781 | Val_ss: 0.6311\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 22: 100%|| 1341/1341 [01:34<00:00, 14.16it/s, loss=0.218]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 22 | Train Loss: 0.5392 | Train_ss: 0.4033 | Val Loss: 0.8506 | Val_ss: 0.6349\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 23: 100%|| 1341/1341 [01:35<00:00, 14.11it/s, loss=0.402]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 23 | Train Loss: 0.5189 | Train_ss: 0.4193 | Val Loss: 0.3447 | Val_ss: 0.6388\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 24: 100%|| 1341/1341 [01:34<00:00, 14.13it/s, loss=0.466]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 24 | Train Loss: 0.5038 | Train_ss: 0.4317 | Val Loss: 0.4919 | Val_ss: 0.6469\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 25: 100%|| 1341/1341 [01:34<00:00, 14.21it/s, loss=0.188]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 25 | Train Loss: 0.4901 | Train_ss: 0.4502 | Val Loss: 0.3206 | Val_ss: 0.6523\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 26: 100%|| 1341/1341 [01:34<00:00, 14.21it/s, loss=0.0979]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 26 | Train Loss: 0.4688 | Train_ss: 0.4557 | Val Loss: 0.2995 | Val_ss: 0.6559\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 27: 100%|| 1341/1341 [01:34<00:00, 14.18it/s, loss=0.443]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 27 | Train Loss: 0.4667 | Train_ss: 0.4686 | Val Loss: 0.4040 | Val_ss: 0.6545\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 28: 100%|| 1341/1341 [01:34<00:00, 14.12it/s, loss=0.258]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 28 | Train Loss: 0.4552 | Train_ss: 0.4723 | Val Loss: 0.2685 | Val_ss: 0.6496\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 29: 100%|| 1341/1341 [01:34<00:00, 14.12it/s, loss=0.189]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 29 | Train Loss: 0.4482 | Train_ss: 0.4823 | Val Loss: 0.2941 | Val_ss: 0.6485\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 30: 100%|| 1341/1341 [01:34<00:00, 14.14it/s, loss=0.126]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 30 | Train Loss: 0.4344 | Train_ss: 0.4831 | Val Loss: 0.2466 | Val_ss: 0.6645\n",
            "Average training time per epoch: 100.24s\n",
            "Test Spearman's : 0.6215\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.9164200283271796, np.float64(0.6214866588478586))"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Start Training\n",
        "model, model_kwargs = FluorescencePredict, {\n",
        "    'vocab_size': tokenizer.vocab_size,\n",
        "    'd_model': 256,\n",
        "    'num_layers': 12,\n",
        "    'dim_feedforward': 128,\n",
        "    'p': 0.4,\n",
        "    'len_seq': 512,\n",
        "    'num_heads': 8,\n",
        "    'attn_mechanism': 'Multiped2D',\n",
        "    \"block_size\": 30,\n",
        "    \"stride\": 15\n",
        "}\n",
        "trained_model, optimizer_Multi2D , loss_Multi2D = train_model(model, model_kwargs, train_loader, valid_loader, epochs=30, lr=1e-5)\n",
        "evaluate_model(trained_model, test_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WnZ-4WGQv0ox",
      "metadata": {
        "id": "WnZ-4WGQv0ox",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5032f7a-4faa-4ba1-b795-4b43c10fb4db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting training from scratch...\n",
            "\n",
            "Linformer2D is activated\n",
            "Linformer2D is activated\n",
            "Linformer2D is activated\n",
            "Linformer2D is activated\n",
            "Linformer2D is activated\n",
            "Linformer2D is activated\n",
            "Linformer2D is activated\n",
            "Linformer2D is activated\n",
            "Linformer2D is activated\n",
            "Linformer2D is activated\n",
            "Linformer2D is activated\n",
            "Linformer2D is activated\n",
            "Total trainable parameters: 21,493,169\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|| 1341/1341 [01:22<00:00, 16.29it/s, loss=0.544]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 1 | Train Loss: 0.8552 | Train_ss: 0.0338 | Val Loss: 3.4969 | Val_ss: 0.6157\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|| 1341/1341 [01:20<00:00, 16.60it/s, loss=0.676]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 2 | Train Loss: 0.8171 | Train_ss: 0.0547 | Val Loss: 3.7255 | Val_ss: 0.6285\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|| 1341/1341 [01:20<00:00, 16.58it/s, loss=1.7]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 3 | Train Loss: 0.8061 | Train_ss: 0.0654 | Val Loss: 6.8127 | Val_ss: 0.6286\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|| 1341/1341 [01:20<00:00, 16.62it/s, loss=0.902]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 4 | Train Loss: 0.7948 | Train_ss: 0.0752 | Val Loss: 6.6080 | Val_ss: 0.6272\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|| 1341/1341 [01:20<00:00, 16.60it/s, loss=0.683]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 5 | Train Loss: 0.7791 | Train_ss: 0.1063 | Val Loss: 4.0970 | Val_ss: 0.6239\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6: 100%|| 1341/1341 [01:20<00:00, 16.57it/s, loss=0.631]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 6 | Train Loss: 0.7644 | Train_ss: 0.1097 | Val Loss: 4.5936 | Val_ss: 0.6239\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7: 100%|| 1341/1341 [01:20<00:00, 16.57it/s, loss=0.337]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 7 | Train Loss: 0.7580 | Train_ss: 0.1370 | Val Loss: 4.4508 | Val_ss: 0.6217\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8: 100%|| 1341/1341 [01:21<00:00, 16.56it/s, loss=0.692]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 8 | Train Loss: 0.7510 | Train_ss: 0.1422 | Val Loss: 6.3266 | Val_ss: 0.6197\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9: 100%|| 1341/1341 [01:20<00:00, 16.61it/s, loss=0.847]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 9 | Train Loss: 0.7330 | Train_ss: 0.1848 | Val Loss: 4.6138 | Val_ss: 0.6163\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10: 100%|| 1341/1341 [01:20<00:00, 16.61it/s, loss=0.232]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 10 | Train Loss: 0.7111 | Train_ss: 0.2130 | Val Loss: 2.8411 | Val_ss: 0.6063\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11: 100%|| 1341/1341 [01:20<00:00, 16.57it/s, loss=0.945]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 11 | Train Loss: 0.6632 | Train_ss: 0.2719 | Val Loss: 2.8692 | Val_ss: 0.5875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12: 100%|| 1341/1341 [01:20<00:00, 16.59it/s, loss=0.405]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 12 | Train Loss: 0.6195 | Train_ss: 0.3362 | Val Loss: 2.0955 | Val_ss: 0.5746\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13: 100%|| 1341/1341 [01:20<00:00, 16.58it/s, loss=0.26]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 13 | Train Loss: 0.5853 | Train_ss: 0.3780 | Val Loss: 3.1381 | Val_ss: 0.5800\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14: 100%|| 1341/1341 [01:20<00:00, 16.61it/s, loss=0.313]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 14 | Train Loss: 0.5531 | Train_ss: 0.4048 | Val Loss: 2.1506 | Val_ss: 0.5840\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15: 100%|| 1341/1341 [01:20<00:00, 16.61it/s, loss=0.295]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 15 | Train Loss: 0.5447 | Train_ss: 0.4209 | Val Loss: 3.0023 | Val_ss: 0.5933\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16: 100%|| 1341/1341 [01:20<00:00, 16.59it/s, loss=0.767]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 16 | Train Loss: 0.5212 | Train_ss: 0.4362 | Val Loss: 2.1965 | Val_ss: 0.6070\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17: 100%|| 1341/1341 [01:20<00:00, 16.61it/s, loss=0.414]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 17 | Train Loss: 0.5077 | Train_ss: 0.4430 | Val Loss: 2.0416 | Val_ss: 0.6115\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18: 100%|| 1341/1341 [01:21<00:00, 16.52it/s, loss=0.227]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 18 | Train Loss: 0.4923 | Train_ss: 0.4522 | Val Loss: 1.4852 | Val_ss: 0.6205\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19: 100%|| 1341/1341 [01:21<00:00, 16.52it/s, loss=0.834]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 19 | Train Loss: 0.4910 | Train_ss: 0.4661 | Val Loss: 2.1196 | Val_ss: 0.6321\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20: 100%|| 1341/1341 [01:20<00:00, 16.57it/s, loss=0.771]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 20 | Train Loss: 0.4716 | Train_ss: 0.4751 | Val Loss: 2.3514 | Val_ss: 0.6310\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 21: 100%|| 1341/1341 [01:20<00:00, 16.57it/s, loss=0.237]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 21 | Train Loss: 0.4584 | Train_ss: 0.4778 | Val Loss: 1.1158 | Val_ss: 0.6250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 22: 100%|| 1341/1341 [01:20<00:00, 16.59it/s, loss=0.223]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 22 | Train Loss: 0.4334 | Train_ss: 0.4782 | Val Loss: 1.9562 | Val_ss: 0.6620\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 23: 100%|| 1341/1341 [01:20<00:00, 16.61it/s, loss=0.499]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 23 | Train Loss: 0.4141 | Train_ss: 0.4818 | Val Loss: 1.9267 | Val_ss: 0.6427\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 24: 100%|| 1341/1341 [01:20<00:00, 16.61it/s, loss=0.326]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 24 | Train Loss: 0.4024 | Train_ss: 0.4858 | Val Loss: 1.8351 | Val_ss: 0.6446\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 25: 100%|| 1341/1341 [01:20<00:00, 16.59it/s, loss=0.174]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 25 | Train Loss: 0.3908 | Train_ss: 0.4960 | Val Loss: 3.1176 | Val_ss: 0.6728\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 26: 100%|| 1341/1341 [01:20<00:00, 16.58it/s, loss=0.528]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 26 | Train Loss: 0.3967 | Train_ss: 0.4873 | Val Loss: 1.1533 | Val_ss: 0.6761\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 27: 100%|| 1341/1341 [01:20<00:00, 16.59it/s, loss=0.132]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 27 | Train Loss: 0.3738 | Train_ss: 0.5026 | Val Loss: 0.9472 | Val_ss: 0.6985\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 28: 100%|| 1341/1341 [01:21<00:00, 16.54it/s, loss=0.325]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 28 | Train Loss: 0.3741 | Train_ss: 0.5047 | Val Loss: 1.2232 | Val_ss: 0.6933\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 29: 100%|| 1341/1341 [01:21<00:00, 16.53it/s, loss=0.0328]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 29 | Train Loss: 0.3608 | Train_ss: 0.5079 | Val Loss: 1.5898 | Val_ss: 0.7060\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 30: 100%|| 1341/1341 [01:21<00:00, 16.53it/s, loss=0.0354]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 30 | Train Loss: 0.3572 | Train_ss: 0.5000 | Val Loss: 1.5619 | Val_ss: 0.7002\n",
            "Average training time per epoch: 85.91s\n",
            "Test Spearman's : 0.6351\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3.1592870209142827, np.float64(0.6350526956012176))"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "# Start Training\n",
        "model, model_kwargs = FluorescencePredict, {\n",
        "    'vocab_size': tokenizer.vocab_size,\n",
        "    'd_model': 256,\n",
        "    'num_layers': 12,\n",
        "    'dim_feedforward': 128,\n",
        "    'p': 0.4,\n",
        "    'len_seq': 512,\n",
        "    'num_heads': 8,\n",
        "    'attn_mechanism': 'Linformer2D',\n",
        "    'num_peds': 20,\n",
        "    'k': 50\n",
        "}\n",
        "trained_model, optimizer_Lin2D , loss_Lin2D = train_model(model, model_kwargs, train_loader, valid_loader, epochs=30, lr=1e-5)\n",
        "evaluate_model(trained_model, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "n0lv0yiEv0rJ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0lv0yiEv0rJ",
        "outputId": "64d05686-8e0c-4b09-c214-5bc8a3eac66a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting training from scratch...\n",
            "\n",
            "MultiLin2D is activated\n",
            "MultiLin2D is activated\n",
            "MultiLin2D is activated\n",
            "MultiLin2D is activated\n",
            "MultiLin2D is activated\n",
            "MultiLin2D is activated\n",
            "MultiLin2D is activated\n",
            "MultiLin2D is activated\n",
            "MultiLin2D is activated\n",
            "MultiLin2D is activated\n",
            "MultiLin2D is activated\n",
            "MultiLin2D is activated\n",
            "Total trainable parameters: 21,314,321\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|| 1341/1341 [01:34<00:00, 14.14it/s, loss=1.1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 1 | Train Loss: 0.8640 | Train_ss: 0.0267 | Val Loss: 4.3531 | Val_ss: 0.5815\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|| 1341/1341 [01:31<00:00, 14.60it/s, loss=2.04]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 2 | Train Loss: 0.8566 | Train_ss: 0.0303 | Val Loss: 4.5528 | Val_ss: 0.6134\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|| 1341/1341 [01:32<00:00, 14.58it/s, loss=1.13]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 3 | Train Loss: 0.8372 | Train_ss: 0.0544 | Val Loss: 8.1782 | Val_ss: 0.6241\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|| 1341/1341 [01:32<00:00, 14.56it/s, loss=0.64]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 4 | Train Loss: 0.8148 | Train_ss: 0.0672 | Val Loss: 5.0027 | Val_ss: 0.6223\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|| 1341/1341 [01:32<00:00, 14.57it/s, loss=0.871]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 5 | Train Loss: 0.8095 | Train_ss: 0.0888 | Val Loss: 5.9699 | Val_ss: 0.6228\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6: 100%|| 1341/1341 [01:31<00:00, 14.58it/s, loss=0.522]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 6 | Train Loss: 0.7947 | Train_ss: 0.1052 | Val Loss: 4.3815 | Val_ss: 0.6206\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7: 100%|| 1341/1341 [01:31<00:00, 14.59it/s, loss=0.679]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 7 | Train Loss: 0.7801 | Train_ss: 0.1244 | Val Loss: 6.0102 | Val_ss: 0.6224\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8: 100%|| 1341/1341 [01:31<00:00, 14.58it/s, loss=0.99]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 8 | Train Loss: 0.7639 | Train_ss: 0.1490 | Val Loss: 7.1515 | Val_ss: 0.6233\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9: 100%|| 1341/1341 [01:31<00:00, 14.58it/s, loss=0.527]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 9 | Train Loss: 0.7688 | Train_ss: 0.1426 | Val Loss: 5.0756 | Val_ss: 0.6226\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10: 100%|| 1341/1341 [01:32<00:00, 14.57it/s, loss=1.36]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 10 | Train Loss: 0.7572 | Train_ss: 0.1633 | Val Loss: 5.7666 | Val_ss: 0.6227\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11: 100%|| 1341/1341 [01:32<00:00, 14.58it/s, loss=0.991]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 11 | Train Loss: 0.7341 | Train_ss: 0.1961 | Val Loss: 3.4065 | Val_ss: 0.6223\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12: 100%|| 1341/1341 [01:32<00:00, 14.55it/s, loss=0.834]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 12 | Train Loss: 0.7293 | Train_ss: 0.2144 | Val Loss: 3.2048 | Val_ss: 0.6227\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13: 100%|| 1341/1341 [01:31<00:00, 14.58it/s, loss=0.645]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 13 | Train Loss: 0.7060 | Train_ss: 0.2478 | Val Loss: 2.3542 | Val_ss: 0.6196\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14: 100%|| 1341/1341 [01:32<00:00, 14.51it/s, loss=1.1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 14 | Train Loss: 0.6823 | Train_ss: 0.2697 | Val Loss: 2.1402 | Val_ss: 0.6261\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15: 100%|| 1341/1341 [01:32<00:00, 14.54it/s, loss=1.5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 15 | Train Loss: 0.6746 | Train_ss: 0.2748 | Val Loss: 2.4305 | Val_ss: 0.6272\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16: 100%|| 1341/1341 [01:32<00:00, 14.57it/s, loss=0.641]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 16 | Train Loss: 0.6662 | Train_ss: 0.2868 | Val Loss: 1.6828 | Val_ss: 0.6267\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17: 100%|| 1341/1341 [01:32<00:00, 14.57it/s, loss=0.423]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 17 | Train Loss: 0.6450 | Train_ss: 0.3207 | Val Loss: 0.6631 | Val_ss: 0.6249\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18: 100%|| 1341/1341 [01:31<00:00, 14.60it/s, loss=1.59]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 18 | Train Loss: 0.6241 | Train_ss: 0.3407 | Val Loss: 1.0074 | Val_ss: 0.6272\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19: 100%|| 1341/1341 [01:32<00:00, 14.55it/s, loss=0.68]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 19 | Train Loss: 0.6169 | Train_ss: 0.3516 | Val Loss: 0.4859 | Val_ss: 0.6293\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20: 100%|| 1341/1341 [01:32<00:00, 14.57it/s, loss=0.268]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 20 | Train Loss: 0.5947 | Train_ss: 0.3687 | Val Loss: 0.5054 | Val_ss: 0.6325\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 21: 100%|| 1341/1341 [01:32<00:00, 14.57it/s, loss=0.947]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 21 | Train Loss: 0.5912 | Train_ss: 0.3740 | Val Loss: 0.3843 | Val_ss: 0.6210\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 22: 100%|| 1341/1341 [01:32<00:00, 14.57it/s, loss=1.59]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 22 | Train Loss: 0.5903 | Train_ss: 0.3826 | Val Loss: 0.4380 | Val_ss: 0.6290\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 23: 100%|| 1341/1341 [01:31<00:00, 14.58it/s, loss=0.218]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 23 | Train Loss: 0.5723 | Train_ss: 0.3886 | Val Loss: 0.3459 | Val_ss: 0.6252\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 24: 100%|| 1341/1341 [01:31<00:00, 14.58it/s, loss=1.09]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 24 | Train Loss: 0.5705 | Train_ss: 0.3995 | Val Loss: 0.3082 | Val_ss: 0.6337\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 25: 100%|| 1341/1341 [01:31<00:00, 14.58it/s, loss=0.198]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 25 | Train Loss: 0.5551 | Train_ss: 0.4094 | Val Loss: 0.4971 | Val_ss: 0.6251\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 26: 100%|| 1341/1341 [01:31<00:00, 14.58it/s, loss=1.03]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 26 | Train Loss: 0.5482 | Train_ss: 0.4128 | Val Loss: 0.6455 | Val_ss: 0.6296\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 27: 100%|| 1341/1341 [01:32<00:00, 14.44it/s, loss=1.16]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 27 | Train Loss: 0.5421 | Train_ss: 0.4172 | Val Loss: 0.4676 | Val_ss: 0.6389\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 28: 100%|| 1341/1341 [01:32<00:00, 14.50it/s, loss=0.429]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 28 | Train Loss: 0.5247 | Train_ss: 0.4343 | Val Loss: 0.8883 | Val_ss: 0.6181\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 29: 100%|| 1341/1341 [01:32<00:00, 14.57it/s, loss=0.45]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 29 | Train Loss: 0.5197 | Train_ss: 0.4433 | Val Loss: 0.9613 | Val_ss: 0.6265\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 30: 100%|| 1341/1341 [01:32<00:00, 14.57it/s, loss=0.953]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 30 | Train Loss: 0.5130 | Train_ss: 0.4483 | Val Loss: 0.6480 | Val_ss: 0.6328\n",
            "Average training time per epoch: 98.41s\n",
            "Test Spearman's : 0.6058\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.5371193170950219, np.float64(0.6057599031864527))"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "# Start Training\n",
        "model, model_kwargs = FluorescencePredict, {\n",
        "    'vocab_size': tokenizer.vocab_size,\n",
        "    'd_model': 256,\n",
        "    'num_layers': 12,\n",
        "    'dim_feedforward': 128,\n",
        "    'p': 0.4,\n",
        "    'len_seq': 512,\n",
        "    'num_heads': 8,\n",
        "    'attn_mechanism': 'MultiLin2D',\n",
        "    'block_size': 30,\n",
        "    'stride': 15,\n",
        "    'k': 10\n",
        "}\n",
        "trained_model, optimizer_MultiLin2D , loss_MultiLin2D = train_model(model, model_kwargs, train_loader, valid_loader, epochs=30, lr=1e-5)\n",
        "evaluate_model(trained_model, test_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZffmcXrbb-fb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "ZffmcXrbb-fb",
        "outputId": "332dd25b-1af4-4ec1-a6ca-68e0858de767"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'avg_val_loss_Plain2D' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2233654828.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Dictionary of all losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m losses = {\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;34m'2D Plain'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mavg_val_loss_Plain2D\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;34m'2D MultiPed'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mloss_Multi2D\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m'2D Lin'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mloss_Lin2D\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'avg_val_loss_Plain2D' is not defined"
          ]
        }
      ],
      "source": [
        "# Dictionary of all losses\n",
        "losses = {\n",
        "    '2D MultiPed': loss_Multi2D,\n",
        "    '2D Lin': loss_Lin2D,\n",
        "    '2D MultiPed Lin': loss_MultiLin2D\n",
        "}\n",
        "import pickle\n",
        "\n",
        "# Save the dictionary\n",
        "with open('losses_FluorescencePredict.pkl', 'wb') as f:\n",
        "    pickle.dump(losses, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PMG4ZKzQlvqF",
      "metadata": {
        "id": "PMG4ZKzQlvqF"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import os\n",
        "\n",
        "filename = 'losses_FluorescencePredict.pkl'\n",
        "\n",
        "# Load existing data if file exists, otherwise start with an empty list\n",
        "if os.path.exists(filename):\n",
        "    with open(filename, 'rb') as f:\n",
        "        all_losses = pickle.load(f)\n",
        "else:\n",
        "    all_losses = []\n",
        "\n",
        "# Your new dictionary of losses\n",
        "losses = {\n",
        "    '2D MultiPed': loss_Multi2D,\n",
        "    '2D Lin': loss_Lin2D,\n",
        "    '2D MultiPed Lin': loss_MultiLin2D\n",
        "}\n",
        "\n",
        "# Append the new dictionary to the list\n",
        "all_losses.update(losses)\n",
        "\n",
        "# Save back to the file\n",
        "with open(filename, 'wb') as f:\n",
        "    pickle.dump(all_losses, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BHe5jWBOl3T9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHe5jWBOl3T9",
        "outputId": "cc9a75ca-5c72-40db-df9c-676adce49194"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['2D Plain', '2D MultiPed', '2D Lin', '2D MultiPed Lin'])"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all_losses.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ys1KIF_0FCfA",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ys1KIF_0FCfA",
        "outputId": "662b0c3c-b2dc-4bf2-91b1-a7ec240635ea"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3323"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gc\n",
        "#del model\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "V01WqMZ0v0uK",
      "metadata": {
        "id": "V01WqMZ0v0uK"
      },
      "outputs": [],
      "source": [
        "# Start Training\n",
        "model, model_kwargs = FluorescencePredict, {\n",
        "    'vocab_size': tokenizer.vocab_size,\n",
        "    'd_model': 24,#256,\n",
        "    'num_layers': 12,\n",
        "    'dim_feedforward': 20,#128,\n",
        "    'p': 0.4,\n",
        "    'len_seq': 512,\n",
        "    'num_heads': 8,\n",
        "    'attn_mechanism': 'Plain3D',\n",
        "    'num_peds': 30,\n",
        "    'k': 30\n",
        "}\n",
        "trained_model, avg_val_loss_Plain2D = train_model(model, model_kwargs, train_loader, valid_loader, epochs=30, lr=0.5e-5)\n",
        "evaluate_model(trained_model, test_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Qucqp54rwXVs",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qucqp54rwXVs",
        "outputId": "441c43a5-8ad9-4f4f-83b0-b5f7d407825c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Starting training from scratch...\n",
            "\n",
            "Multiped3D is activated\n",
            "Multiped3D is activated\n",
            "Multiped3D is activated\n",
            "Multiped3D is activated\n",
            "Multiped3D is activated\n",
            "Multiped3D is activated\n",
            "Multiped3D is activated\n",
            "Multiped3D is activated\n",
            "Multiped3D is activated\n",
            "Multiped3D is activated\n",
            "Multiped3D is activated\n",
            "Multiped3D is activated\n",
            "Total trainable parameters: 22,096,385\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1:  33%|      | 439/1341 [04:38<09:28,  1.59it/s, loss=0.837]"
          ]
        }
      ],
      "source": [
        "# Start Training\n",
        "model, model_kwargs = FluorescencePredict, {\n",
        "    'vocab_size': tokenizer.vocab_size,\n",
        "    'd_model': 256,\n",
        "    'num_layers': 12,\n",
        "    'dim_feedforward': 128,\n",
        "    'p': 0.4,\n",
        "    'len_seq': 512,\n",
        "    'num_heads': 8,\n",
        "    'attn_mechanism': 'Multiped3D',\n",
        "    \"block_size\": 30,\n",
        "    \"stride\": 15\n",
        "\n",
        "}\n",
        "trained_model, optimizer_Multi3D , loss_Multi3D = train_model(model, model_kwargs, train_loader, valid_loader, epochs=50, lr=1e-5)\n",
        "evaluate_model(trained_model, test_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "THSp9_hww-4m",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "THSp9_hww-4m",
        "outputId": "ea954438-9210-41ea-d0af-f7b837fab31f"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Starting training from scratch...\n",
            "\n",
            "Linformer3D is activated\n",
            "Linformer3D is activated\n",
            "Linformer3D is activated\n",
            "Linformer3D is activated\n",
            "Linformer3D is activated\n",
            "Linformer3D is activated\n",
            "Linformer3D is activated\n",
            "Linformer3D is activated\n",
            "Linformer3D is activated\n",
            "Linformer3D is activated\n",
            "Linformer3D is activated\n",
            "Linformer3D is activated\n",
            "Total trainable parameters: 22,590,473\n",
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|| 1341/1341 [10:57<00:00,  2.04it/s, loss=1.02]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 1 | Train Loss: 0.8466 | Train_ss: 0.0290 | Val Loss: 5.0341 | Val_ss: 0.6043\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|| 1341/1341 [10:57<00:00,  2.04it/s, loss=0.451]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 2 | Train Loss: 0.8141 | Train_ss: 0.0607 | Val Loss: 5.7588 | Val_ss: 0.6304\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|| 1341/1341 [10:57<00:00,  2.04it/s, loss=0.8]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 3 | Train Loss: 0.8000 | Train_ss: 0.0604 | Val Loss: 5.6878 | Val_ss: 0.6296\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|| 1341/1341 [10:57<00:00,  2.04it/s, loss=0.564]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 4 | Train Loss: 0.7964 | Train_ss: 0.0748 | Val Loss: 4.2298 | Val_ss: 0.6301\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5: 100%|| 1341/1341 [10:57<00:00,  2.04it/s, loss=0.474]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 5 | Train Loss: 0.7910 | Train_ss: 0.0846 | Val Loss: 7.8734 | Val_ss: 0.6301\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6: 100%|| 1341/1341 [10:58<00:00,  2.04it/s, loss=0.931]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 6 | Train Loss: 0.7806 | Train_ss: 0.1082 | Val Loss: 5.1187 | Val_ss: 0.6272\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7: 100%|| 1341/1341 [10:58<00:00,  2.04it/s, loss=0.126]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 7 | Train Loss: 0.7596 | Train_ss: 0.1241 | Val Loss: 5.6832 | Val_ss: 0.6250\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8: 100%|| 1341/1341 [10:57<00:00,  2.04it/s, loss=1.78]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 8 | Train Loss: 0.7611 | Train_ss: 0.1317 | Val Loss: 3.5965 | Val_ss: 0.6235\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9: 100%|| 1341/1341 [10:57<00:00,  2.04it/s, loss=0.345]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 9 | Train Loss: 0.7404 | Train_ss: 0.1594 | Val Loss: 4.5410 | Val_ss: 0.6238\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10: 100%|| 1341/1341 [10:57<00:00,  2.04it/s, loss=0.301]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 10 | Train Loss: 0.7253 | Train_ss: 0.1866 | Val Loss: 4.2713 | Val_ss: 0.6232\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11: 100%|| 1341/1341 [10:57<00:00,  2.04it/s, loss=0.487]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 11 | Train Loss: 0.7193 | Train_ss: 0.2079 | Val Loss: 4.7766 | Val_ss: 0.6236\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12: 100%|| 1341/1341 [10:57<00:00,  2.04it/s, loss=1.01]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 12 | Train Loss: 0.6923 | Train_ss: 0.2306 | Val Loss: 3.4181 | Val_ss: 0.6205\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 13: 100%|| 1341/1341 [10:57<00:00,  2.04it/s, loss=1.12]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 13 | Train Loss: 0.6818 | Train_ss: 0.2516 | Val Loss: 3.9645 | Val_ss: 0.6204\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 14: 100%|| 1341/1341 [10:57<00:00,  2.04it/s, loss=0.182]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 14 | Train Loss: 0.6516 | Train_ss: 0.2826 | Val Loss: 2.0501 | Val_ss: 0.6051\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 15: 100%|| 1341/1341 [10:57<00:00,  2.04it/s, loss=0.306]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 15 | Train Loss: 0.6119 | Train_ss: 0.3484 | Val Loss: 1.5371 | Val_ss: 0.5669\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 16: 100%|| 1341/1341 [10:58<00:00,  2.04it/s, loss=0.426]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 16 | Train Loss: 0.5819 | Train_ss: 0.3848 | Val Loss: 1.3186 | Val_ss: 0.5686\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 17: 100%|| 1341/1341 [10:58<00:00,  2.04it/s, loss=0.655]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 17 | Train Loss: 0.5579 | Train_ss: 0.4069 | Val Loss: 0.4262 | Val_ss: 0.5703\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18: 100%|| 1341/1341 [10:58<00:00,  2.04it/s, loss=0.726]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 18 | Train Loss: 0.5350 | Train_ss: 0.4402 | Val Loss: 1.0221 | Val_ss: 0.5776\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19: 100%|| 1341/1341 [10:58<00:00,  2.04it/s, loss=0.227]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 19 | Train Loss: 0.5131 | Train_ss: 0.4424 | Val Loss: 0.8282 | Val_ss: 0.6127\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20: 100%|| 1341/1341 [10:58<00:00,  2.04it/s, loss=0.296]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 20 | Train Loss: 0.4906 | Train_ss: 0.4592 | Val Loss: 0.6179 | Val_ss: 0.5774\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 21: 100%|| 1341/1341 [10:58<00:00,  2.04it/s, loss=0.562]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 21 | Train Loss: 0.4655 | Train_ss: 0.4641 | Val Loss: 0.7693 | Val_ss: 0.6361\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 22: 100%|| 1341/1341 [10:58<00:00,  2.04it/s, loss=0.32]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 22 | Train Loss: 0.4323 | Train_ss: 0.4945 | Val Loss: 1.6129 | Val_ss: 0.6437\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 23: 100%|| 1341/1341 [10:58<00:00,  2.04it/s, loss=0.0861]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 23 | Train Loss: 0.4234 | Train_ss: 0.4910 | Val Loss: 1.3964 | Val_ss: 0.6313\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 24: 100%|| 1341/1341 [10:58<00:00,  2.04it/s, loss=0.198]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 24 | Train Loss: 0.4138 | Train_ss: 0.4977 | Val Loss: 0.6039 | Val_ss: 0.6431\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 25: 100%|| 1341/1341 [10:58<00:00,  2.04it/s, loss=0.607]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 25 | Train Loss: 0.3944 | Train_ss: 0.5031 | Val Loss: 1.1987 | Val_ss: 0.6683\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 26: 100%|| 1341/1341 [10:58<00:00,  2.04it/s, loss=0.7]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 26 | Train Loss: 0.3770 | Train_ss: 0.5141 | Val Loss: 1.6969 | Val_ss: 0.6596\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 27: 100%|| 1341/1341 [10:58<00:00,  2.04it/s, loss=0.447]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 27 | Train Loss: 0.3673 | Train_ss: 0.5080 | Val Loss: 0.6147 | Val_ss: 0.6780\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 28: 100%|| 1341/1341 [10:58<00:00,  2.04it/s, loss=0.161]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 28 | Train Loss: 0.3542 | Train_ss: 0.5222 | Val Loss: 0.5207 | Val_ss: 0.6854\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 29: 100%|| 1341/1341 [10:58<00:00,  2.04it/s, loss=0.295]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 29 | Train Loss: 0.3577 | Train_ss: 0.5175 | Val Loss: 1.0568 | Val_ss: 0.6924\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 30: 100%|| 1341/1341 [10:58<00:00,  2.04it/s, loss=0.151]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 30 | Train Loss: 0.3327 | Train_ss: 0.5317 | Val Loss: 0.8912 | Val_ss: 0.6917\n",
            "Average training time per epoch: 703.22s\n",
            "Test Spearman's : 0.6340\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.893426024929547, np.float64(0.6339806719192936))"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "# Start Training\n",
        "model, model_kwargs = FluorescencePredict, {\n",
        "    'vocab_size': tokenizer.vocab_size,\n",
        "    'd_model': 256,\n",
        "    'num_layers': 12,\n",
        "    'dim_feedforward': 128,\n",
        "    'p': 0.4,\n",
        "    'len_seq': 512,\n",
        "    'num_heads': 8,\n",
        "    'attn_mechanism': 'Linformer3D',\n",
        "    'k': 50\n",
        "}\n",
        "trained_model, optimizer_Lin3D , loss_Lin3D = train_model(model, model_kwargs, train_loader, valid_loader, epochs=30, lr=1e-5)\n",
        "evaluate_model(trained_model, test_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pEVXktXH-S4_",
      "metadata": {
        "id": "pEVXktXH-S4_"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import os\n",
        "\n",
        "filename = 'losses_FluorescencePredict.pkl'\n",
        "\n",
        "# Load existing data if file exists, otherwise start with an empty list\n",
        "if os.path.exists(filename):\n",
        "    with open(filename, 'rb') as f:\n",
        "        all_losses = pickle.load(f)\n",
        "else:\n",
        "    all_losses = []\n",
        "# Your new dictionary of losses\n",
        "\n",
        "losses = {\n",
        "    'C2D 3D Linformer': loss_C2D3DLin,\n",
        "}\n",
        "\n",
        "# Append the new dictionary to the list\n",
        "all_losses.update(losses)\n",
        "\n",
        "# Save back to the file\n",
        "with open(filename, 'wb') as f:\n",
        "    pickle.dump(all_losses, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MziQQ08meA7f",
      "metadata": {
        "id": "MziQQ08meA7f"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import signal\n",
        "\n",
        "# Kill the current Python process\n",
        "os.kill(os.getpid(), signal.SIGKILL)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0q5uoyrxKIO",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0q5uoyrxKIO",
        "outputId": "a9af4392-d964-4efa-97e8-3e7b88001964"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Starting training from scratch...\n",
            "\n",
            "Attn3DLinformerMultiPed is activated\n",
            "Attn3DLinformerMultiPed is activated\n",
            "Attn3DLinformerMultiPed is activated\n",
            "Attn3DLinformerMultiPed is activated\n",
            "Attn3DLinformerMultiPed is activated\n",
            "Attn3DLinformerMultiPed is activated\n",
            "Attn3DLinformerMultiPed is activated\n",
            "Attn3DLinformerMultiPed is activated\n",
            "Attn3DLinformerMultiPed is activated\n",
            "Attn3DLinformerMultiPed is activated\n",
            "Attn3DLinformerMultiPed is activated\n",
            "Attn3DLinformerMultiPed is activated\n",
            "Total trainable parameters: 22,107,545\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|| 1341/1341 [03:04<00:00,  7.27it/s, loss=0.584]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 1 | Train Loss: 0.8570 | Train_ss: 0.0376 | Val Loss: 5.9911 | Val_ss: 0.5920\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|| 1341/1341 [03:05<00:00,  7.24it/s, loss=1.64]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 2 | Train Loss: 0.8319 | Train_ss: 0.0445 | Val Loss: 5.7239 | Val_ss: 0.6233\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|| 1341/1341 [03:04<00:00,  7.27it/s, loss=0.384]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 3 | Train Loss: 0.8257 | Train_ss: 0.0574 | Val Loss: 3.8057 | Val_ss: 0.6224\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|| 1341/1341 [03:04<00:00,  7.26it/s, loss=0.348]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 4 | Train Loss: 0.8135 | Train_ss: 0.0742 | Val Loss: 5.9439 | Val_ss: 0.6252\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5: 100%|| 1341/1341 [03:04<00:00,  7.26it/s, loss=0.651]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 5 | Train Loss: 0.8051 | Train_ss: 0.0800 | Val Loss: 4.1948 | Val_ss: 0.6234\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6: 100%|| 1341/1341 [03:04<00:00,  7.27it/s, loss=0.23]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 6 | Train Loss: 0.7940 | Train_ss: 0.0954 | Val Loss: 6.9108 | Val_ss: 0.6237\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7: 100%|| 1341/1341 [03:04<00:00,  7.26it/s, loss=0.345]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 7 | Train Loss: 0.7828 | Train_ss: 0.1105 | Val Loss: 4.5447 | Val_ss: 0.6233\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8: 100%|| 1341/1341 [03:04<00:00,  7.26it/s, loss=0.181]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 8 | Train Loss: 0.7783 | Train_ss: 0.1316 | Val Loss: 5.9128 | Val_ss: 0.6219\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9: 100%|| 1341/1341 [03:04<00:00,  7.25it/s, loss=0.248]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 9 | Train Loss: 0.7653 | Train_ss: 0.1417 | Val Loss: 4.4174 | Val_ss: 0.6223\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10: 100%|| 1341/1341 [03:04<00:00,  7.27it/s, loss=0.542]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 10 | Train Loss: 0.7585 | Train_ss: 0.1524 | Val Loss: 4.9827 | Val_ss: 0.6220\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11: 100%|| 1341/1341 [03:04<00:00,  7.25it/s, loss=0.347]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 11 | Train Loss: 0.7486 | Train_ss: 0.1648 | Val Loss: 4.2367 | Val_ss: 0.6207\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12: 100%|| 1341/1341 [03:04<00:00,  7.27it/s, loss=0.416]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 12 | Train Loss: 0.7286 | Train_ss: 0.1888 | Val Loss: 3.9614 | Val_ss: 0.6207\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 13: 100%|| 1341/1341 [03:04<00:00,  7.27it/s, loss=0.466]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 13 | Train Loss: 0.7086 | Train_ss: 0.2216 | Val Loss: 5.2368 | Val_ss: 0.6210\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 14: 100%|| 1341/1341 [03:04<00:00,  7.26it/s, loss=0.108]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 14 | Train Loss: 0.7069 | Train_ss: 0.2310 | Val Loss: 3.1954 | Val_ss: 0.6197\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 15: 100%|| 1341/1341 [03:04<00:00,  7.27it/s, loss=0.555]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 15 | Train Loss: 0.6832 | Train_ss: 0.2619 | Val Loss: 2.9947 | Val_ss: 0.6203\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 16: 100%|| 1341/1341 [03:05<00:00,  7.25it/s, loss=0.28]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 16 | Train Loss: 0.6725 | Train_ss: 0.2737 | Val Loss: 3.5811 | Val_ss: 0.6210\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 17: 100%|| 1341/1341 [03:04<00:00,  7.26it/s, loss=1.11]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 17 | Train Loss: 0.6522 | Train_ss: 0.3025 | Val Loss: 1.0259 | Val_ss: 0.6209\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 18: 100%|| 1341/1341 [03:04<00:00,  7.26it/s, loss=0.548]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 18 | Train Loss: 0.6416 | Train_ss: 0.3166 | Val Loss: 0.5052 | Val_ss: 0.6138\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 19: 100%|| 1341/1341 [03:04<00:00,  7.25it/s, loss=0.513]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 19 | Train Loss: 0.6322 | Train_ss: 0.3323 | Val Loss: 0.4714 | Val_ss: 0.6193\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 20: 100%|| 1341/1341 [03:05<00:00,  7.24it/s, loss=0.402]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 20 | Train Loss: 0.6186 | Train_ss: 0.3444 | Val Loss: 0.5496 | Val_ss: 0.6263\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 21: 100%|| 1341/1341 [03:04<00:00,  7.25it/s, loss=0.662]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 21 | Train Loss: 0.6109 | Train_ss: 0.3545 | Val Loss: 0.4428 | Val_ss: 0.6190\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 22: 100%|| 1341/1341 [03:04<00:00,  7.25it/s, loss=0.365]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 22 | Train Loss: 0.5898 | Train_ss: 0.3743 | Val Loss: 0.3940 | Val_ss: 0.6202\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 23: 100%|| 1341/1341 [03:04<00:00,  7.25it/s, loss=0.112]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 23 | Train Loss: 0.5881 | Train_ss: 0.3809 | Val Loss: 0.3897 | Val_ss: 0.6309\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 24: 100%|| 1341/1341 [03:04<00:00,  7.26it/s, loss=0.594]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 24 | Train Loss: 0.5645 | Train_ss: 0.4048 | Val Loss: 0.4617 | Val_ss: 0.6264\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 25: 100%|| 1341/1341 [03:05<00:00,  7.25it/s, loss=0.343]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 25 | Train Loss: 0.5623 | Train_ss: 0.4065 | Val Loss: 0.3157 | Val_ss: 0.6296\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 26: 100%|| 1341/1341 [03:04<00:00,  7.25it/s, loss=0.58]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 26 | Train Loss: 0.5469 | Train_ss: 0.4164 | Val Loss: 0.5726 | Val_ss: 0.6187\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 27: 100%|| 1341/1341 [03:05<00:00,  7.25it/s, loss=0.4]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 27 | Train Loss: 0.5368 | Train_ss: 0.4361 | Val Loss: 0.5036 | Val_ss: 0.6173\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 28: 100%|| 1341/1341 [03:04<00:00,  7.25it/s, loss=0.0395]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 28 | Train Loss: 0.5326 | Train_ss: 0.4373 | Val Loss: 0.3202 | Val_ss: 0.6321\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 29: 100%|| 1341/1341 [03:04<00:00,  7.25it/s, loss=0.466]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 29 | Train Loss: 0.5146 | Train_ss: 0.4497 | Val Loss: 0.9628 | Val_ss: 0.6177\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 30: 100%|| 1341/1341 [03:04<00:00,  7.25it/s, loss=0.359]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 30 | Train Loss: 0.5074 | Train_ss: 0.4546 | Val Loss: 1.1296 | Val_ss: 0.6191\n",
            "Average training time per epoch: 197.50s\n",
            "Test Spearman's : 0.5996\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.4647517183578883, np.float64(0.5996491763715334))"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Start Training\n",
        "model, model_kwargs = FluorescencePredict, {\n",
        "    'vocab_size': tokenizer.vocab_size,\n",
        "    'd_model': 256,\n",
        "    'num_layers': 12,\n",
        "    'dim_feedforward': 128,\n",
        "    'p': 0.4,\n",
        "    'len_seq': 512,\n",
        "    'num_heads': 8,\n",
        "    'attn_mechanism': 'Attn3DLinformerMultiPed',\n",
        "    'block_size': 30,\n",
        "    'stride': 15,\n",
        "    'k': 10\n",
        "}\n",
        "trained_model, optimizer_MultiLin3D , loss_MultiLin3D = train_model(model, model_kwargs, train_loader, valid_loader, epochs=30, lr=1e-5)\n",
        "evaluate_model(trained_model, test_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GfBPkC2xal3G",
      "metadata": {
        "id": "GfBPkC2xal3G"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8j6mmLADCsvF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8j6mmLADCsvF",
        "outputId": "3d1956db-480a-4e48-fa7a-630431b517fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Starting training from scratch...\n",
            "\n",
            "Attn2DMultiPed3Dselected is activated\n",
            "Attn2DMultiPed3Dselected is activated\n",
            "Attn2DMultiPed3Dselected is activated\n",
            "Attn2DMultiPed3Dselected is activated\n",
            "Attn2DMultiPed3Dselected is activated\n",
            "Attn2DMultiPed3Dselected is activated\n",
            "Attn2DMultiPed3Dselected is activated\n",
            "Attn2DMultiPed3Dselected is activated\n",
            "Attn2DMultiPed3Dselected is activated\n",
            "Attn2DMultiPed3Dselected is activated\n",
            "Attn2DMultiPed3Dselected is activated\n",
            "Attn2DMultiPed3Dselected is activated\n",
            "Total trainable parameters: 22,121,345\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|| 1341/1341 [03:44<00:00,  5.97it/s, loss=1.04]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 1 | Train Loss: 0.8595 | Train_ss: 0.0182 | Val Loss: 5.4995 | Val_ss: 0.5420\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|| 1341/1341 [03:44<00:00,  5.97it/s, loss=0.217]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 2 | Train Loss: 0.8104 | Train_ss: 0.0538 | Val Loss: 9.9259 | Val_ss: 0.6113\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|| 1341/1341 [03:44<00:00,  5.97it/s, loss=0.782]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 3 | Train Loss: 0.7941 | Train_ss: 0.0819 | Val Loss: 7.5856 | Val_ss: 0.6111\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|| 1341/1341 [03:44<00:00,  5.97it/s, loss=2.05]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 4 | Train Loss: 0.7999 | Train_ss: 0.0731 | Val Loss: 7.0951 | Val_ss: 0.6224\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5: 100%|| 1341/1341 [03:44<00:00,  5.97it/s, loss=0.98]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 5 | Train Loss: 0.7851 | Train_ss: 0.1033 | Val Loss: 7.7834 | Val_ss: 0.6294\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6: 100%|| 1341/1341 [03:44<00:00,  5.96it/s, loss=0.691]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 6 | Train Loss: 0.7820 | Train_ss: 0.0946 | Val Loss: 5.5732 | Val_ss: 0.6253\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7: 100%|| 1341/1341 [03:44<00:00,  5.97it/s, loss=0.118]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 7 | Train Loss: 0.7761 | Train_ss: 0.1149 | Val Loss: 6.8985 | Val_ss: 0.6284\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8: 100%|| 1341/1341 [03:44<00:00,  5.97it/s, loss=0.676]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 8 | Train Loss: 0.7595 | Train_ss: 0.1309 | Val Loss: 4.4702 | Val_ss: 0.6343\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9: 100%|| 1341/1341 [03:44<00:00,  5.96it/s, loss=0.511]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 9 | Train Loss: 0.7503 | Train_ss: 0.1473 | Val Loss: 6.3099 | Val_ss: 0.6325\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10: 100%|| 1341/1341 [03:45<00:00,  5.96it/s, loss=0.551]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 10 | Train Loss: 0.7293 | Train_ss: 0.1820 | Val Loss: 3.8303 | Val_ss: 0.6362\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11: 100%|| 1341/1341 [03:45<00:00,  5.96it/s, loss=1.31]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 11 | Train Loss: 0.7205 | Train_ss: 0.1931 | Val Loss: 5.2806 | Val_ss: 0.6341\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12: 100%|| 1341/1341 [03:45<00:00,  5.96it/s, loss=0.193]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 12 | Train Loss: 0.7037 | Train_ss: 0.2207 | Val Loss: 4.1442 | Val_ss: 0.6405\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 13: 100%|| 1341/1341 [03:45<00:00,  5.95it/s, loss=0.611]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 13 | Train Loss: 0.6878 | Train_ss: 0.2440 | Val Loss: 3.4364 | Val_ss: 0.6277\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 14: 100%|| 1341/1341 [03:45<00:00,  5.95it/s, loss=0.166]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 14 | Train Loss: 0.6803 | Train_ss: 0.2615 | Val Loss: 3.9169 | Val_ss: 0.6369\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 15: 100%|| 1341/1341 [03:45<00:00,  5.96it/s, loss=0.132]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 15 | Train Loss: 0.6531 | Train_ss: 0.2919 | Val Loss: 4.5470 | Val_ss: 0.6414\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 16: 100%|| 1341/1341 [03:45<00:00,  5.96it/s, loss=0.536]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 16 | Train Loss: 0.6389 | Train_ss: 0.3143 | Val Loss: 1.7683 | Val_ss: 0.6383\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 17: 100%|| 1341/1341 [03:44<00:00,  5.96it/s, loss=0.146]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 17 | Train Loss: 0.6222 | Train_ss: 0.3277 | Val Loss: 2.4264 | Val_ss: 0.6402\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 18: 100%|| 1341/1341 [03:44<00:00,  5.96it/s, loss=0.577]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 18 | Train Loss: 0.6031 | Train_ss: 0.3461 | Val Loss: 1.3126 | Val_ss: 0.6423\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 19: 100%|| 1341/1341 [03:45<00:00,  5.96it/s, loss=0.486]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 19 | Train Loss: 0.5797 | Train_ss: 0.3711 | Val Loss: 1.2696 | Val_ss: 0.6460\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 20: 100%|| 1341/1341 [03:45<00:00,  5.96it/s, loss=0.869]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 20 | Train Loss: 0.5677 | Train_ss: 0.3890 | Val Loss: 0.8626 | Val_ss: 0.6418\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 21: 100%|| 1341/1341 [03:45<00:00,  5.96it/s, loss=0.479]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 21 | Train Loss: 0.5593 | Train_ss: 0.3953 | Val Loss: 1.0469 | Val_ss: 0.6476\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 22: 100%|| 1341/1341 [03:44<00:00,  5.96it/s, loss=0.804]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 22 | Train Loss: 0.5591 | Train_ss: 0.4069 | Val Loss: 0.5025 | Val_ss: 0.6409\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 23: 100%|| 1341/1341 [03:45<00:00,  5.96it/s, loss=1.37]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 23 | Train Loss: 0.5375 | Train_ss: 0.4291 | Val Loss: 0.3623 | Val_ss: 0.6627\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 24: 100%|| 1341/1341 [03:45<00:00,  5.96it/s, loss=0.0244]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 24 | Train Loss: 0.5160 | Train_ss: 0.4484 | Val Loss: 0.7263 | Val_ss: 0.6651\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 25: 100%|| 1341/1341 [03:44<00:00,  5.96it/s, loss=0.227]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 25 | Train Loss: 0.5051 | Train_ss: 0.4539 | Val Loss: 0.4305 | Val_ss: 0.6610\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 26: 100%|| 1341/1341 [03:45<00:00,  5.96it/s, loss=0.885]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 26 | Train Loss: 0.4983 | Train_ss: 0.4634 | Val Loss: 0.2958 | Val_ss: 0.6525\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 27: 100%|| 1341/1341 [03:45<00:00,  5.95it/s, loss=1.43]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 27 | Train Loss: 0.4830 | Train_ss: 0.4701 | Val Loss: 0.2754 | Val_ss: 0.6732\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 28: 100%|| 1341/1341 [03:45<00:00,  5.96it/s, loss=1.43]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 28 | Train Loss: 0.4750 | Train_ss: 0.4744 | Val Loss: 0.2637 | Val_ss: 0.6682\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 29: 100%|| 1341/1341 [03:45<00:00,  5.95it/s, loss=0.67]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 29 | Train Loss: 0.4678 | Train_ss: 0.4856 | Val Loss: 0.2913 | Val_ss: 0.6720\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 30: 100%|| 1341/1341 [03:45<00:00,  5.95it/s, loss=0.367]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 30 | Train Loss: 0.4660 | Train_ss: 0.4827 | Val Loss: 0.2884 | Val_ss: 0.6713\n",
            "Average training time per epoch: 242.15s\n",
            "Test Spearman's : 0.6286\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.6955815574436609, np.float64(0.6286389319632872))"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Start Training\n",
        "model, model_kwargs = FluorescencePredict, {\n",
        "    'vocab_size': tokenizer.vocab_size,\n",
        "    'd_model': 256,\n",
        "    'num_layers': 12,\n",
        "    'dim_feedforward': 128,\n",
        "    'p': 0.4,\n",
        "    'len_seq': 512,\n",
        "    'num_heads': 8,\n",
        "    'attn_mechanism': 'Attn2DMultiPed3Dselected',\n",
        "    'block_size': 30,\n",
        "    'stride': 15,\n",
        "    'k': 10\n",
        "}\n",
        "trained_model, optimizer_C2D3DLin , loss_2dmulti3dselect = train_model(model, model_kwargs, train_loader, valid_loader, epochs=30, lr=1e-5)\n",
        "evaluate_model(trained_model, test_loader)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start Training\n",
        "model, model_kwargs = FluorescencePredict, {\n",
        "    'vocab_size': tokenizer.vocab_size,\n",
        "    'd_model': 256,\n",
        "    'num_layers': 12,\n",
        "    'dim_feedforward': 128,\n",
        "    'p': 0.4,\n",
        "    'len_seq': 512,\n",
        "    'num_heads': 8,\n",
        "    'attn_mechanism': 'Attn2DMultiPed3Dselected_Dual',\n",
        "    'block_size': 30,\n",
        "    'stride': 15,\n",
        "    'k': 10\n",
        "}\n",
        "trained_model, optimizer_C2D3DLin , loss_2dmulti3dselect = train_model(model, model_kwargs, train_loader, valid_loader, epochs=30, lr=1e-5)\n",
        "evaluate_model(trained_model, test_loader)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 808
        },
        "id": "E1P_0plDcfNz",
        "outputId": "7f47ea17-5e4e-4726-9730-ce41b7e9afd4"
      },
      "id": "E1P_0plDcfNz",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting training from scratch...\n",
            "\n",
            "Attn2DMultiPed3Dselected_Dual is activated\n",
            "Attn2DMultiPed3Dselected_Dual is activated\n",
            "Attn2DMultiPed3Dselected_Dual is activated\n",
            "Attn2DMultiPed3Dselected_Dual is activated\n",
            "Attn2DMultiPed3Dselected_Dual is activated\n",
            "Attn2DMultiPed3Dselected_Dual is activated\n",
            "Attn2DMultiPed3Dselected_Dual is activated\n",
            "Attn2DMultiPed3Dselected_Dual is activated\n",
            "Attn2DMultiPed3Dselected_Dual is activated\n",
            "Attn2DMultiPed3Dselected_Dual is activated\n",
            "Attn2DMultiPed3Dselected_Dual is activated\n",
            "Attn2DMultiPed3Dselected_Dual is activated\n",
            "Total trainable parameters: 24,850,049\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|| 1341/1341 [06:48<00:00,  3.28it/s, loss=nan]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 1 | Train Loss: nan | Train_ss: nan | Val Loss: nan | Val_ss: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|| 1341/1341 [06:47<00:00,  3.29it/s, loss=nan]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 2 | Train Loss: nan | Train_ss: nan | Val Loss: nan | Val_ss: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|| 1341/1341 [06:48<00:00,  3.29it/s, loss=nan]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 3 | Train Loss: nan | Train_ss: nan | Val Loss: nan | Val_ss: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|| 1341/1341 [06:48<00:00,  3.29it/s, loss=nan]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 4 | Train Loss: nan | Train_ss: nan | Val Loss: nan | Val_ss: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5:  32%|      | 424/1341 [02:09<04:39,  3.28it/s, loss=nan]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4220639711.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;34m'k'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m }\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mtrained_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_C2D3DLin\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mloss_2dmulti3dselect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrained_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1077128608.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model_class, model_kwargs, train_loader, val_loader, epochs, lr, continue_train, trained_model, trained_optimizer)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-705914771.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids)\u001b[0m\n\u001b[1;32m    224\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m       \u001b[0;31m#print(f'shape of output of encoder layers : {x.shape}')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m       \u001b[0;31m# Apply the classifier and squeeze the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-705914771.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mffn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mffn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2464520467.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;31m# Branch 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mq2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW_q2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW_k2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW_l2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m         \u001b[0mQ2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_split_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sliding_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mq2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0mout2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_branch_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "obGdZluRyBWr",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "obGdZluRyBWr",
        "outputId": "101ab0b3-907b-4109-d930-d323230bfc05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Starting training from scratch...\n",
            "\n",
            "Attn2D3DLinAlter is activated\n",
            "Attn2D3DLinAlter is activated\n",
            "Attn2D3DLinAlter is activated\n",
            "Attn2D3DLinAlter is activated\n",
            "Attn2D3DLinAlter is activated\n",
            "Attn2D3DLinAlter is activated\n",
            "Attn2D3DLinAlter is activated\n",
            "Attn2D3DLinAlter is activated\n",
            "Attn2D3DLinAlter is activated\n",
            "Attn2D3DLinAlter is activated\n",
            "Attn2D3DLinAlter is activated\n",
            "Attn2D3DLinAlter is activated\n",
            "Total trainable parameters: 22,616,969\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|| 1341/1341 [12:33<00:00,  1.78it/s, loss=1.38]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 1 | Train Loss: 0.8492 | Train_ss: 0.0181 | Val Loss: 5.0629 | Val_ss: 0.5969\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|| 1341/1341 [12:31<00:00,  1.78it/s, loss=0.101]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 2 | Train Loss: 0.8111 | Train_ss: 0.0468 | Val Loss: 6.2868 | Val_ss: 0.6207\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|| 1341/1341 [12:31<00:00,  1.78it/s, loss=0.572]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 3 | Train Loss: 0.8133 | Train_ss: 0.0586 | Val Loss: 7.3496 | Val_ss: 0.6220\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|| 1341/1341 [12:31<00:00,  1.78it/s, loss=0.289]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 4 | Train Loss: 0.7996 | Train_ss: 0.0828 | Val Loss: 7.6810 | Val_ss: 0.6194\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5: 100%|| 1341/1341 [12:31<00:00,  1.78it/s, loss=0.5]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 5 | Train Loss: 0.7904 | Train_ss: 0.0954 | Val Loss: 5.4950 | Val_ss: 0.6179\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6: 100%|| 1341/1341 [12:31<00:00,  1.78it/s, loss=0.585]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 6 | Train Loss: 0.7808 | Train_ss: 0.1163 | Val Loss: 5.7130 | Val_ss: 0.6162\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7: 100%|| 1341/1341 [12:31<00:00,  1.78it/s, loss=0.249]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 7 | Train Loss: 0.7723 | Train_ss: 0.1215 | Val Loss: 5.9608 | Val_ss: 0.6160\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8: 100%|| 1341/1341 [12:31<00:00,  1.78it/s, loss=0.487]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 8 | Train Loss: 0.7676 | Train_ss: 0.1276 | Val Loss: 5.4255 | Val_ss: 0.6132\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9: 100%|| 1341/1341 [12:31<00:00,  1.78it/s, loss=0.654]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 9 | Train Loss: 0.7522 | Train_ss: 0.1607 | Val Loss: 4.1362 | Val_ss: 0.6121\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10: 100%|| 1341/1341 [12:31<00:00,  1.78it/s, loss=0.746]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 10 | Train Loss: 0.7314 | Train_ss: 0.1896 | Val Loss: 3.7445 | Val_ss: 0.6089\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11: 100%|| 1341/1341 [12:31<00:00,  1.78it/s, loss=0.472]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 11 | Train Loss: 0.7151 | Train_ss: 0.2035 | Val Loss: 2.5074 | Val_ss: 0.6016\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12: 100%|| 1341/1341 [12:31<00:00,  1.78it/s, loss=1.36]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 12 | Train Loss: 0.6962 | Train_ss: 0.2277 | Val Loss: 2.5577 | Val_ss: 0.6087\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 13: 100%|| 1341/1341 [12:31<00:00,  1.78it/s, loss=1.52]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 13 | Train Loss: 0.6795 | Train_ss: 0.2644 | Val Loss: 1.7141 | Val_ss: 0.6036\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 14: 100%|| 1341/1341 [12:31<00:00,  1.78it/s, loss=0.193]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 14 | Train Loss: 0.6593 | Train_ss: 0.2852 | Val Loss: 1.9329 | Val_ss: 0.6073\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 15: 100%|| 1341/1341 [12:31<00:00,  1.78it/s, loss=0.224]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 15 | Train Loss: 0.6512 | Train_ss: 0.2984 | Val Loss: 1.2664 | Val_ss: 0.6015\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 16: 100%|| 1341/1341 [12:31<00:00,  1.78it/s, loss=0.117]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 16 | Train Loss: 0.6239 | Train_ss: 0.3335 | Val Loss: 0.7169 | Val_ss: 0.5947\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 17: 100%|| 1341/1341 [12:31<00:00,  1.78it/s, loss=0.373]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 17 | Train Loss: 0.6086 | Train_ss: 0.3505 | Val Loss: 0.3738 | Val_ss: 0.6010\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 18: 100%|| 1341/1341 [12:31<00:00,  1.78it/s, loss=0.152]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 18 | Train Loss: 0.5870 | Train_ss: 0.3769 | Val Loss: 0.4816 | Val_ss: 0.5987\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 19: 100%|| 1341/1341 [12:31<00:00,  1.78it/s, loss=0.693]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 19 | Train Loss: 0.5759 | Train_ss: 0.3901 | Val Loss: 0.3805 | Val_ss: 0.6078\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 20: 100%|| 1341/1341 [12:31<00:00,  1.78it/s, loss=0.415]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 20 | Train Loss: 0.5472 | Train_ss: 0.4132 | Val Loss: 0.3426 | Val_ss: 0.6092\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 21: 100%|| 1341/1341 [12:31<00:00,  1.78it/s, loss=0.435]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 21 | Train Loss: 0.5445 | Train_ss: 0.4185 | Val Loss: 0.4711 | Val_ss: 0.6118\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 22: 100%|| 1341/1341 [12:31<00:00,  1.78it/s, loss=0.552]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 22 | Train Loss: 0.5288 | Train_ss: 0.4337 | Val Loss: 0.6726 | Val_ss: 0.6233\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 23: 100%|| 1341/1341 [12:31<00:00,  1.78it/s, loss=0.607]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 23 | Train Loss: 0.5210 | Train_ss: 0.4504 | Val Loss: 0.3307 | Val_ss: 0.6207\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 24: 100%|| 1341/1341 [12:31<00:00,  1.78it/s, loss=0.233]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 24 | Train Loss: 0.5011 | Train_ss: 0.4595 | Val Loss: 0.9148 | Val_ss: 0.6230\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 25: 100%|| 1341/1341 [12:31<00:00,  1.78it/s, loss=0.361]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 25 | Train Loss: 0.4984 | Train_ss: 0.4694 | Val Loss: 0.5206 | Val_ss: 0.6347\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 26: 100%|| 1341/1341 [12:31<00:00,  1.78it/s, loss=0.479]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 26 | Train Loss: 0.4785 | Train_ss: 0.4802 | Val Loss: 0.6564 | Val_ss: 0.6263\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 27: 100%|| 1341/1341 [12:32<00:00,  1.78it/s, loss=0.269]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 27 | Train Loss: 0.4750 | Train_ss: 0.4832 | Val Loss: 0.7729 | Val_ss: 0.6333\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 28: 100%|| 1341/1341 [12:32<00:00,  1.78it/s, loss=0.209]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 28 | Train Loss: 0.4705 | Train_ss: 0.4884 | Val Loss: 1.2207 | Val_ss: 0.6274\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 29: 100%|| 1341/1341 [12:32<00:00,  1.78it/s, loss=0.456]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 29 | Train Loss: 0.4569 | Train_ss: 0.4991 | Val Loss: 1.2858 | Val_ss: 0.6411\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 30: 100%|| 1341/1341 [12:32<00:00,  1.78it/s, loss=0.416]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 30 | Train Loss: 0.4519 | Train_ss: 0.5021 | Val Loss: 0.6752 | Val_ss: 0.6394\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 31: 100%|| 1341/1341 [12:32<00:00,  1.78it/s, loss=0.399]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 31 | Train Loss: 0.4516 | Train_ss: 0.5016 | Val Loss: 1.0476 | Val_ss: 0.6456\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 32: 100%|| 1341/1341 [12:32<00:00,  1.78it/s, loss=1.58]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 32 | Train Loss: 0.4419 | Train_ss: 0.5076 | Val Loss: 0.9131 | Val_ss: 0.6524\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 33: 100%|| 1341/1341 [12:31<00:00,  1.78it/s, loss=0.237]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 33 | Train Loss: 0.4288 | Train_ss: 0.5133 | Val Loss: 1.2870 | Val_ss: 0.6560\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 34:  57%|    | 762/1341 [07:07<05:24,  1.78it/s, loss=0.681]"
          ]
        }
      ],
      "source": [
        "# Start Training\n",
        "model, model_kwargs = FluorescencePredict, {\n",
        "    'vocab_size': tokenizer.vocab_size,\n",
        "    'd_model': 256,\n",
        "    'num_layers': 12,\n",
        "    'dim_feedforward': 128,\n",
        "    'p': 0.4,\n",
        "    'len_seq': 512,\n",
        "    'num_heads': 8,\n",
        "    'attn_mechanism': 'Attn2D3DLinAlter',\n",
        "    'num_peds': 20,\n",
        "    'k': 50\n",
        "}\n",
        "trained_model, optimizer_C2D3DLin , loss_C2D3DLin = train_model(model, model_kwargs, train_loader, valid_loader, epochs=50, lr=1e-5)\n",
        "evaluate_model(trained_model, test_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Lu8onGqXyARX",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lu8onGqXyARX",
        "outputId": "ec1834dc-341f-4923-b90d-0d030cd2c320"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Starting training from scratch...\n",
            "\n",
            "Attn2D3D_MultiPedAlter is activated\n",
            "Attn2D3D_MultiPedAlter is activated\n",
            "Attn2D3D_MultiPedAlter is activated\n",
            "Attn2D3D_MultiPedAlter is activated\n",
            "Attn2D3D_MultiPedAlter is activated\n",
            "Attn2D3D_MultiPedAlter is activated\n",
            "Attn2D3D_MultiPedAlter is activated\n",
            "Attn2D3D_MultiPedAlter is activated\n",
            "Attn2D3D_MultiPedAlter is activated\n",
            "Attn2D3D_MultiPedAlter is activated\n",
            "Attn2D3D_MultiPedAlter is activated\n",
            "Attn2D3D_MultiPedAlter is activated\n",
            "Total trainable parameters: 21,957,761\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|| 1341/1341 [08:49<00:00,  2.53it/s, loss=1.22]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 1 | Train Loss: 0.8476 | Train_ss: 0.0352 | Val Loss: 5.0345 | Val_ss: 0.6095\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|| 1341/1341 [08:49<00:00,  2.53it/s, loss=1.16]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 2 | Train Loss: 0.8129 | Train_ss: 0.0506 | Val Loss: 5.6253 | Val_ss: 0.6268\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|| 1341/1341 [08:49<00:00,  2.53it/s, loss=0.225]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 3 | Train Loss: 0.8027 | Train_ss: 0.0657 | Val Loss: 8.0635 | Val_ss: 0.6314\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|| 1341/1341 [08:49<00:00,  2.53it/s, loss=0.393]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 4 | Train Loss: 0.7796 | Train_ss: 0.0930 | Val Loss: 8.0187 | Val_ss: 0.6281\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5: 100%|| 1341/1341 [08:49<00:00,  2.53it/s, loss=1.37]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 5 | Train Loss: 0.7834 | Train_ss: 0.0890 | Val Loss: 6.0410 | Val_ss: 0.6274\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6: 100%|| 1341/1341 [08:49<00:00,  2.53it/s, loss=0.935]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 6 | Train Loss: 0.7811 | Train_ss: 0.1046 | Val Loss: 6.0272 | Val_ss: 0.6263\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7: 100%|| 1341/1341 [08:49<00:00,  2.53it/s, loss=1.07]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 7 | Train Loss: 0.7572 | Train_ss: 0.1276 | Val Loss: 4.5868 | Val_ss: 0.6255\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8: 100%|| 1341/1341 [08:49<00:00,  2.53it/s, loss=0.187]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 8 | Train Loss: 0.7516 | Train_ss: 0.1366 | Val Loss: 7.4360 | Val_ss: 0.6221\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9: 100%|| 1341/1341 [08:49<00:00,  2.53it/s, loss=0.695]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 9 | Train Loss: 0.7374 | Train_ss: 0.1569 | Val Loss: 5.2353 | Val_ss: 0.6239\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10: 100%|| 1341/1341 [08:49<00:00,  2.53it/s, loss=0.784]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 10 | Train Loss: 0.7257 | Train_ss: 0.1781 | Val Loss: 3.4555 | Val_ss: 0.6231\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11: 100%|| 1341/1341 [08:49<00:00,  2.53it/s, loss=0.0669]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 11 | Train Loss: 0.7086 | Train_ss: 0.2090 | Val Loss: 5.0123 | Val_ss: 0.6241\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12: 100%|| 1341/1341 [08:49<00:00,  2.53it/s, loss=0.589]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 12 | Train Loss: 0.6982 | Train_ss: 0.2183 | Val Loss: 3.3033 | Val_ss: 0.6238\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 13: 100%|| 1341/1341 [08:49<00:00,  2.53it/s, loss=0.586]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 13 | Train Loss: 0.6793 | Train_ss: 0.2492 | Val Loss: 2.1950 | Val_ss: 0.6228\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 14: 100%|| 1341/1341 [08:49<00:00,  2.53it/s, loss=0.0531]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 14 | Train Loss: 0.6617 | Train_ss: 0.2711 | Val Loss: 2.5299 | Val_ss: 0.6221\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 15: 100%|| 1341/1341 [08:49<00:00,  2.53it/s, loss=0.702]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 15 | Train Loss: 0.6393 | Train_ss: 0.3015 | Val Loss: 1.6979 | Val_ss: 0.6241\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 16: 100%|| 1341/1341 [08:49<00:00,  2.53it/s, loss=0.109]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 16 | Train Loss: 0.6186 | Train_ss: 0.3250 | Val Loss: 1.0922 | Val_ss: 0.6225\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 17: 100%|| 1341/1341 [08:49<00:00,  2.53it/s, loss=0.95]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 17 | Train Loss: 0.6022 | Train_ss: 0.3556 | Val Loss: 0.5137 | Val_ss: 0.6166\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 18: 100%|| 1341/1341 [08:49<00:00,  2.53it/s, loss=0.466]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 18 | Train Loss: 0.5805 | Train_ss: 0.3741 | Val Loss: 0.3621 | Val_ss: 0.6127\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 19: 100%|| 1341/1341 [08:49<00:00,  2.53it/s, loss=0.199]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 19 | Train Loss: 0.5562 | Train_ss: 0.3975 | Val Loss: 0.3435 | Val_ss: 0.6207\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 20: 100%|| 1341/1341 [08:49<00:00,  2.53it/s, loss=0.493]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 20 | Train Loss: 0.5504 | Train_ss: 0.4173 | Val Loss: 0.3987 | Val_ss: 0.6142\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 21: 100%|| 1341/1341 [08:49<00:00,  2.53it/s, loss=0.343]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 21 | Train Loss: 0.5204 | Train_ss: 0.4278 | Val Loss: 0.4054 | Val_ss: 0.6248\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 22: 100%|| 1341/1341 [08:48<00:00,  2.54it/s, loss=0.213]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 22 | Train Loss: 0.5083 | Train_ss: 0.4486 | Val Loss: 0.6332 | Val_ss: 0.6302\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 23: 100%|| 1341/1341 [08:49<00:00,  2.53it/s, loss=0.847]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 23 | Train Loss: 0.4949 | Train_ss: 0.4534 | Val Loss: 0.8600 | Val_ss: 0.6285\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 24: 100%|| 1341/1341 [08:48<00:00,  2.54it/s, loss=0.0689]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 24 | Train Loss: 0.4801 | Train_ss: 0.4654 | Val Loss: 0.3652 | Val_ss: 0.6400\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 25: 100%|| 1341/1341 [08:48<00:00,  2.54it/s, loss=0.249]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 25 | Train Loss: 0.4619 | Train_ss: 0.4796 | Val Loss: 0.3217 | Val_ss: 0.6477\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 26: 100%|| 1341/1341 [08:49<00:00,  2.53it/s, loss=0.598]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 26 | Train Loss: 0.4496 | Train_ss: 0.4897 | Val Loss: 0.6448 | Val_ss: 0.6475\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 27: 100%|| 1341/1341 [08:49<00:00,  2.53it/s, loss=0.283]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 27 | Train Loss: 0.4428 | Train_ss: 0.4926 | Val Loss: 0.5646 | Val_ss: 0.6500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 28: 100%|| 1341/1341 [08:49<00:00,  2.53it/s, loss=0.12]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 28 | Train Loss: 0.4284 | Train_ss: 0.5053 | Val Loss: 0.2833 | Val_ss: 0.6547\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 29: 100%|| 1341/1341 [08:49<00:00,  2.53it/s, loss=0.957]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 29 | Train Loss: 0.4236 | Train_ss: 0.5088 | Val Loss: 0.2482 | Val_ss: 0.6591\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 30: 100%|| 1341/1341 [08:48<00:00,  2.54it/s, loss=0.376]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 30 | Train Loss: 0.4094 | Train_ss: 0.5044 | Val Loss: 0.6323 | Val_ss: 0.6650\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 31: 100%|| 1341/1341 [08:49<00:00,  2.53it/s, loss=0.547]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 31 | Train Loss: 0.4049 | Train_ss: 0.5162 | Val Loss: 0.9142 | Val_ss: 0.6611\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 32: 100%|| 1341/1341 [08:49<00:00,  2.53it/s, loss=0.0763]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 32 | Train Loss: 0.3973 | Train_ss: 0.5275 | Val Loss: 0.5101 | Val_ss: 0.6602\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 33: 100%|| 1341/1341 [08:49<00:00,  2.53it/s, loss=0.495]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 33 | Train Loss: 0.3942 | Train_ss: 0.5204 | Val Loss: 0.3505 | Val_ss: 0.6719\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 34: 100%|| 1341/1341 [08:48<00:00,  2.54it/s, loss=0.284]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 34 | Train Loss: 0.3938 | Train_ss: 0.5263 | Val Loss: 0.2286 | Val_ss: 0.6739\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 35: 100%|| 1341/1341 [08:48<00:00,  2.53it/s, loss=0.344]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 35 | Train Loss: 0.3776 | Train_ss: 0.5294 | Val Loss: 0.3946 | Val_ss: 0.6805\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 36: 100%|| 1341/1341 [08:48<00:00,  2.54it/s, loss=0.424]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 36 | Train Loss: 0.3803 | Train_ss: 0.5391 | Val Loss: 0.4480 | Val_ss: 0.6782\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 37: 100%|| 1341/1341 [08:48<00:00,  2.54it/s, loss=0.689]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 37 | Train Loss: 0.3821 | Train_ss: 0.5289 | Val Loss: 0.3879 | Val_ss: 0.6813\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 38: 100%|| 1341/1341 [08:48<00:00,  2.54it/s, loss=0.877]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 38 | Train Loss: 0.3659 | Train_ss: 0.5426 | Val Loss: 0.2104 | Val_ss: 0.6814\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 39: 100%|| 1341/1341 [08:49<00:00,  2.53it/s, loss=0.722]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 39 | Train Loss: 0.3608 | Train_ss: 0.5489 | Val Loss: 0.2579 | Val_ss: 0.6892\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 40: 100%|| 1341/1341 [08:48<00:00,  2.54it/s, loss=0.171]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 40 | Train Loss: 0.3588 | Train_ss: 0.5421 | Val Loss: 0.2350 | Val_ss: 0.6881\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 41: 100%|| 1341/1341 [08:48<00:00,  2.54it/s, loss=0.467]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 41 | Train Loss: 0.3499 | Train_ss: 0.5506 | Val Loss: 0.2011 | Val_ss: 0.6929\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 42: 100%|| 1341/1341 [08:49<00:00,  2.53it/s, loss=0.037]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 42 | Train Loss: 0.3495 | Train_ss: 0.5459 | Val Loss: 0.2264 | Val_ss: 0.6859\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 43: 100%|| 1341/1341 [08:48<00:00,  2.54it/s, loss=0.125]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 43 | Train Loss: 0.3546 | Train_ss: 0.5407 | Val Loss: 0.2437 | Val_ss: 0.6919\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 44: 100%|| 1341/1341 [08:48<00:00,  2.54it/s, loss=0.181]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 44 | Train Loss: 0.3447 | Train_ss: 0.5498 | Val Loss: 0.1938 | Val_ss: 0.6951\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 45: 100%|| 1341/1341 [08:49<00:00,  2.53it/s, loss=0.54]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 45 | Train Loss: 0.3333 | Train_ss: 0.5573 | Val Loss: 0.1872 | Val_ss: 0.6981\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 46: 100%|| 1341/1341 [08:48<00:00,  2.54it/s, loss=0.463]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 46 | Train Loss: 0.3299 | Train_ss: 0.5593 | Val Loss: 0.2312 | Val_ss: 0.6980\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 47: 100%|| 1341/1341 [08:49<00:00,  2.53it/s, loss=0.144]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 47 | Train Loss: 0.3275 | Train_ss: 0.5581 | Val Loss: 0.2055 | Val_ss: 0.7003\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 48:  10%|         | 130/1341 [00:51<07:58,  2.53it/s, loss=0.257]"
          ]
        }
      ],
      "source": [
        "# Start Training\n",
        "model, model_kwargs = FluorescencePredict, {\n",
        "    'vocab_size': tokenizer.vocab_size,\n",
        "    'd_model': 256,\n",
        "    'num_layers': 12,\n",
        "    'dim_feedforward': 128,\n",
        "    'p': 0.4,\n",
        "    'len_seq': 512,\n",
        "    'num_heads': 8,\n",
        "    'attn_mechanism': 'Attn2D3D_MultiPedAlter',\n",
        "    'num_peds': 20,\n",
        "}\n",
        "trained_model, optimizer_C2D3DMulti , loss_C2D3DMulti = train_model(model, model_kwargs, train_loader, valid_loader, epochs=50, lr=1e-5)\n",
        "evaluate_model(trained_model, test_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "z4Uc_bomxvln",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4Uc_bomxvln",
        "outputId": "fdd8b361-965b-4c80-985c-05f3a0dcc96c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Spearman's : 0.6437\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.8179461062764344, np.float64(0.6436688570899828))"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate_model(trained_model, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yvvXms1PyBPu",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvvXms1PyBPu",
        "outputId": "ff919abf-ddb1-4ade-f376-9ee21d273755"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Starting training from scratch...\n",
            "\n",
            "Attn2D3D_MultiPedLinAlter is activated\n",
            "Attn2D3D_MultiPedLinAlter is activated\n",
            "Attn2D3D_MultiPedLinAlter is activated\n",
            "Attn2D3D_MultiPedLinAlter is activated\n",
            "Attn2D3D_MultiPedLinAlter is activated\n",
            "Attn2D3D_MultiPedLinAlter is activated\n",
            "Attn2D3D_MultiPedLinAlter is activated\n",
            "Attn2D3D_MultiPedLinAlter is activated\n",
            "Attn2D3D_MultiPedLinAlter is activated\n",
            "Attn2D3D_MultiPedLinAlter is activated\n",
            "Attn2D3D_MultiPedLinAlter is activated\n",
            "Attn2D3D_MultiPedLinAlter is activated\n",
            "Total trainable parameters: 22,145,321\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|| 1341/1341 [08:10<00:00,  2.73it/s, loss=0.901]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 1 | Train Loss: 0.8294 | Train_ss: 0.0351 | Val Loss: 4.3295 | Val_ss: 0.6013\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|| 1341/1341 [08:08<00:00,  2.74it/s, loss=0.417]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 2 | Train Loss: 0.8090 | Train_ss: 0.0436 | Val Loss: 6.4831 | Val_ss: 0.6218\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|| 1341/1341 [08:08<00:00,  2.74it/s, loss=0.577]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 3 | Train Loss: 0.7968 | Train_ss: 0.0594 | Val Loss: 8.8073 | Val_ss: 0.6260\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|| 1341/1341 [08:08<00:00,  2.74it/s, loss=1.57]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 4 | Train Loss: 0.7951 | Train_ss: 0.0642 | Val Loss: 5.9414 | Val_ss: 0.6231\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5: 100%|| 1341/1341 [08:08<00:00,  2.74it/s, loss=1.37]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 5 | Train Loss: 0.7746 | Train_ss: 0.0984 | Val Loss: 5.8786 | Val_ss: 0.6232\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6: 100%|| 1341/1341 [08:08<00:00,  2.74it/s, loss=0.567]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 6 | Train Loss: 0.7746 | Train_ss: 0.1057 | Val Loss: 7.4917 | Val_ss: 0.6212\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7: 100%|| 1341/1341 [08:08<00:00,  2.74it/s, loss=0.203]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 7 | Train Loss: 0.7590 | Train_ss: 0.1173 | Val Loss: 8.3609 | Val_ss: 0.6198\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8: 100%|| 1341/1341 [08:08<00:00,  2.74it/s, loss=0.309]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 8 | Train Loss: 0.7475 | Train_ss: 0.1479 | Val Loss: 8.2729 | Val_ss: 0.6193\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9: 100%|| 1341/1341 [08:08<00:00,  2.74it/s, loss=0.668]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 9 | Train Loss: 0.7441 | Train_ss: 0.1539 | Val Loss: 3.7358 | Val_ss: 0.6174\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10: 100%|| 1341/1341 [08:08<00:00,  2.74it/s, loss=0.367]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 10 | Train Loss: 0.7126 | Train_ss: 0.1977 | Val Loss: 3.0285 | Val_ss: 0.6169\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11: 100%|| 1341/1341 [08:08<00:00,  2.74it/s, loss=0.624]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 11 | Train Loss: 0.7082 | Train_ss: 0.2016 | Val Loss: 3.4837 | Val_ss: 0.6177\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12: 100%|| 1341/1341 [08:08<00:00,  2.74it/s, loss=0.303]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 12 | Train Loss: 0.6804 | Train_ss: 0.2387 | Val Loss: 2.3288 | Val_ss: 0.6124\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 13: 100%|| 1341/1341 [08:08<00:00,  2.74it/s, loss=0.572]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 13 | Train Loss: 0.6718 | Train_ss: 0.2518 | Val Loss: 1.5825 | Val_ss: 0.6114\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 14: 100%|| 1341/1341 [08:08<00:00,  2.74it/s, loss=0.311]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 14 | Train Loss: 0.6545 | Train_ss: 0.2852 | Val Loss: 1.1525 | Val_ss: 0.6097\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 15: 100%|| 1341/1341 [08:08<00:00,  2.74it/s, loss=0.754]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 15 | Train Loss: 0.6320 | Train_ss: 0.3052 | Val Loss: 1.0242 | Val_ss: 0.6123\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 16: 100%|| 1341/1341 [08:08<00:00,  2.74it/s, loss=0.285]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 16 | Train Loss: 0.6063 | Train_ss: 0.3445 | Val Loss: 0.6408 | Val_ss: 0.6074\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 17: 100%|| 1341/1341 [08:08<00:00,  2.74it/s, loss=0.127]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 17 | Train Loss: 0.5945 | Train_ss: 0.3558 | Val Loss: 0.3789 | Val_ss: 0.5999\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 18: 100%|| 1341/1341 [08:08<00:00,  2.74it/s, loss=0.285]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 18 | Train Loss: 0.5732 | Train_ss: 0.3819 | Val Loss: 0.4301 | Val_ss: 0.6116\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 19: 100%|| 1341/1341 [08:08<00:00,  2.74it/s, loss=1.77]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 19 | Train Loss: 0.5421 | Train_ss: 0.4160 | Val Loss: 0.3634 | Val_ss: 0.6094\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 20: 100%|| 1341/1341 [08:08<00:00,  2.74it/s, loss=0.213]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 20 | Train Loss: 0.5337 | Train_ss: 0.4200 | Val Loss: 0.4573 | Val_ss: 0.6097\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 21: 100%|| 1341/1341 [08:08<00:00,  2.74it/s, loss=0.243]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 21 | Train Loss: 0.5243 | Train_ss: 0.4361 | Val Loss: 0.4947 | Val_ss: 0.6120\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 22: 100%|| 1341/1341 [08:08<00:00,  2.74it/s, loss=0.623]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 22 | Train Loss: 0.5136 | Train_ss: 0.4465 | Val Loss: 0.3618 | Val_ss: 0.6126\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 23: 100%|| 1341/1341 [08:08<00:00,  2.74it/s, loss=2.11]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 23 | Train Loss: 0.5077 | Train_ss: 0.4551 | Val Loss: 0.7793 | Val_ss: 0.6092\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 24: 100%|| 1341/1341 [08:09<00:00,  2.74it/s, loss=0.281]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 24 | Train Loss: 0.4894 | Train_ss: 0.4608 | Val Loss: 0.5426 | Val_ss: 0.6187\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 25: 100%|| 1341/1341 [08:08<00:00,  2.74it/s, loss=0.359]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 25 | Train Loss: 0.4775 | Train_ss: 0.4762 | Val Loss: 0.7136 | Val_ss: 0.6175\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 26: 100%|| 1341/1341 [08:08<00:00,  2.74it/s, loss=0.214]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 26 | Train Loss: 0.4726 | Train_ss: 0.4813 | Val Loss: 0.4145 | Val_ss: 0.6237\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 27: 100%|| 1341/1341 [08:08<00:00,  2.74it/s, loss=0.171]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 27 | Train Loss: 0.4607 | Train_ss: 0.4875 | Val Loss: 0.6541 | Val_ss: 0.6283\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 28: 100%|| 1341/1341 [08:08<00:00,  2.74it/s, loss=0.51]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 28 | Train Loss: 0.4564 | Train_ss: 0.4946 | Val Loss: 0.6411 | Val_ss: 0.6313\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 29: 100%|| 1341/1341 [08:08<00:00,  2.74it/s, loss=0.465]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 29 | Train Loss: 0.4422 | Train_ss: 0.5031 | Val Loss: 0.7135 | Val_ss: 0.6390\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 30: 100%|| 1341/1341 [08:08<00:00,  2.74it/s, loss=0.773]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 30 | Train Loss: 0.4364 | Train_ss: 0.5162 | Val Loss: 0.6265 | Val_ss: 0.6380\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 31: 100%|| 1341/1341 [08:08<00:00,  2.74it/s, loss=0.772]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 31 | Train Loss: 0.4314 | Train_ss: 0.5115 | Val Loss: 0.5501 | Val_ss: 0.6384\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 32: 100%|| 1341/1341 [08:08<00:00,  2.74it/s, loss=0.816]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 32 | Train Loss: 0.4296 | Train_ss: 0.5050 | Val Loss: 0.2590 | Val_ss: 0.6527\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 33: 100%|| 1341/1341 [08:09<00:00,  2.74it/s, loss=0.187]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 33 | Train Loss: 0.4182 | Train_ss: 0.5123 | Val Loss: 0.3186 | Val_ss: 0.6511\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 34: 100%|| 1341/1341 [08:08<00:00,  2.74it/s, loss=0.21]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 34 | Train Loss: 0.4149 | Train_ss: 0.5160 | Val Loss: 0.4529 | Val_ss: 0.6520\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 35: 100%|| 1341/1341 [08:09<00:00,  2.74it/s, loss=0.888]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 35 | Train Loss: 0.4159 | Train_ss: 0.5256 | Val Loss: 0.3814 | Val_ss: 0.6621\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 36: 100%|| 1341/1341 [08:08<00:00,  2.74it/s, loss=0.115]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 36 | Train Loss: 0.4010 | Train_ss: 0.5257 | Val Loss: 0.5443 | Val_ss: 0.6550\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 37: 100%|| 1341/1341 [08:08<00:00,  2.74it/s, loss=0.325]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 37 | Train Loss: 0.3945 | Train_ss: 0.5325 | Val Loss: 0.3046 | Val_ss: 0.6647\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 38: 100%|| 1341/1341 [08:08<00:00,  2.74it/s, loss=0.791]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 38 | Train Loss: 0.3914 | Train_ss: 0.5373 | Val Loss: 0.2294 | Val_ss: 0.6725\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 39: 100%|| 1341/1341 [08:08<00:00,  2.74it/s, loss=0.274]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 39 | Train Loss: 0.3997 | Train_ss: 0.5269 | Val Loss: 0.4961 | Val_ss: 0.6735\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 40: 100%|| 1341/1341 [08:08<00:00,  2.74it/s, loss=0.413]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 40 | Train Loss: 0.3851 | Train_ss: 0.5368 | Val Loss: 0.2314 | Val_ss: 0.6787\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 41: 100%|| 1341/1341 [08:08<00:00,  2.74it/s, loss=0.319]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 41 | Train Loss: 0.3759 | Train_ss: 0.5489 | Val Loss: 0.2163 | Val_ss: 0.6808\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 42: 100%|| 1341/1341 [08:09<00:00,  2.74it/s, loss=0.416]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 42 | Train Loss: 0.3753 | Train_ss: 0.5375 | Val Loss: 0.5330 | Val_ss: 0.6813\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 43: 100%|| 1341/1341 [08:08<00:00,  2.74it/s, loss=0.138]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 43 | Train Loss: 0.3652 | Train_ss: 0.5444 | Val Loss: 0.2356 | Val_ss: 0.6794\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 44: 100%|| 1341/1341 [08:08<00:00,  2.74it/s, loss=0.306]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 44 | Train Loss: 0.3628 | Train_ss: 0.5451 | Val Loss: 0.2010 | Val_ss: 0.6875\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 45: 100%|| 1341/1341 [08:08<00:00,  2.74it/s, loss=0.16]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 45 | Train Loss: 0.3579 | Train_ss: 0.5502 | Val Loss: 0.2008 | Val_ss: 0.6900\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 46: 100%|| 1341/1341 [08:08<00:00,  2.74it/s, loss=0.231]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 46 | Train Loss: 0.3548 | Train_ss: 0.5562 | Val Loss: 0.2362 | Val_ss: 0.6909\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 47: 100%|| 1341/1341 [08:08<00:00,  2.74it/s, loss=0.152]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 47 | Train Loss: 0.3540 | Train_ss: 0.5501 | Val Loss: 0.4110 | Val_ss: 0.6906\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 48: 100%|| 1341/1341 [08:08<00:00,  2.74it/s, loss=0.257]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 48 | Train Loss: 0.3451 | Train_ss: 0.5558 | Val Loss: 0.2506 | Val_ss: 0.6934\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 49: 100%|| 1341/1341 [08:08<00:00,  2.74it/s, loss=0.916]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 49 | Train Loss: 0.3500 | Train_ss: 0.5541 | Val Loss: 0.2152 | Val_ss: 0.6942\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 50: 100%|| 1341/1341 [08:09<00:00,  2.74it/s, loss=0.276]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 50 | Train Loss: 0.3518 | Train_ss: 0.5500 | Val Loss: 0.2210 | Val_ss: 0.7030\n",
            "Average training time per epoch: 527.61s\n",
            "Test Spearman's : 0.6399\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.6041253073716626, np.float64(0.6398635349868419))"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Start Training\n",
        "model, model_kwargs = FluorescencePredict, {\n",
        "    'vocab_size': tokenizer.vocab_size,\n",
        "    'd_model': 256,\n",
        "    'num_layers': 12,\n",
        "    'dim_feedforward': 128,\n",
        "    'p': 0.4,\n",
        "    'len_seq': 512,\n",
        "    'num_heads': 8,\n",
        "    'attn_mechanism': 'Attn2D3D_MultiPedLinAlter',\n",
        "    'num_peds': 20,\n",
        "    'k': 10\n",
        "}\n",
        "trained_model, optimizer_C2D3DMultiLin , loss_C2D3DMultiLin = train_model(model, model_kwargs, train_loader, valid_loader, epochs=50, lr=1e-5)\n",
        "evaluate_model(trained_model, test_loader)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PtQgkAfdyBD4",
      "metadata": {
        "id": "PtQgkAfdyBD4"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import os\n",
        "\n",
        "filename = 'losses_FluorescencePredict.pkl'\n",
        "\n",
        "# Load existing data if file exists, otherwise start with an empty list\n",
        "if os.path.exists(filename):\n",
        "    with open(filename, 'rb') as f:\n",
        "        all_losses = pickle.load(f)\n",
        "else:\n",
        "    all_losses = []\n",
        "# Your new dictionary of losses\n",
        "\n",
        "losses = {\n",
        "    'C2D 3D Linformer': loss_C2D3DLin,\n",
        "    'C2D 3D MultiLin': loss_C2D3DMultiLin\n",
        "}\n",
        "\n",
        "# Append the new dictionary to the list\n",
        "all_losses.update(losses)\n",
        "\n",
        "# Save back to the file\n",
        "with open(filename, 'wb') as f:\n",
        "    pickle.dump(all_losses, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kAmfTl-4WASw",
      "metadata": {
        "id": "kAmfTl-4WASw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "eiD5K9aD546K",
        "t5AiSMrK1ak1",
        "isKqf_0s67l3",
        "EqH2H92sF2iq",
        "QJ_WvQiwTB-O",
        "JJusOvNIa8Bh",
        "lRi0YkXIKPjX",
        "Aa9qPp3zgTNo",
        "23iE0JhQyyqX",
        "xPorzgr-7bZF",
        "CXBpsWkwwzrX",
        "fn1vtP4TZGEn",
        "kPftPWQasXoV",
        "nzMF2b7M9bEE",
        "BSDkIuMZ9bEE",
        "wa7-qpLi9bEE",
        "ZH_dV5ab9bEE",
        "pBKtmPNx9tzP",
        "TS2CIa9U9tzQ",
        "FjNHDWXi9tzQ",
        "1x1V_9ju9tzR",
        "6rlzr68s9tzR",
        "KQVgSfCB9tzR",
        "955FF5wm9tzS",
        "guQM6nb39tzS",
        "Vm5KwmNU9tzS",
        "j1_CopA59tzT",
        "8wHzgGwh9tzT",
        "Ms8JLk449tzU",
        "i7h1ykMh9DEg"
      ],
      "gpuType": "A100",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}